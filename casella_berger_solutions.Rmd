---
title: "Soultions to Exercises from Casella & Berger"
author: "Adam Bartonicek"
date: 'Last updated: `r Sys.Date()`'
output:
  html_document: default
  pdf_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chaper 2: Transformations and expectations

## 2.1 

In each of the following, find the pdf of $Y$ and show that it integrates to 1:

### (a)

$$Y = X^3 \qquad f_X(x) = 42x^5 (1 - x) \qquad 0 < x < 1$$
$$y = y(x) = x^3$$
$$\implies x = x(y) = y^{\frac{1}{3}}$$
$$\Bigg\lvert \frac{\partial x}{\partial y} \Bigg\lvert = \Bigg\lvert \frac{1}{3} y^{-\frac{2}{3}} \Bigg\lvert$$

$$f_Y(y) = 42y^{\frac{5}{3}} (1 - y^{\frac{1}{3}}) \cdot \frac{1}{3}y^{-\frac{2}{3}} = 14y(1 - y^{\frac{1}{3}}) \qquad \text{for } 0 < y < 1$$

$$\int_0^1 14 y (1 - y^{\frac{1}{3}} ) dy = 14 \int_0^1 y - y^{\frac{4}{3}} dy = 14 \bigg[ \frac{y^2}{2} - \frac{3y^{\frac{7}{3}}}{7} \bigg]_0^1 = 14 \bigg( \frac{1}{2} - \frac{3}{7} \bigg) = \frac{14}{14} = 1$$

### (b)

$$Y = 4X + 3 \qquad f_X(x) = 7e^{-7x} \qquad 0 < x < \infty$$
$$y = y(x) = 4x + 3$$
$$\implies x = x(y) = \frac{y - 3}{4}$$
$$\Bigg\lvert \frac{\partial x}{\partial y} \Bigg\lvert = \Bigg\lvert \frac{1}{4} \Bigg\lvert$$

$$f_Y(y) = \frac{7}{4}e^{-\frac{7(y - 3)}{4}} \qquad \text{for } 3 < y < \infty$$

$$\int_3^\infty \frac{7}{4} e^{-\frac{7(y - 3)}{4}} dy = \frac{7}{4} \int_3^\infty e^{-\frac{7(y - 3)}{4}} dy = \frac{7}{4} \bigg[ - \frac{4e^{\frac{7(y-3)}{4}}}{7} \bigg]_3^\infty = \frac{7}{4} \bigg( 0 - \bigg( - \frac{4}{7} \bigg) \bigg) = 1$$

### (c)

$$Y = X^2 \qquad f_X(x) = 30x^2(1 - x^2) \qquad 0 < x < 1$$
$$y = y(x) = x^2$$
$$\implies x = x(y) = \sqrt{y} \qquad \text{(since } x \text{ is positive only)}$$
$$\Bigg\lvert \frac{\partial x}{\partial y} \Bigg\lvert = \Bigg\lvert \frac{1}{2 \sqrt{y}} \Bigg\lvert$$

$$f_Y(y) = 30y(1 - \sqrt{y})^2 \cdot \frac{1}{2 \sqrt{y}} = 15\sqrt{y}(1 - \sqrt{y})^2 \qquad \text{for } 0 < y < 1$$

$$\int_0^1 15\sqrt{y}(1 - \sqrt{y})^2 dy = 15 \int_0^1 \sqrt{y} - 2y + y^{\frac{3}{2}} dy = 15 \bigg[ \frac{2y^{\frac{3}{2}}}{3} - y^2 + \frac{2y^{\frac{5}{2}}}{5} \bigg]_0^1 = 15 \bigg( \frac{2}{3} - 1 + \frac{2}{5} \bigg) = 10 - 15 + 6 = 1$$

## 2.2

In each of the following, find the pdf of $Y$:

### (a)

$$Y = X^2 \qquad \text{ and } \qquad f_X(x) = 1, \qquad 0 < x < 1$$
$$y = y(x) = x^2$$
$$x = x(y) = \sqrt{y} \qquad \text{(since } x > 0)$$
$$\Bigg\lvert \frac{\partial x}{\partial y} \Bigg\lvert = \Bigg\lvert \frac{1}{2 \sqrt{y}} \Bigg\lvert$$

$$f_Y(y) = \frac{1}{2 \sqrt{y}}, \qquad 0 < y < 1$$

### (b)

$$Y = -\log(X) \qquad f_X(x) = \frac{(n + m + 1)!}{n!m!} x^n (1 - x)^m, \qquad 0 < x < 1$$
$$y = y(x) - log(x)$$
$$x = x(y) = e^{-y}$$
$$\Bigg\lvert \frac{\partial x}{\partial y} \Bigg\lvert = \Bigg\lvert -e^{-y} \Bigg\lvert$$
$$f_Y(y) = \frac{(n + m + 1)!}{n!m!} e^{-ny} (1 - e^{-y})^m e^{-y} = \frac{(n + m + 1)!}{n!m!} e^{-n(n + 1)y}(1 - e^{-y})^m \qquad 0 < y < \infty$$

### (c)

$$Y = e^X \qquad f_X(x) = \frac{1}{\sigma^2} xe^{-(x/\sigma)^2 / 2}$$
$$y = y(x) = e^x$$
$$x = x(y) = \log(y)$$
$$\Bigg\lvert \frac{\partial x}{\partial y} \Bigg\lvert = \frac{1}{y}$$

$$f_Y(y) = \frac{1}{\sigma^2} \log(y)e^{-(\log(y)/\sigma)^2 / 2} \cdot \frac{1}{y} = \frac{1}{\sigma^2} \log(y) e^{-(\log(y) / \sigma^2) / 2} \qquad 0 < y < \infty$$

## 2.3

Suppose $X$ has a geometric pmg $f_X(x) = \frac{1}{3} \bigg( \frac{2}{3} \bigg)^x, \qquad x = 0, 1, 2, \ldots$. Determine the distribution of $Y = X / (X + 1)$

$$y = y(x) = x/(x + 1)$$
$$x = x(y) = y/(1 - y)$$
Since $Y$ is a discrete random variable (same as $X$), we do not need to calculate a Jacobian of the transformation (each transformed outcome has the same probability as its corresponding untransformed outcome). 

$$f_Y(y) = \frac{1}{3} \bigg( \frac{2}{3} \bigg)^{\frac{y}{1- y}}$$

## 2.4

Let $\lambda$ be a fixed constant and define $f_X(x) = \frac{1}{2} \lambda e^{-\lambda x}$ if $x \geq 0$ and $f_X(x) = \frac{1}{2} \lambda e^{\lambdax}$ if $x < 0$.

### (a)

Verify that $f_X(x)$ is a pdf:

In both cases, the function $f_X(x)$ returns a non-negative number (density). Now:

$$\int f_X(x) = \frac{\lambda}{2} \int_0^\infty e^{-\lambda x} dx + \frac{\lambda}{2} \int_{-\infty}^0 e^{\lambda x} dx$$
$$= \frac{\lambda}{2} \bigg[- \frac{e^{-\lambda x}}{\lambda} \bigg]_0^\infty + \frac{\lambda}{2} \bigg[ \frac{e^{\lambda x}}{\lambda} \bigg]_{-\infty}^0 = \frac{1}{2} + \frac{1}{2} = 1$$

Hence $f_X(x)$ is a valid pdf.

### (b)

If $X$ is a random variable given by $f_X(x)$, find $P(X < t)$ for all $t$. Evaluate all intergrals.

$$P(X < t) = \begin{cases} \frac{\lambda}{2} \int_{-\infty}^t e^{-\lambda x} dx & \text{if } t < 0 \\ \frac{1}{2} + \frac{\lambda}{2} \int_{0}^t e^{\lambda x} dx & \text{if } t \geq 0  \end{cases}$$
$$ \frac{\lambda}{2} \int_{\infty}^t e^{\lambda x} = \frac{\lambda}{2} \bigg[\frac{e^{\lambda x}}{\lambda} \bigg]_{-\infty}^t = \frac{\lambda}{2} \bigg(\frac{e^{-\lambda t)} - 0}{\lambda} \bigg) = \frac{e^{\lambda t}}{2}$$
$$ \frac{\lambda}{2} \int_{0}^t e^{-\lambda x} = \frac{\lambda}{2} \bigg[ -\frac{e^{-\lambda x}}{\lambda} \bigg]_{0}^t = \frac{\lambda}{2} \bigg(\frac{1 -e^{-\lambda t)}}{\lambda} \bigg) = \frac{1 - e^{-\lambda t}}{2}$$

$$P(X < t) = \begin{cases} \frac{e^{\lambda t}}{2} & \text{if } t < 0 \\ \frac{1}{2} + \frac{1 - e^{-\lambda t}}{2} & \text{if } t \geq 0  \end{cases}$$

## 2.5

Let $Y = \sin^2(x)$, and $f_X(x) = \frac{1}{2 \pi}$ for $0 < x < 2 \pi$. Find the pdf of $Y$.

```{r}

x <- seq(0, 2 * pi, 0.01)
y <- sin(x) * sin(x)

plot(x, y, type = 'l', col = 'steelblue', axes = FALSE)
axis(1, tick = FALSE, at = pi * c(0, 1/2, 1, 3/2, 2), 
     labels = round(pi * c(0, 1/2, 1, 3/2, 2), 2))
box(bty = 'L', col = 'grey60')

```

The function is monotone in regions: $(0, \pi/2), (\pi/2, \pi), (\pi, 3/2 \pi), (3/2 \pi, 2 \pi)$.

The inverses will be: $g_1(y) = \sin^{-1}( \sqrt{y})$, $g_2(y) = \pi - \sin^{-1}( \sqrt{y})$, $g_3(y) = \pi + \sin^{-1}( \sqrt{y})$, $g_4(y) = 2 \pi - \sin^{-1}( \sqrt{y})$. Their Jacobians will be $\frac{1}{2\sqrt{y}} \frac{1}{\sqrt{1 - y}}$ (the only thing that differs in the derivative inverses is the sign, so the Jacobians are the same), so the pdf ends up being:

$$f_Y(y) = 4 \cdot \frac{1}{2 \pi} \frac{1}{2\sqrt{y}} \frac{1}{\sqrt{1 - y}} = \frac{1}{\pi \sqrt{y} \sqrt{1 - y}} \qquad 0 \leq y \leq 1$$


## 2.6

In each of the following find the pdf of $Y$ and shw that it integrates to 1.

### (a)

$$f_X(x) = \frac{1}{2}e^{- \lvert x \lvert}, \qquad -\infty < x < \infty; \qquad Y = \lvert X \lvert^3$$
$$y = y(x) = \lvert x \lvert ^{1/3}$$
$$x = x(y) = \pm y^{1/3}$$
$$\Bigg\lvert \frac{\partial x}{\partial y} \Bigg\lvert = \Bigg\lvert \pm \frac{1}{3}y^{-2/3} \Bigg\lvert = \frac{1}{3}y^{-2/3}$$
$$f_Y(y) = \frac{1}{2}e^{-\lvert y^{1/3} \lvert} \cdot \frac{1}{3}y^{-2/3} + \frac{1}{2}e^{-\lvert y^{1/3} \lvert} \cdot \frac{1}{3}y^{-2/3}$$
$$= \frac{1}{3}y^{-2/3}e^{-y^{1/3}} \qquad 0 < y < \infty$$

$$\int_0^\infty \frac{1}{3}y^{-2/3} e^{-y^{1/3}} dy$$

Let $z = y^{\frac{1}{3}}$, $y = z^3$, $dy = 3 z^2 dz$:

$$\frac{1}{3} \int_0^\infty z^{-2} e^{-z} \cdot 3 z^2 dz = \frac{1}{3} \int_0^\infty 3 e^{-z} dz = \frac{1}{3} \cdot 3 \cdot \Gamma(1) = 1$$

### (b)

$$f_X(x) = \frac{3}{8}(x + 1)^2, \qquad -1 < x < 1; \qquad Y = 1 - X^2$$
$$y = y(x) = 1 - x^2$$
$$x = x(y) = \pm \sqrt{1 - y}$$
$$\Bigg \lvert \frac{\partial x}{\partial y} \Bigg \lvert = \Bigg \lvert -\frac{1}{2 \sqrt{1 - y}} \Bigg \lvert = \frac{1}{2 \sqrt{1 - y}}$$
$$f_Y(y) = \frac{3}{8}[-\sqrt{1 - y} + 1]^2 \cdot \frac{1}{2 \sqrt{1 - y}} + \frac{3}{8}[\sqrt{1 - y} + 1] \cdot \frac{1}{2 \sqrt{1 - y}} $$
$$= \frac{3}{8} \frac{1 - y - 2\sqrt{1 - y} + 1}{2\sqrt{1 - y}} + \frac{3}{8} \frac{1 - y + 2\sqrt{1 - y} + 1}{2\sqrt{1 - y}} = \frac{3}{8} \frac{2 - y}{\sqrt{1 - y}}$$
$$= \frac{3}{8} \frac{1}{\sqrt{1 - y}} + \frac{3}{8} \sqrt{1 - y}, \qquad 0 < y < 1$$

$$\int_0^1 \frac{3}{8}  \frac{1}{\sqrt{1 - y}} + \frac{3}{8} \sqrt{1 - y} \; dy = \frac{3}{8} \bigg[ \int_0^1 \frac{1}{\sqrt{1 - y}} dy + \int_0^1 \sqrt{1 - y} \; dy  \bigg]$$
$$= \frac{3}{8} \Bigg[ -2 \bigg[ \sqrt{1-y} \bigg]_0^1 - \frac{2}{3} \bigg[ (1 - y)^{3/2} \bigg]_0^1 \Bigg]$$
$$= \frac{3}{8} \Bigg[ -2 (0 - 1) - \frac{2}{3}(0 - 1) \Bigg] = \frac{3}{8} \cdot \frac{8}{3} = 1$$

### (c)

$$f_X(x) = \frac{3}{8})(x + 1)^2, \qquad -1 < x < 1; \qquad Y = 1 - X^2 \; \text{ if } X \leq 0, \qquad Y = 1 - X \; \text{ if } X > 0$$
$$y = y(x) = \begin{cases} 1 - x^2 & \text{if } x \leq 0 \\ 1 - x & \text{if } x > 0  \end{cases} $$
$$x = x(y) = \begin{cases} -\sqrt{1 - y} \\ 1 - y  \end{cases}$$
$$\Bigg \lvert \frac{\partial x}{\partial y} \Bigg \lvert = \begin{cases} \frac{1}{2 \sqrt{1 - y}} \\ 1 \end{cases}$$
$$f_Y(y) = \frac{3}{8}(-\sqrt{1 - y} + 1)^2 \cdot \frac{1}{2 \sqrt{1 - y}} + \frac{3}{8}(1 - y + 1) \cdot 1$$
$$= \frac{3}{16} \frac{2 - y - 2 \sqrt{1 - y}}{\sqrt{1 - y}} + \frac{3}{8} (2 - y)^2 \qquad 0 < y < 1$$

## 2.7

Let $X$ have pdf $f_X(x) = \frac{2}{9}(x + 1), \qquad -1 \leq x \leq 2$

### (a)

Find the pdf of $Y = X^2$ (theorem 2.1.8 is not directly applicable):

$$P(Y \leq y) = P(X^2 \leq y) = \begin{cases} P(-\sqrt{y} \leq X \sqrt{y}) & \text{if } \lvert x \lvert \leq 1 \\  P(1 \leq X \leq \sqrt{y}) & \text{if } x \geq 1 \end{cases}$$
$$= \begin{cases} \int_{-\sqrt{y}}^{\sqrt{y}} f_X(x)dx & \text{if } \lvert x \lvert \leq 1 \\ \int_1^{\sqrt{y}} f_X(x) dx & \text{if } x \geq 1  \end{cases}$$
$$\int f_X(x) dx = \frac{2}{9} \int (x + 1) dx = \frac{2}{9} \bigg[ \frac{x^2}{2} + x \bigg]$$

$$\implies F_Y(y) = \begin{cases} \frac{2}{9} \bigg[ \frac{x^2}{2} + x \bigg]_{-\sqrt{y}}^{\sqrt{y}} & \text{if } y \leq 1 \\ \frac{2}{9} \bigg[ \frac{x^2}{2} + x \bigg]_{1}^{\sqrt{y}} & \text{if } y \geq 1  \ \end{cases}$$
$$= \begin{cases} \frac{4}{9} \sqrt{y} & \text{if } y \leq 1 \\ \frac{2}{9} \frac{y-3}{2} + \frac{2}{9} \sqrt{y} \qquad \text{if } y \geq 1  \end{cases}$$

Differentiate:

$$f_X(x) = \begin{cases} \frac{2}{9} \frac{1}{\sqrt{y}} & \text{if } y \leq 1 \\ \frac{1}{9} + \frac{1}{9} \frac{1}{\sqrt{y}} & \text{if } y \geq 1  \end{cases}$$

### (b)

If sets $B_1, B_2, \ldots B_K$ are partitions of the range of $Y$, we can write:

$$f_Y(y) = \sum_k f_Y(y) I(y \in B_k) $$
and then we can apply theorem 2.1.8 to each of the pieces $B_k$ and add them up. 

## 2.8

In each of the following show that the given function is a cdf and find $F^{-1}_X(y)$:

### (a)

$$F_X(x) = \begin{cases} 0 & \text{if } x < 0 \\ 1 - e^{-x} & \text{if } x \geq 0 \end{cases}$$

$$\lim_{x \to 0} 1-e^{-x} = 0 \qquad \lim_{x \to \infty} 1 - e^{-x} = 1$$

$F_X(x)$ is increasing in $x$, continuous.

$$F_X^{-1}(y) = -\log(1 - y)$$

### (b)

$$F_X(x) = \begin{cases} e^{x}/2 & \text{if } x < 0 \\ 1/2 & \text{if } 0 \leq x \leq 1 \\ 1 - e^{(1 - x)}/2 & \text{if } 1 \leq x \end{cases}$$

$$\lim_{x \to -\infty} e^x / 2 = 0 \qquad \lim_{x \to \infty} 1 - e^{1 - x}/2 = 1$$

$e^x/2$ is increasing in $x$, $\frac{1}{2}$ is non-decreasing, $1 - e^{1 - x} / 2$ is increasing in $x$.

$$F_X^{-1}(y) = \begin{cases} \log(2y) & \text{if } y \leq 1/2 \\ 1 - \log(2(1 - y)) & \text{if } y > 1/2 \end{cases}$$

### (c)

$$F_X(x) = \begin{cases} e^x / 4 & \text{if } x < 0 \\ 1 - (e^{-x} / 4) & \text{if } x \geq 0 \end{cases}$$

$$\lim_{x \to -\infty} e^x/4 = 0 \qquad \lim_{x \to \infty} 1 - (e^{-x} / 4) = 1$$

$F_X(x)$ is increasing in $x$.

$$F_X^{-1}(y) = \begin{cases} \log(4y) & \text{if } y \leq 1/4 \\ -\log(4(1 - y)) & \text{if } y > 1/4 \end{cases}$$

## 2.9

$X$ has a pdf:

$$f_X(x) = \begin{cases} \frac{x-1}{2} & \text{if } 1 < x < 3 \\ 0 & \text{otherwise} \end{cases}$$

Find a monotone function $u(x)$ such that $Y = u(x)$ has a $\text{Uniform}(0, 1)$ distribution.

If $u(x) = F_x(x)$ then $F_X(x) \sim \text{Uniform}(0, 1)$. Thus:

$$\implies y = y(x) = \int_1^x \frac{z - 1}{2} dz = \frac{1}{2} \bigg[ \frac{z^2}{2} - z \bigg]_1^x = \frac{1}{2} \bigg( \frac{x^2}{2} - x - \frac{1}{2} + 1 \bigg)$$
$$= \frac{1}{2} \bigg( \frac{x^2 - 2x + 1}{2} \bigg) = \frac{(x - 1)^2}{4}$$

Now:

$$y = y(x) = \frac{(x-1)^2}{4}$$
$$x = x(y) = \sqrt{4y} + 1$$
$$\Bigg \lvert \frac{\partial x}{\partial y} \Bigg \lvert = \frac{2}{\sqrt{4y}}$$
$$f_Y(y) = \frac{\sqrt{4y} + 1 - 1}{2} \cdot \frac{2}{\sqrt{4y}} = \frac{\sqrt{4y}}{\sqrt{4y}} = 1$$

## 2.10

Let $X$ be a discrete random variable with cdf $F_X(x)$ and define a random variable $Y$ as $Y = F_X(x)$

### (a)

Prove that $Y$ is stochastically greater than $\text{Uniform}(0, 1)$; that is, if $U \sim \text{Uniform}(0, 1)$, then:

$$P(Y > y) \geq P(U \geq y) = 1 - y \qquad \text{for all y}, 0 < y < 1$$
$$P(Y > y) > P(U > y) = 1 - y \qquad \text{for some y}, 0 < y < 1$$
DO SOME OTHER TIME

## 2.11

Let $X$ have the standard normal pdf, $f_X(x) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}}$

### (a)

Find $E(X^2)$ directly, and then by using the pdf of $Y = X^2$ and calculating $E(Y)$.

$$E(X^2) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty x^2  e^{-\frac{x^2}{2}} dx$$

Let $u = x, u' = 1, v = -\frac{e^{-\frac{x^2}{2}}}{2}, v' = xe^{-\frac{x^2}{2}}$

$$= \frac{1}{\sqrt{2 \pi}} \Bigg[ \bigg[ -xe^{-\frac{x^2}{2}} \bigg]_{-\infty}^{\infty} + \int_{-\infty}^{\infty} e^{-\frac{x^2}{2}} dz \Bigg]$$
$$= \frac{1}{\sqrt{2 \pi}} \bigg( 0 + \sqrt{2 \pi } \bigg)$$
$$= \frac{\sqrt{2 \pi}}{\sqrt{2 \pi}} = 1$$

We know that if $X \sim \text{N}(0, 1)$ and $Y = X^2$, then $Y \sim \chi^2_1$, $f_Y(y) = \frac{1}{\sqrt{2 \pi y}} e^{-\frac{y}{2}}$ for $0 < y < \infty$.

Then:

$$E(Y) = \int_0^\infty y \frac{1}{\sqrt{2 \pi y} } e^{-\frac{y}{2}} = \frac{1}{\sqrt{2 \pi}} \int_{0}^\infty y^{\frac{1}{2}}e^{-\frac{y}{2}} dy$$

Let $z = \frac{y}{2}$, $y = 2z$, $dy = 2 dz$:

$$= \frac{1}{\sqrt{2 \pi}} \int_{0}^\infty (2z)^{\frac{1}{2}}e^{-z} 2dz = \frac{2}{\sqrt{\pi}} \int_0^\infty z^{\frac{1}{2}}e^{-z} dz = \frac{2}{\sqrt{\pi}} \Gamma \bigg( \frac{1}{2} \bigg) = 1$$

(we could also note that the expectation of $\chi^2_k$ is $k$)

### (b)

Find the pdf of $Y = \lvert X \lvert$, and its mean and variance.

$$F_y(y) = P(Y \leq y) = P( \lvert X \lvert \leq y) = P(-y \leq X \leq y)$$
$$P(X \leq y) - P(X \leq - y) = F_X(y) - F_X(-y)$$
$$\implies F_Y(y) = \frac{\partial}{\partial y} F_X(y) - F_X(-y) = f_X(y) + f_X(-y) = \frac{1}{\sqrt{2 \pi}}e^{-\frac{y^2}{2}} + \frac{1}{\sqrt{2 \pi}}e^{-\frac{y^2}{2}} = \sqrt{\frac{2}{\pi}} e^{-\frac{y^2}{2}}$$

(there's a typo in the texbook solutions: $\sqrt{\frac{2}{\pi}} e^{-\frac{y}{2}}$ instead of $\sqrt{\frac{2}{\pi}} e^{-\frac{y^2}{2}}$)

$$E(Y) = \sqrt{\frac{2}{\pi}} \int_0^{\infty} ye^{-\frac{y^2}{2}} dy$$

Let $z = \frac{y^2}{2}$, $y = \sqrt{2z}$, $dy = \frac{1}{\sqrt{2z}}$:

$$= \sqrt{\frac{2}{\pi}} \int_0^{\infty} \sqrt{2z} e^{-z} \cdot \frac{1}{\sqrt{2z}} dz = \sqrt{\frac{2}{\pi}} \int_0^\infty e^{-z} dz = \sqrt{\frac{2}{\pi}}$$

$$E(Y^2) = \sqrt{\frac{2}{\pi}} \int_0^{\infty} y^2e^{-\frac{y^2}{2}} dy = \sqrt{\frac{2}{\pi}} \int_0^\infty 2ze^{-z} \cdot \frac{1}{\sqrt{2 z}} dz$$

$$\frac{2}{\sqrt{\pi}} \int_0^{\infty} z^{\frac{1}{2}} e^{-z} dz =  \frac{2}{\sqrt{\pi}} \Gamma \bigg( \frac{3}{2} \bigg) = \frac{2}{\sqrt{\pi}} \cdot \frac{\sqrt{\pi}}{2} = 1$$

$$\implies Var(Y) = 1 - \frac{2}{\pi}$$

## 2.12

A random right triangle can be constructed by drawing an angle $X \sim \text{Uniform}(0, \pi/2)$, and then drawing a line of a distance $d$ such that $X$ is the angle between it and the x-axis. Let $Y$ be the height of the random triangle. Find the distribution and the mean of $Y$.

$$\tan x = \frac{y}{d} \qquad \tan^{-1} \bigg( \frac{y}{d} \bigg) = x \qquad  \frac{\partial}{\partial y} \tan^{-1} \bigg( \frac{y}{d} \bigg) = \frac{1}{1 + (y / d)^2} \cdot \frac{1}{d}$$

Hence:

$$f_Y(y) = f_X(g^{-1}(y)) \Bigg \lvert \frac{\partial x}{\partial y} \Bigg \lvert = \frac{2}{\pi} \cdot \frac{1}{1 + (y / d)^2} \cdot \frac{1}{d} \qquad 0 < y < \infty$$

I.e. a half-Cauchy distribution, with an infinite mean.

## 2.13

Consider a sequence of coin flips, each of which has a probability $p$ of landing heads. Define $X$ as the length of the run of either heads or tails, starting with first trial. Find the distribution of $X$ and $E(X)$

$$f_X(x) = p^x(1-p) + (1-p)^x p$$
$$E(X) = \sum_{x = 1}^\infty x \bigg[ p^x(1-p) + (1-p)^x p \bigg]$$
$$= p(1-p) \bigg[ \sum_{x=1}^\infty xp^{x - 1} + \sum_{x = 1}^\infty x(1-p)^{x - 1} \bigg]$$
$$= p(1-p) \bigg[ \frac{\partial}{\partial p} \sum_{x=1}^\infty p^{x} + \frac{\partial}{\partial p} \sum_{x = 1}^\infty -(1-p)^{x} \bigg]$$
$$= p(1 - p) \bigg[ \frac{\partial}{\partial p} \bigg( \frac{1}{1 - p} - 1 \bigg) + \frac{\partial}{\partial p} \bigg( -\frac{1}{p} - 1 \bigg) \bigg]$$
$$= p(1 - p) \bigg[ \frac{\partial}{\partial p} \bigg( \frac{p}{1 - p} \bigg) + \frac{\partial}{\partial p} \bigg( -\frac{1 - p}{p} \bigg) \bigg]$$
$$= p(1 - p) \bigg[ \frac{(1 - p) + p}{(1 - p)^2} +  -\frac{-  p - (1 - p)}{p^2} \bigg]$$
$$= \frac{p}{1 - p} + \frac{1 - p}{p}$$

## 2.14

Let $X$ be a continuous, non-negative random variable ($f_X(x) = 0 \text{ for } x < 0)$. Show that:

$$\int_0^\infty [1 - F_X(x)] dx = \int_0^\infty P(X > x) dx$$
$$= \int_0^\infty \int_x^\infty f_X(y) dy dx$$
$$= \int_0^\infty \int_0^y dx f_X(y) dy \qquad \text{(changing the order of integration)}$$
$$= \int_0^\infty y f_X(y) dy = E(X)$$

This is possible by switching the order of integration and changing the limits: the area described by $x \leq y < \infty$ and $0 \leq x < \infty$ (the first set of limits), is the same as $0 \leq x \leq y$ and $0 \leq y \leq \infty$.

```{r, echo = FALSE}

plot(0:10, 0:10, type = 'n', xlab = 'x', ylab = 'y', axes = FALSE, asp = 1)
polygon(c(0, 10, 0), c(0, 10, 10), col = 'antiquewhite', border = NA)
text(3, 7, labels = expression({x <= y} < infinity), col = 'steelblue')
text(3, 6, labels = expression({0 <= x} < y), col = 'firebrick')
text(5, 0.5, labels = expression({0 <= x} < infinity), col = 'steelblue')
text(-2, 5, labels = expression({0 <= y} < infinity), col = 'firebrick')
axis(1, at = c(0, 10), labels = c(0, expression(infinity)), tick = FALSE)
axis(2, at = 10, labels = expression(infinity), las = 1, tick = FALSE)
box(bty = 'l', col = 'grey80')

```

### (b)

Let $X$ be a discrete random variable whose range is the non-negative integers. Show that:

$$E(X) = \sum_{x = 0}^\infty (1 - F_X(x))$$

$$\sum_{x = 0}^\infty (1 - F_X(x)) = \sum_{x = 0}^\infty P(X > x)$$
$$= \sum_{x=0}^\infty \sum_{k = x + 1}^\infty f_X(k)$$ 
$$= \sum_{k = \infty}^0 \sum_{x = 0}^k f_X(x) \qquad \text{(each } k \text{ gets counted } x = k \text{ times, starting from the end)}$$

$$\sum_{k=0}^\infty k f_X(k) = E(X)$$

## 2.15

Let $X$ and $Y$ be any two random variables and define:

$$X \wedge Y = \min(X, Y) \qquad X \vee Y = \max(X, Y)$$
Show that: 

$$E(X \wedge Y) = E(X) + E(Y) - E(X \vee Y)$$
Starting from the assumption that sum of $X$ and $Y$ is equal to the sum of their respective minimum and maximum:

$$E(X + Y) = E(X \wedge Y) + E(X \vee Y)$$
$$\implies E(X) + E(Y) = E(X \wedge Y) + E(X \vee Y)$$
$$\implies E(X \vee Y) = E(X) + E(Y) - E(X \wedge Y)$$
## 2.16

Use the result of Exercise 2.14 to find the mean duration of certain telephone calls, where we assume that the duration, $T$, of a particular call can be described probabilistically by $P(T > t) = ae^{-\lambda t} + (1 - a)e^{-\mu t}$, where $0 < a < 1$, $\lambda > 0$, $\mu > 0$.

$$E(T) = \int_0^\infty [1 - F_T(t)] dt = \int_0^\infty ae^{-\lambda t} + (1 - a)e^{-\mu t} dt$$
$$= a\int_0^\infty e^{-\lambda t} dt + (1- a) \int_0^\infty e^{-\mu t} dt$$
$$= a \bigg[ - \frac{e^{-\lambda t}}{\lambda} \bigg]_0^\infty + (1 - a) \bigg[ -\frac{e^{-\mu t}}{\mu} \bigg]_0^\infty$$
$$= \frac{a}{\lambda} + \frac{1 - a}{\mu}$$

## 2.17

A median of a distribution is a value $m$ such that $P(X \leq m) \geq \frac{1}{2}$ and $P(X \geq m) \leq \frac{1}{2}$. Find the median of the following distributions:

### (a)

$$f_X(x) = 3x^2, \qquad 0 < x < 1$$
$$F_X(x) = \int_0^x 3y^2 dy = 3 \bigg[ \frac{y^3}{3} \bigg]_0^x = x^3 \qquad 0 < x < 1$$

$$F_X(m) = \frac{1}{2}$$
$$\implies m^3 = \frac{1}{2}$$
$$\implies m = \sqrt[3]{\frac{1}{2}} \approx 0.79$$

### (b)

$$f_X(x) = \frac{1}{\pi (1 + x^2)}, \qquad -\infty < x < \infty$$

The pdf is symmetric around 0, so the median will be 0, as long as the function is a valid pdf. Check if integrates to 1:

$$\frac{1}{\pi} \int_{-\infty}^\infty \frac{1}{1 + x^2} dx = \frac{1}{\pi} \bigg[ \tan^{-1}(x) \bigg]_{-\infty}^\infty = \frac{1}{\pi} \bigg( \frac{\pi}{2} - \bigg(- \frac{\pi}{2} \bigg) \bigg) = 1$$

## 2.18

Show that if $X$ is a continuous R.V., then $\text{min}_a E \lvert X - a \lvert = E \lvert X - m \lvert$

where $m$ is the median of $X$.

$$E(\lvert X - a \lvert ) = \int_{-\infty}^a -(x - a) f_X(x) dx + \int_{a}^\infty (x - a) f_X(x) dx$$
$$\frac{\partial}{\partial a} \int_{-\infty}^a -(x - a) f_X(x) dx + \int_{a}^\infty (x - a) f_X(x) dx = \int_{-\infty }^a f_X(x) dx - \int_{a}^\infty f_X(x) dx$$
$$\implies \text{Minimized if } \int_{-\infty }^a f_X(x) dx = \int_{a}^\infty f_X(x) dx$$
$$\implies a \text{ is the median}$$

## 2.19

Prove that $\frac{\partial}{\partial a} E(X - a)^2 = 0 \iff E(X) = a$

$$E(X - a)^2 = \int_{-\infty}^\infty (x - a)^2 f_X(x) dx$$

$$\frac{\partial}{\partial a} \int_{-\infty}^\infty (x - a)^2 f_X(x) dx = \int_{-\infty}^\infty (x - a)^2 f_X(x) dx$$
$$= -\int_{-\infty}^\infty 2x f_X(x)dx + \int_{-\infty}^\infty 2a f_X(x) dx$$
$$= -2 E (X) + 2a \int_{-\infty}^\infty f_X(x) dx = -2 E (X) + 2a$$
$$\implies \text{Will be zero iff } a = E(X)$$

## 2.20

A couple decides to continue having children until a daughter is born. What are the expected number of children of this couple?

$$E(X) = 1 + \sum_{k = 0}^\infty k(1 - p) p^k$$
$$= 1 + (1 - p)  \sum_{k = 0}^\infty k p^k$$
$$= 1 + (1 - p)p  \sum_{k = 0}^\infty k p^{k-1}$$
$$= 1 + (1 - p)p \frac{\partial}{\partial p}  \sum_{k = 0}^\infty p^{k}$$
$$= 1 + (1 - p)p  \frac{\partial}{\partial p} \frac{1}{1 - p}$$
$$= 1 + (1 - p) p \cdot \frac{1}{(1 - p)^2} = 1 + \frac{p}{1 - p} = \frac{1 - p + p}{1 - p} = \frac{1}{1 - p}$$

If we assume the probability of daughter born $= \frac{1}{2}$, then $E(X) = 2$

## 2.21

Prove the "two-way" rule for expectations: $E(g(X)) = E(Y)$ where $Y = g(X)$. Assume $g(x)$ is a monotone function.

$$E(g(X)) = \int_{-\infty}^\infty g(x) f_X(x) dx$$
$$E(Y) = \int_{-\infty}^\infty y f_Y(y) dy = \int_{-\infty}^\infty g(x) f_X(g^{-1}(y)) \frac{dx}{dy} dy = \int_{-\infty}^\infty g(x) f_X(x) dx$$

## 2.22

Let $X$ have pdf:

$$f_X(x) = \frac{4}{\beta^3 \sqrt{\pi}} x^2 e^{-x^2 / \beta^2} \qquad 0 < x < \infty, \qquad \beta > 0$$

### (a) 

Verify that $f_X(x)$ is a pdf:

$$\int_0^\infty \frac{4}{\beta^3 \sqrt{\pi}} x^2 e^{-x^2 / \beta^2} dx = \frac{4}{\beta^3 \sqrt{\pi}} \int_0^\infty x^2 e^{-x^2 / \beta^2} dx$$

Let $z = \frac{x^2}{\beta^2}$:

$$x = \sqrt{\beta^2 z} \qquad \text{(since } x \text{ is positive)}$$
$$dx = \frac{\beta}{2 \sqrt{z}} dz$$

$$\implies \frac{4}{\beta^3 \sqrt{\pi}} \int_0^\infty \beta^2 z e^{-z} \frac{\beta}{2 \sqrt{z}} dz = \frac{2}{\sqrt{\pi}} \int_0^\infty z^{\frac{1}{2}}e^{-z}$$
$$= \frac{2}{\sqrt{\pi}} \cdot \Gamma \bigg(\frac{3}{2} \bigg) = \frac{2}{\sqrt{\pi}} \cdot \frac{\sqrt{\pi}}{2} = 1$$

### (b)

Find the mean and variance of $X$:

$$E(X) = \int_0^\infty x f_X(x) dx = \frac{4}{\beta^3 \sqrt{\pi}} \int_0^\infty x^3 e^{-x^2 / \beta^2} dx$$

Let $z = \frac{x^2}{\beta^2}$:

$$x = \sqrt{\beta^2 z} \qquad \text{(since } x \text{ is positive)}$$
$$x^3 = \beta^3 z^{\frac{3}{2}}$$
$$dx = \frac{\beta}{2 \sqrt{z}} dz$$

$$\implies E(X) = \frac{4}{\beta^3 \sqrt{\pi}} \int_0^\infty  \beta^3 z^{\frac{3}{2}} e^{-z} \frac{\beta}{2 \sqrt{z}} dx$$
$$= \frac{2 \beta}{\sqrt{\pi}} \cdot \Gamma(2) = \frac{2 \beta}{\sqrt{\pi}}$$

$$\implies E(X^2) = \frac{2 \beta^2}{\sqrt{\pi}} \cdot \Gamma \bigg( \frac{5}{2} \bigg) = \frac{2 \beta^2}{\sqrt{\pi}} \cdot \frac{3 \sqrt{\pi}}{4} = \frac{3 \beta^2}{2}$$

$$Var(X) = E(X^2) - E(X)^2 = \frac{3 \beta^2}{2} - \bigg( \frac{2 \beta}{\sqrt{\pi}} \bigg)^2 = \frac{3 \beta^2}{2} - \frac{4 \beta^2}{\pi} = \frac{\beta^2(3 \pi - 8)}{2 \pi} $$

## 2.23 

Let $X$ have pdf: 

$$f_X(x) = \frac{1}{2} (1 + x) \qquad -1 < x < 1$$

### (a)

Find the pdf of $Y = X^2$:

$$f_Y(y) = f_X(g^{-1}(y)) \Bigg\lvert \frac{dx}{dy} \Bigg\lvert$$

$$y = g(x) = x^2$$
$$x = g^{-1}(y) = \pm \sqrt{y}$$
$$dx = \pm \frac{1}{2 \sqrt{y}} dy$$
$$\Bigg\lvert \frac{dx}{dy} \Bigg\lvert = \frac{1}{2 \sqrt{y}}$$

$$\implies f_Y(y) = \frac{1}{2}(1 - \sqrt{y}) \cdot \frac{1}{2 \sqrt{y}} + \frac{1}{2}(1 + \sqrt{y}) \cdot \frac{1}{2 \sqrt{}y} = \frac{1 - \sqrt{y} + 1 + \sqrt{y}}{4 \sqrt{y}}$$
$$= \frac{1}{2 \sqrt{y}} \qquad \text{for } 0 < y < 1$$


### (b)

Find the mean and variance of $y$:

$$E(Y) = \int_{0}^1 y\frac{1}{2 \sqrt{y}} dy = \frac{1}{2} \int_{0}^1 \sqrt{y} dy = \frac{1}{2} \bigg[ \frac{2y^{\frac{3}{2}}}{3} \bigg]_0^1 = \frac{1}{3}$$
$$E(Y^2) = \frac{1}{2} \int_0^1 y^{\frac{3}{2}} dy = \frac{1}{2} \bigg[ \frac{2y^{\frac{5}{2}}}{5} \bigg] = \frac{1}{5}$$
$$Var(Y) = E(Y^2) - E(Y)^2 = \frac{1}{5} - \bigg( \frac{1}{3} \bigg)^2 = \frac{1}{5} - \frac{1}{9} = \frac{9 - 5}{45} = \frac{4}{45}$$

## 2.24

Compute $E(X)$ and $Var(X)$ for each of the following pdfs:

### (a)

$$f_X(x) = ax^{a - 1} \qquad 0 < x < 1, \qquad a > 0$$

$$E(X) = \int_0^1 xax^{a - 1} dx = a \int_0^1 x^a dx = a \bigg[ \frac{x^{a + 1}}{a + 1} \bigg]_0^1 = \frac{a}{a + 1}$$
$$E(X^2) = \ldots = \frac{a}{a + 2}$$
$$Var(X) = E(X^2) - E(X)^2 = \bigg( \frac{a}{a + 2} \bigg) - \bigg( \frac{a}{a + 1} \bigg)^2$$

### (b)

$$f_X(x) = \frac{1}{n}, \qquad x = 1, 2, \ldots, \qquad n > 0$$

$$E(X) = \sum_{x = 1}^n \frac{x}{n} = \frac{1}{n} \bigg( \frac{n(n + 1)}{2} \bigg) = \frac{n + 1}{2}$$
$$E(X^2) = \sum_{x = 1}^n \frac{x^2}{n} = \frac{1}{n} \bigg( \frac{n(n + 1)(2n + 1)}{6} \bigg) = \frac{(n + 1)(2n + 1)}{6}$$
$$Var(X) = \frac{(n + 1)(2n + 1)}{6} - \bigg( \frac{n + 1}{2} \bigg)^2 = \frac{(n + 1)(2n + 1)}{6} - \frac{(n + 1)^2}{4}$$
$$= \frac{2(n + 1)(2n + 1) - 3(n + 1)^2}{12}$$
$$= \frac{(n + 1)[4n + 2 - 3n - 3]}{12}$$
$$= \frac{n^2 - 1}{12}$$

(there's a typo in the texbook solutions: $\frac{n^2 + 1}{12}$ instead of $\frac{n^2 - 1}{12}$)

### (c)

$$f_X(x) = \frac{3}{2}(x - 1)^2, \qquad 0 < x < 2$$
$$E(X) = \frac{3}{2} \int_0^2 x(x - 1)^2 dx = \int_0^2 x^3 - 2x^2 = x dx = \frac{3}{2} \bigg[ \frac{x^4}{4} - \frac{2x^3}{3} + \frac{x^2}{2} \bigg]_0^2$$
$$= \frac{3}{2} \bigg( \frac{16}{4} - \frac{16}{3} + \frac{4}{2} \bigg)$$
$$= \frac{3}{2} \bigg( \frac{48 - 64 + 24}{12} \bigg) = \frac{3}{2} \cdot \frac{8}{12} = 1$$

$$E(X^2) = \int_0^2 x^4 - 2x^3 + x^2 dx = \frac{3}{2} \bigg[ \frac{x^5}{5} - \frac{2x^4}{4} + \frac{x^3}{3} \bigg]_0^2$$
$$= \frac{3}{2} \bigg( \frac{32}{5} + \frac{32}{4} + \frac{8}{3} \bigg)$$
$$= \frac{3}{2} \bigg( -\frac{32}{20} + \frac{8}{3} \bigg)$$
$$= \frac{3}{2} \bigg(\frac{8}{3} - \frac{8}{5} \bigg)$$
$$= \frac{3}{2} \bigg( \frac{40 - 24}{15} \bigg) = \frac{3}{2} \cdot \frac{16}{15} = \frac{8}{5}$$
$$Var(X) = \frac{8}{5} - 1^2 = \frac{3}{5}$$

## 2.25

Suppose the pdf $f_X(x)$ of a random variable $X$ is an even function ($f_X(x) = f_X(-x)$). Show that:

### (a)

$X$ and $-X$ are identically distributed:

Let $Y = -X$, $g^{-1}(y) = -y$. Then:

$$f_Y(y) = f_X(g^{-1}(y)) \Bigg\lvert \frac{\partial x}{\partial y} \Bigg\lvert = f_X(-x) \cdot 1 = f_X(x)$$
### (b)

$M_X(t)$ is symmetric around zero.

$$M_X(0 + \epsilon) = \int_{-\infty}^\infty e^{(0 + \epsilon)x} f_X(x) dx$$
$$= \int_{-\infty}^0 e^{\epsilon x} f_X(x) dx + \int_0^\infty e^{\epsilon x} f_X(x) dx$$
$$= \int_0^\infty e^{-\epsilon x} f_X(-x) dx  + \int_{-\infty}^0 e^{-\epsilon x} f_X(-x) dx $$
$$= \int_{-\infty}^\infty e^{(0 - \epsilon) x} f_X(-x) dx$$
$$= M_X(0 - \epsilon)$$

## 2.26

Let $f_X(x)$ be a pdf and let $a$ be a number such that, for all $\epsilon > 0$, $f(a + \epsilon) = f(a - \epsilon)$. Such pdf is said to be symmetric around point $a$.

### (a)

Give three examples of such pdfs:

$\text{Normal}(0, 1)$ is symmetric around $a = 0$, $\text{Normal}(\mu, \sigma^2)$ is symmetric around $a = \mu$, $\text{Uniform}(a, b)$ is symmetric around $a = (a + b)/2$.

### (b)

Show that if $X$ is symmetric around $a$, then $a$ is the median of $X$.

$$\int_a^\infty f_X(x) dx$$
Let $\epsilon = x - a, \qquad 0 < \epsilon < \infty$. Then:

$$\int_a^\infty f_X(x) dx = \int_0^\infty f_X(a + \epsilon) d \epsilon$$
$$= \int_0^\infty f_X(a - \epsilon) d \epsilon$$
$$= \int_{-\infty}^a f_X(x) dx$$

Now:

$$\int_{-\infty}^a f_X(x) dx + \int_a^\infty f_X(x) dx = 1$$
and:

$$\int_{-\infty}^a f_X(x) dx = \int_a^\infty f_X(x) dx$$

$$\implies \int_{-\infty}^a f_X(x) dx = \int_a^\infty f_X(x) dx = \frac{1}{2}$$

### (c)

Show that if $f_X(x)$ is symmetric and $E(X)$ exists, then $E(X) = a$

$$E(X - a) = \int_{-\infty}^\infty (x - a) f_X(x) dx = \int_{-\infty}^a (x - a) f_X(x) dx + \int_a^{\infty} (x - a) f_X(x)dx$$
$$= \int_0^\infty - \epsilon f_X(a - \epsilon) d \epsilon + \int_0^\infty \epsilon f_X(a + \epsilon) d \epsilon$$
$$= \int_0^\infty - \epsilon f_X(a - \epsilon) d \epsilon + \int_0^\infty \epsilon f_X(a - \epsilon) d \epsilon \qquad (f_X(a - \epsilon) \text{ and } f_X(a + \epsilon) \text{ are the same})$$
$$= 0$$

$$\implies E(X - a) = E(X) - a = 0 \implies E(X) = a$$

### (d)

Show that $f_X(x) = e^{-x}, \qquad x > 0$ is not a symmetric pdf:

$$f_X(a - \epsilon) = e^{-(a - \epsilon)} \neq e^{-(a + \epsilon)} = f_X(a + \epsilon) \qquad \text{for all } a \text{ and } \epsilon > 0$$

### (e)

Show that for the pdf in part (d), the median is less than the mean:

$$F_X(x) = 1 - e^{-x}$$
$$m = F_X^{-1}(1/2)$$
$$\implies 1 - e^{-m} = \frac{1}{2}$$
$$\implies e^{-m} = \frac{1}{2}$$
$$\implies m = - \log \bigg( \frac{1}{2} \bigg) = \log(2) \approx 0.69$$

$$E(X) = \int_0^\infty xe^{-x} dx = \bigg[ -xe^{-x} \bigg]_0^\infty - \int_0^\infty -e^{-x}  = 0 + \bigg[ -e^{-x} \bigg]_{0}^\infty = (-0 - (-1)) = 1$$

## 2.27

Let $f(x)$ be a pdf and let $a$ be a number such that if $a \geq x \geq y$ then $f(a) \geq f(x) \geq f(y)$ and if $a \leq x \leq y$ then $f(a) \geq f(x) \geq f(y)$. Such pdf is called unimodal with mode equal to $a$.

### (a)

Give example of a unimodal pdf for which the mode is unique:

Standard normal, normal, Cauchy, Gamma, etc...

### (b)

Give example of a unimodal pdf for which the mode is not unique:

Uniform

### (c)

Show that if $f_X(x)$ is both symmetric and unimodal, the point of symmetry is the mode.

Let $a = m + \epsilon$ be some point at distance $\epsilon > 0$ from the mode $m$.

$$f(m) > f(m + \epsilon) \geq f(m + 2 \epsilon)$$
$$\implies f(a - \epsilon) > f(a) \geq f(a + \epsilon)$$
$$\implies f(a - \epsilon) > f(a + \epsilon)$$
$$\implies \text{pdf is NOT symmetric}$$
Thus, $a$ has to be the mode. 


### (d)

Consider the pdf $f_X(x) = e^{-x}, \qquad x > 0$. Show that this pdf is unimodal. What is the mode?

$f(0) > f(x) > f(y)$ for all $0 < x < y$. Thus, $f_X(x)$ is unimodal and 0 is the mode.

## 2.28

Let $\mu_n$ denote the $n$th central moment of a random variable $X$. Two quantities of interest, in addition to mean and variance, are:

$$\alpha_3 = \frac{\mu_3}{(\mu_2)^{3/2}} \qquad \text{and} \qquad \alpha_4 = \frac{\mu_4}{\mu_2^2}$$

$\alpha_3$ is called skewness and $\alpha_4$ is called kurtosis. 

### (a)

Show that if a pdf is symmetric about a point $a$, then skewness $\alpha_3 = 0$.

$$\int_{-\infty}^\infty (x - a)^3 f_X(x) dx = \int_{-\infty}^a (x - a)^3 f_X(x) dx + \int_a^{\infty} (x - a)^3 f_X(x) dx$$
$$= \int_{-\infty}^0 \epsilon^3 f_X(\epsilon + a) d \epsilon + \int_0^\infty \epsilon^3 f_X(\epsilon + a) d \epsilon$$

$$= \int_0^\infty -\epsilon^3 f_X(-\epsilon + a) d \epsilon + \int_0^\infty \epsilon^3 f_X(\epsilon + a) d \epsilon$$
$$= \int_0^\infty -\epsilon^3 f_X(\epsilon + a) d \epsilon + \int_0^\infty \epsilon^3 f_X(\epsilon + a) d \epsilon \qquad \text{(since } f_X(a + \epsilon) = f_X(a - \epsilon))$$

$$= 0$$

### (b)

Calculate $\alpha_3$ for $f_X(x) = e^{-x}, \qquad x \geq 0$, a pdf that is skewed to the right.

$$\mu_1 = E(X) = \int_0^\infty xe^{-x} dx = \bigg[ -xe^{-x} \bigg]_0^\infty - \int_0^\infty -e^{-x}  = 0 + \bigg[ -e^{-x} \bigg]_{0}^\infty = (-0 - (-1)) = 1$$
$$\mu_2 = \int_0^\infty (x - 1)^2 e^{-x} dx = \int_0^\infty x^2 e^{-x} dx - 2 \int_0^\infty xe^{-x} + \int_0^\infty e^{-x} dx$$
$$= \bigg[ -x^2e^{-x} \bigg]_{0}^\infty -2 \int_{0}^\infty -xe^{-x} dx - 2 + 1$$
$$= 0 + 2 - 2 + 1 = 1$$

$$\mu_3 = \int_0^\infty (x - 1)^3 e^{-x} dx = \int_0^\infty x^3 e^{-x} dx - 3\int_0^\infty x^2 e^{-x} dx + 3 \int_0^1 xe^{-x} dx - \int_0^\infty e^{-x} dx$$
$$\Gamma(4) - 3 \Gamma(3) + 3 \Gamma(2) - \Gamma(1)$$
$$= 6 - 3 \cdot 2 + 3 - 1 = 2$$

$$\alpha_3 = \frac{\mu_3}{(\mu_2)^{3/2}} = \frac{2}{1} = 2$$

### (c)

Calculate $\alpha_4$ for each of the following pdfs and comment on the peakedness of each:

#### i.

$$f_X(x) = \frac{1}{\sqrt{2 \pi}} e^{-x^2 / 2} \qquad - \infty < x < \infty$$

Since the mean $\mu$ is zero, and the second moment $\sigma^2$ is 1, we just need the fourth moment, $E(X^4)$. Let $W = X^2 \implies E(X^4) = E(W^2)$. Then:

$$E(W^2) = Var(W) + E(W)^2$$

$W$ is a Chi-squared distribution with one degree of freedom, so its expectation is 1 and its variance is 2. Thus:

$$\alpha_4 = mu_4 = E(X^4) = E(W^2) = 2 + 1^3 = 3$$

#### ii.

$$f_X(x) = \frac{1}{2}, \qquad -1 < x < 1$$
$$\mu = E(X) = 0$$
$$\mu_2 = \frac{1}{2} \int_{-1}^1 x^2 = \frac{1}{2} \bigg[ \frac{x^3}{3} \bigg]_{-1}^1 = \frac{1}{2} \bigg( \frac{1}{3} - \bigg( -\frac{1}{3} \bigg) \bigg) = \frac{1}{3}$$
$$\mu_4 = \frac{1}{2} \int_{-1}^1 x^4 dx = \frac{1}{2} \bigg[ \frac{x^5}{5} \bigg] = \frac{1}{5}$$
$$\alpha_4 = \frac{\mu_4}{\mu_2^2} = \frac{\frac{1}{5}}{ \big( \frac{1}{3} \big)^2} = \frac{9}{5}$$

#### iii.

$$f_X(x) = \frac{1}{2}e^{- \lvert x \lvert}, \qquad -\infty < x < \infty$$
$$\mu = E(X) = 0$$
$$\mu_2 = \frac{1}{2} \int_{-\infty}^\infty x^2e^{- \lvert x \lvert} dx  = \frac{1}{2} \int_{-\infty}^0 x^2e^x dx + \frac{1}{2} \int_0^\infty x^2e^{-x} dx$$
$$= \int_0^\infty x^2 e^{-x} dx \qquad \text{(by symmetry)} $$
$$= \Gamma(3) = 2$$
$$\mu_4 = \frac{1}{2} \int_{-\infty}^\infty x^4 e^{- \lvert x \lvert} dx  = \Gamma(5) = 24$$
$$\alpha_4 = \frac{\mu_4}{\mu_2^2} = \frac{24}{2^2} = 6$$

## 2.29

To calculate moments of discrete distributions, it is often easier to work with the factorial moments.

### (a)

Calculate the factorial moment $E[X(X-1)]$ for the Binomial and Poisson distribution.

For Binomial:

$$E[X(X - 1)] = \sum_{x=0}^n x(x-1) {n \choose x} p^x (1 - p)^{n - x}$$
$$= \sum_{x = 0}^n x(x - 1) \frac{n!}{x! (n - x)!} p^x (1 - p)^{n - x}$$
$$= n(n - 1)p^2 \sum_{x =0}^n \frac{(n-2)!}{(x - 2)! (n - x)!} p^{x - 2} (1 - p)^{n - x}$$
$$= n(n-1)p^2$$

For Poisson:

$$E[X(X - 1)] = \sum_{x = 0}^\infty x(x-1) \frac{\lambda^x e^{-\lambda}}{x!}$$
$$= \lambda^2 \sum_{x = 0}^\infty \frac{\lambda^{x - 2} e^{\lambda}}{(x - 2)!} = \lambda^2$$

### (b)

Use the results from (a) to calculate variances.

For Binomial:

$$Var(X) = E(X^2) - E(X)^2 = E[X(X - 1]) - E(X)^2 + E(X)$$
$$= n(n-1)p^2 - n^2 p^2 + np$$
$$= n[(n - 1)p^2 - np^2 + p]$$
$$= n[p - p^2] = np(1 - p)$$
For Poisson:

$$Var(X) = \lambda^2 - \lambda^2 + \lambda = \lambda$$

### (c)

A particularly nasty discrete distribution is Beta-Binomial, with pmf:

$$P(Y = y) = a(y + a) \frac{{n \choose y} {a + b - 1 \choose a}}{{n + a + b - 1 \choose y + a}}$$
Use factorial moments to calculate the variance of Beta-Binomial:

$$E[Y(Y-1)] = \sum_{y = 0}^n y(y - 1) a (y + a) \frac{{n \choose y} {a + b - 1 \choose a}}{{n + a + b - 1 \choose y + a}}$$
$$= \frac{a^2 n(n-1)}{(y + a)(y - 1 + a)} \sum_{y = 0}^n a(y + a - 2) \frac{{n-2 \choose y-2} {a + b - 1 \choose a}}{{n + a + b - 1 \choose y - 2 + a}} $$

DO SOME OTHER TIME

## 2.30

Find the moment generating functions for the following pdfs:

### (a)

$$f_X(x) = \frac{1}{c}, \qquad 0 < x < c$$

$$M_X(t) = \frac{1}{c} \int_{0}^c e^{tx} dx = \frac{1}{c} \bigg[ \frac{e^{tx}}{t} \bigg]_0^c = \frac{1}{c} \cdot \frac{e^{tc} - 1}{t} = \frac{e^{tc} - 1}{ct}$$
### (b)

$$f_X(x) = \frac{2x}{c^2}, \qquad 0 < x < c$$

$$M_X(t) = \frac{2}{c^2} \int_{0}^c xe^{tx} dx = \frac{2}{c^2} \Bigg[ \bigg[ xe^{tx} \bigg]_0^c - \int_0^c e^{tx} dx \Bigg]$$
$$= \frac{2}{c^2} \Bigg[ ce^{tc} - \bigg[ e^{tx} \bigg]_0^c \Bigg]$$
$$= \frac{2}{c^2} \bigg( ce^{tc} - e^{tc} + 1 \bigg) = \frac{2(ce^{tc} - e^{tc} + 1)}{c^2}$$

### (c)

$$f_X(x) = \frac{1}{2 \beta} e^{- \lvert x - \alpha \lvert / \beta}, \qquad -\infty < x < \infty, \; -\infty < \alpha < \infty, \; \beta > 0$$
$$M_X(t) = \frac{1}{2 \beta} \int_{-\infty}^\alpha e^{tx} e^{(x - \alpha) / \beta} dx + \frac{1}{2 \beta} \int_\alpha^\infty e^{tx} e^{(\alpha - x) / \beta} dx$$
$$= \frac{e^{-\alpha/\beta}}{2 \beta} \int_{-\infty}^\alpha e^{ x / \beta + xt} dx + \frac{e^{-\alpha/\beta}}{2 \beta} \int_{\alpha}^\infty e^{-x / \beta + xt} dx$$
$$= \frac{e^{-\alpha/\beta}}{2 \beta} \bigg[ \frac{e^{x(1 / \beta + t)}}{1/\beta + t} \bigg]_{-\infty}^\alpha + \frac{e^{-\alpha/\beta}}{2 \beta}  \bigg[ \frac{e^{x(-1 / \beta + t)}}{-1/\beta + t} \bigg]_{-\infty}^\alpha$$
$$= \frac{e^{-\alpha/\beta}}{2 \beta} \cdot \frac{e^{\alpha (1 / \beta + t)}}{1 / \beta + t} - \frac{e^{-\alpha \beta}}{2 \beta} \cdot \frac{e^{\alpha(-1 / \beta + t)}}{1 / \beta - t}$$

### (d)

$$f_X(x) = {r + x - 1 \choose x} p^r ( 1 - p)^x, \qquad x = 0, 1, \ldots, \; 0 < p < 1, \; r > 0$$
$$M_X(t) = \sum_{x = 0}^\infty e^{tx} {r + x - 1 \choose x} p^r ( 1 - p)^x $$
$$= p^r \sum_{x = 0}^\infty {r + x - 1 \choose x} [(1 - p)e^t]^x$$
$$= p^r \cdot \frac{1}{\big(1 - (1 - p)e^t \big)^r} \qquad \text{(normalizing constant for the original pmf)}$$
$$\bigg( \frac{p}{1 - (1 -p)e^t} \bigg)^r \qquad t < - \log(1 - p)$$

## 2.31

Does a distribution exist for which $M_X(t) = t / (1 - t), \; \lvert t \lvert < 1$. If yes, find it. If no, prove it.

$$M_X(0) = E(e^{0}) = 1$$
However, $\frac{0}{1 - 0} = 0$, therefore it cannot be an mgf. 

## 2.32

Let $M_X(t)$ be the moment generating function of $X$ and definte $S(t) = \log(M_X(t))$. Show that:

$$\frac{\partial}{\partial t} S(t) \Bigg\lvert_{t = 0} = E(X) \qquad \text{and} \qquad \frac{\partial^2}{\partial t^2} S(t) \Bigg\lvert_{t = 0} = Var(X)$$

$$\frac{\partial}{\partial t} S(t) = \frac{\partial}{\partial t} \log(M_X(t)) = \frac{\frac{\partial}{\partial t} M_X(t)}{M_X(t)} \Bigg\lvert_{t = 0} = \frac{E(X)}{1} = E(X)$$
$$\frac{\partial^2}{\partial t^2} S(t) = \frac{\partial^2}{\partial t^2} \log(M_X(t)) = \frac{\partial}{\partial t} \frac{\frac{\partial}{\partial t} M_X(t)}{M_X(t)} = \frac{\frac{\partial^2}{\partial t^2} M_X(t) \cdot M_X(t) - \big[ \frac{\partial}{\partial t} M_X(t) \big]^2}{\big[ M_X(t) \big]^2} \Bigg\lvert_{t=0} = \frac{E(X^2) - E(X)^2}{1^2} = Var(X)$$

## 2.33

In each of the following cases verify the expression given for the moment generating function, and in each case use mgf to calculate $E(X)$ and $Var(X)$:

### (a)

$$f_X(x) = \frac{e^{-\lambda} \lambda^x}{x!}, \qquad M_X(t) = e^{\lambda(e^t - 1)}, \qquad x = 0, 1, \ldots \; \lambda >0$$
$$M_X(t) = \sum_{x = 0}^\infty e^{tx} \frac{e^{-\lambda} \lambda^x}{x!} = e^{-\lambda} \sum_{x = 0}^\infty \frac{(\lambda e^t)^x}{x!} = e^{-\lambda} e^{\lambda e^t} = e^{\lambda(e^t - 1)}$$
$$E(X) = \frac{\partial}{\partial t} M_X(t) \Bigg \lvert_{t = 0} = e^{\lambda(e^t - 1)}  \lambda e^t \Bigg \lvert_{t = 0} = \lambda$$
$$E(X) = \frac{\partial^2}{\partial t^2} M_X(t) \Bigg \lvert_{t = 0} = \frac{\partial}{\partial t} \lambda e^{\lambda(e^t - 1) + t}   \Bigg \lvert_{t = 0} = \lambda e^{\lambda(e^t - 1) + t} \lambda e^t +  \lambda e^{\lambda(e^t - 1) + t}   \Bigg \lvert_{t = 0} = \lambda^2 + \lambda$$
$$Var(X) = \lambda^2 + \lambda - \lambda^2 = \lambda$$

### (b)

$$f_X(x) = p(1-p)^x, \qquad M_X(t) = \frac{p}{1 - (1 - p)e^t}, \qquad x = 0, 1, \ldots, \; 0 < p < 1$$
$$M_X(t) = \sum_{x = 0}^\infty e^{tx} p(1 - p)^x = p \sum_{x = 0}^\infty [(1 - p)e^t]^x = \frac{p}{1 - (1 - p)e^t} \qquad t < -\log(1 - p)$$
$$E(X) = \frac{\partial}{\partial t} \frac{p}{1 - (1 - p)e^t} \Bigg \lvert_{t = 0} = p \frac{(1 - p)e^t}{[1 - (1 - p)e^t]^2} \Bigg \lvert_{t = 0} = \frac{p(1- p)}{p^2} = \frac{1 - p}{p}$$
$$E(X^2) = \frac{\partial}{\partial t} p \frac{(1 - p)e^t}{[1 - (1 - p)e^t]^2} \Bigg \lvert_{t = 0} = p(1 - p) \frac{e^t[1 - (1 - p)e^t]^2 + 2[1 - (1 - p)e^t](1 - p)e^t}{[1 - (1 - p)e^t]^4} \Bigg \lvert_{t = 0}$$
$$= p(1 - p) \frac{p^2 + 2p(1 - p)}{p^4} = \frac{(1 - p)p + 2(1 - p)^2}{p^2}$$
$$Var(X) = \frac{(1 - p)p + 2(1 - p)^2}{p^2} - \bigg( \frac{1 - p}{p} \bigg)^2 =  \frac{(1 - p)p + 2(1 - p)^2 - (1 -p^2)}{p^2}$$
$$= \frac{(1 - p)p + (1 - p)^2}{p^2} = \frac{p - p^2 + 1 -2p + p^2}{p^2} = \frac{1 - p}{p^2}$$

### (c)

$$f_X(x) = \frac{e^{-(x - \mu)^2 / (2 \sigma^2)}}{\sqrt{2 \pi \sigma}}, \qquad M_X(t) = e^{\mu t + \sigma^2 t^2 / 2} \qquad -\infty < \mu < \infty; \sigma > 0$$

$$M_X(t) = \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^\infty e^{tx} e^{-(x - \mu)^2 / (2 \sigma^2)} dx = \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^\infty  e^{ -(x^2 - 2 \mu x - 2 \sigma^2 t + \mu^2) / (2 \sigma^2) } dx$$

Complete the square in the exponent:

$$x^2 - 2 \mu x - 2 \sigma^2 t + \mu^2 = x^2 - 2x(\mu + \sigma^2 t) + \mu^2$$
$$= [x - (\mu  + \sigma^2 t)]^2 - (\mu + \sigma^2 t)^2 + \mu^2$$
$$= [x - (\mu  + \sigma^2 t)]^2 - [2 \mu \sigma^2 t + (\sigma^2 t)^2]$$
Going back to the density:

$$\frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^\infty  e^{ -(x^2 - 2 \mu x - 2 \sigma^2 t + \mu^2) / (2 \sigma^2)} dx = \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^\infty  e^{ -([x - (\mu + \sigma^2 t)]^2 + [2 \mu \sigma^2 t + (\sigma^2 t)^2]) / (2 \sigma^2)} dx$$
$$= \frac{e^{[2 \mu \sigma^2 t + (\sigma^2 t)^2] / (2 \sigma^2)}}{\sqrt{2 \pi \sigma}} \int_{-\infty}^\infty e^{-[x - (\mu + \sigma^2 t)]^2 / (2 \sigma^2)} dx$$
$$= e^{\mu t + (\sigma^2 t^2) / 2}$$
$$E(X) = \frac{\partial}{\partial t} M_X(t) \Bigg \lvert_{t = 0} = \frac{\partial}{\partial t} e^{\mu t + (\sigma^2 t^2) / 2} \Bigg \lvert_{t = 0} =  (\mu + \sigma^2 t) e^{\mu t + (\sigma^2 t^2) / 2} \Bigg \lvert_{t = 0} = \mu$$
$$E(X^2) = \frac{\partial}{\partial t} \mu + \sigma^2 t(e^{\mu t + (\sigma^2 t^2) / 2}) \Bigg \lvert_{t = 0} = (\mu + \sigma^2 t)^2 e^{\mu t + (\sigma^2 t^2) / 2} + \sigma^2 e^{\mu t + (\sigma^2 t^2) / 2} \Bigg \lvert_{t = 0} = \mu^2 + \sigma^2$$
$$Var(X) = \sigma^2$$

## 2.34

Let $X$ have the standard normal distribution:

$$f_X(x) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}}, \qquad -\infty < x < \infty$$

Define a discrete random variable $Y$ scuh that:

$$f_Y(y) = \begin{cases} \frac{1}{6} & \text{if } y = \sqrt{3} \\ \frac{2}{3} & \text{if } y = 0 \\ \frac{1}{6} & \text{if } y = -\sqrt{3} \end{cases}$$
Show that:

$$E(X^r) = E(Y^r) \qquad \text{for } r = 1, 2, 3, 4, 5$$
$$E(X) = 0 \qquad E(Y) = \frac{1}{6} \cdot \sqrt{3} + \frac{2}{3} \cdot 0 - \frac{1}{6} \cdot \sqrt{3} = 0$$
$$E(X^2) = \mu^2 + \sigma^2 = 1 \qquad E(Y) = \frac{1}{6} \cdot \sqrt{3}^2 + \frac{1}{6} \cdot (-\sqrt{3})^2 = 1$$
$$E(X^3) = \frac{\partial^3}{\partial t^3} e^{t^2 / 2} \Bigg \lvert_{t = 0} = \frac{\partial^2}{\partial t^2} te^{t^2 / 2} \Bigg \lvert_{t = 0} = \frac{\partial}{\partial t} e^{t^2 / 2} + t^2e^{t^2 / 2} \Bigg \lvert_{t = 0} = te^{t^2 / 2} + 2te^{t^2 / 2} + t^3e^{t^2 / 2} \Bigg \lvert_{t = 0} = 0$$
$$E(Y^3) = \frac{1}{6} \cdot 3^{3/2} - \frac{1}{6} \cdot 3^{3/2} = 0$$

$$E(X^4) = \frac{\partial}{\partial t} te^{t^2 / 2} + 2te^{t^2 / 2} + t^3 e^{t^2 / 2} \Bigg \lvert_{t = 0} = e^{t^2 / 2} + t^2 e^{t^2 / 2} + 2 e^{t^2 / 2} + 2t^2 e^{t^2 / 2} + 3t^2 e^{t^2 / 2} + t^4 e^{t^2 / 2} \Bigg \lvert_{t = 0} = 3$$
$$E(Y^4) = \frac{1}{6} \cdot 3^2 + \frac{1}{6} \cdot 3^2 = \frac{18}{6} = 3$$

$$E(X^5) = 0 \qquad \text{(there's no terms with } t^1 \text{ in the previous equation)}$$
$$E(Y^5) = \frac{1}{6} \cdot 3^{\frac{5}{2}} - \frac{1}{6} \cdot 3^{\frac{5}{2}} = 0$$
## 2.35

Let $X$ have the following distribution:

$$f_X(x) = \frac{1}{\sqrt{2 \pi} x} e^{-[\log(x)]^2 / 2}, \qquad 0 \leq x < \infty$$

### (a)

Show that:

$$E(X^r) = e^{r^2 / 2}$$

$$E(X^r) = \frac{1}{\sqrt{2 \pi}} \int_0^\infty x^{r - 1} e^{-[\log(x)]^2 / 2} dx$$
$$= \frac{1}{\sqrt{2 \pi}} \int_{0}^\infty (e^z)^{r - 1} e^{-z^2 / 2} e^z dz \qquad \text{(change of variable } z = \log(x))$$
$$= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty e^{-(z^2 - 2zr) / 2} dz$$
$$= \frac{1}{\sqrt{2 \pi }} \int_{-\infty}^\infty e^{- [(z - r)^2 - r^2] / 2} \qquad \text{(complete the square: } z^2 - 2zr = (z - r)^2 - r^2)$$
$$= \frac{1}{\sqrt{2 \pi}} e^{r^2 / 2} \int_{-\infty}^{\infty} e^{-(z - r)^2 / 2}$$
$$= e^{r^2 / 2}$$
### (b)

Now show that:

$$\int_0^\infty x^r \sin[2 \pi \cdot \log(x)] f_X(x)  dx = 0$$

$$\int_0^\infty x^r \sin[2 \pi \cdot \log(x)] f_X(x)  dx = \frac{1}{\sqrt{2 \pi}} \int_0^\infty x^{r - 1} \sin[2 \pi \cdot \log(x)]  e^{-[\log(x)]^2 / 2} dx$$
$$= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty e^{zr} \sin[2 \pi \cdot \log (e^z)] e^{-z^2 / 2} dz \qquad \text{(change of variable } z = \log(x))$$
$$= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty \sin[2 \pi \cdot z] e^{-(z^2 - 2zr) / 2}$$
$$= \frac{1}{\sqrt{2 \pi }} \int_{-\infty}^\infty \sin[2 \pi z] e^{-[(z - r)^2 - r^2] / 2}$$

The integrand is an odd function: $$\sin[2 \pi z] e^{-[(z - r)^2 - r^2] / 2} = -\sin[2 \pi z] e^{-[(z - r)^2 - r^2] / 2}$$. As such, the integral will be 0. 

```{r}

r <- 3
x <- seq(-4 * pi, 4 * pi, 0.01)
f <- function(x) sin(2 * pi * x) * exp(-((x - r)^2 - r^2)/ 2)

plot(x, f(x), type = 'l', axes = FALSE, 
     ylab = expression(f[X](x)), col = 'steelblue')
axis(1, tick = FALSE)
axis(2, tick = FALSE, las = 1)
box(bty = 'L')

integrate(f, -4 * pi, 4 * pi)

```

## 2.36

The lognormal distribution has an interesting property. If we have the pdf: 

$$f_X(x) = \frac{1}{\sqrt{2 \pi} x} e^{-[\log(x)]^2 / 2}, \qquad 0 \leq x < \infty$$

Then, as the previous exercise (Exercise 2.35) shows, all moments of the function exists and are finite. However, the distribution does not have a moment generating function. Prove this.

$$M_X(t) = \frac{1}{\sqrt{2 \pi}} \int_0^\infty e^{tx} e^{-[\log(x)]^2 / 2} dx$$
$$= \frac{1}{\sqrt{2 \pi}} \int_0^\infty e^{-\log(x)^2 /  2 + tx} dx$$

Now:

$$\lim_{x \to \infty} e^{tx - \log(x)^2/2 } = \lim_{x \to \infty} \frac{e^{tx}}{e^{\log(x)^2 / 2}} = \lim_{x \to \infty} \frac{te^{tx}}{e^{\log(x)^2 / 2} \frac{\log(x)}{x}} = \lim_{x \to \infty} \frac{xte^{tx}}{e^{\log(x)^2 / 2} \log(x)} = \infty$$

As such, the integrand will tend to infinity and the integral won't converge, hence mgf does not exist. 

## 2.37

Let:

$$f_X(x) = \frac{1}{\sqrt{2 \pi}} e^{-x^2 / 2} \qquad f_Y(y) = \frac{1}{\sqrt{2 \pi}} e^{-x^2 / 2} \bigg[ 1 + \frac{1}{2} \sin(2 \pi x) \bigg]$$

be pdfs with the following cumulant generating functions:

$$K_X(t) = t^2 / 2 \qquad \text{and} K_Y(t) = K_X(t) + \log \bigg[ 1 + \frac{1}{2}e^{-2 \pi^2 } \sin(2 \pi t) \bigg]$$

### (a)

Plot $f_X(x)$ and $f_Y(y)$ to illustrate their difference:

```{r}

x <- seq(-5, 5, 0.01)
d1 <- dnorm(x)
d2 <- dnorm(x) * (1 + 1/2 * sin(2 * pi * x))

plot(x, d2, type = 'l', col = 'firebrick', ylab = 'Density', axes = FALSE)
lines(x, d1, col = 'steelblue')
axis(1, tick = FALSE)
box(bty = 'L', col = 'grey80')

```

### (b)

Plot the cumulant generating functions $K_X$ and $K_Y$ to illustrate their similarity:

```{r}

t <- seq(-5, 5, 0.01)

k1 <- t^2 / 2
k2 <- t^2 / 2 + log(1 + 1/2 * exp(-2 * pi^2) * sin(2 * pi * t))

plot(x, k1, type = 'l', col = 'firebrick', ylab = '', axes = FALSE)
lines(x, k2, col = 'steelblue', lty = 'dashed')
axis(1, tick = FALSE)
box(bty = 'L', col = 'grey80')

```

### (c)

Calculate moment generating functions of $f_X(x)$ and $f_Y(Y)$. Are they similar or different?

DO SOME OTHER TIME

## 2.38

Let $X$ have a negative binomial distribution with pmf:

$$f_X(x) = {r + x - 1 \choose x} p^r ( 1 - p)^x, \qquad x = 0, 1, \ldots, \; r = 0, 1, \ldots, \; 0 < p < 1$$

### (a)

Calculate the mgf of $X$:

$$M_X(t) = p^r \sum_{x = 0}^\infty e^{tx} {r + x - 1 \choose x}  (1 - p)^x$$
$$= p^r \sum_{x = 0}^\infty {r + x - 1 \choose x} [(1 - p)e^t]^x$$
$$= \frac{p^r}{[1 - (1 - p)e^t]^r} = \bigg( \frac{p}{1 - (1 - p)e^t} \bigg)^r$$

### (b)

Define a new random variable $Y = 2 pX$. Show that as $p \to 0$, the mgf of $Y$ converges to that of a Chi-squared random variable with $2r$ degrees if freedom by showin that:

$$\lim_{p \to 0} M_Y(t) = \bigg( \frac{1}{1 - 2t} \bigg)^r, \qquad \lvert t \lvert < \frac{1}{2}$$

$$M_Y(t) = M_X(2pt) = \bigg( \frac{p}{1 - (1 - p)e^{2 pt}} \bigg)^r$$

$$\lim_{p \to 0 } \frac{p}{1 - (1 - p)e^{2 pt}} = \lim_{p \to 0} \frac{1}{1 + pe^{2pt} - 2t(1 - p) e^{2pt}} = \frac{1}{1 - 2t}$$

$$\implies \lim_{p \to 0} M_Y(t) = \bigg( \frac{1}{1 - 2t} \bigg)^r$$

## 2.39

In each of the following cases calculate the indicated derivatives, justifying all operations:

### (a)

$$\frac{\partial}{\partial x} \int_0^x e^{-\lambda t} dt = e^{-\lambda x}$$

Verify:

$$\frac{\partial}{\partial x} \bigg[ \int_0^x e^{-\lambda t} dt \bigg] = \frac{\partial}{\partial x} \bigg[ -\frac{e^{-\lambda t}}{\lambda} \bigg]_0^x = \frac{\partial}{\partial x} \bigg( \frac{1}{\lambda} - \frac{e^{-\lambda x}}{\lambda} \bigg) = e^{-\lambda x}$$

### (b)

$$\frac{\partial}{\partial \lambda} \int_0^\infty e^{-\lambda t} dt =  \int_0^\infty \frac{\partial}{\partial \lambda} e^{-\lambda t} dt = \int_0^\infty -te^{-\lambda t} dt = - \frac{\Gamma(2)}{\lambda^2} = -\frac{1}{\lambda^2}$$

Verify:

$$\frac{\partial}{\partial \lambda} \bigg[ \int_0^\infty e^{-\lambda t} dt \bigg] =  \frac{\partial}{\partial \lambda} \bigg( \frac{1}{\lambda} \bigg) = -\frac{1}{\lambda^2}$$

### (c)

$$\frac{\partial}{\partial t} \int_0^1 \frac{1}{x^2} dx = -\frac{1}{t^2}$$

Verify:

$$\frac{\partial}{\partial t} \bigg[ \int_0^1 \frac{1}{x^2} dx \bigg] = \frac{\partial}{\partial t} \bigg[ -\frac{1}{x} \bigg]_t^1$$
$$= \frac{\partial}{\partial t} \bigg( \frac{1}{t} - 1  \bigg) = -\frac{1}{t^2}$$

### (d)

$$\frac{\partial}{\partial t} \int_1^\infty \frac{1}{(x - t)^2} dx = \int_1^\infty \frac{\partial}{\partial t} \frac{1}{(x - t)^2} dx = \int_1^\infty -2(x - t)^{-3} dx = \bigg[(x - t)^{-2} \bigg]_{1}^\infty = \frac{1}{(1 - t)^2}$$
Verify:

$$\frac{\partial}{\partial t} \bigg[ \int_1^\infty \frac{1}{(x - t)^2} dx \bigg] = \frac{\partial}{\partial t} \bigg[ - \frac{1}{x - t} \bigg]_1^\infty = \frac{\partial}{\partial t} \frac{1}{1 - t} = \frac{1}{(1 - t)^2}$$

## 2.40

Prove:

$$\sum_{k=0}^x {n \choose k} p^k (1 - p)^{n - k} = (n - x) {n \choose x} \int_{0}^{1 - p} t^{n - x - 1}(1 - t)^x dx$$

(hint: integrate by parts or differentiate both sides wre to $p$)

DO SOME OTHER TIME

# Chapter 3: Common families of distributions

## 3.1

Find expressions for $E(X)$ and $Var(X)$ if $X$ is a random variable with the general discrete $\text{Uniform}(a, b)$ distribution that puts equal probability on values $a, a + 1, \ldots, +b$.

$$E(X) = \sum_{x=a}^b \frac{x}{b - a + 1} = \frac{1}{b - a + 1} \bigg[ \sum_{x = 1}^b x - \sum_{x = 1}^{a - 1} x \bigg] = \frac{1}{b - a + 1} \cdot \bigg[ \frac{b(b + 1)}{2} - \frac{(a - 1)a}{2} \bigg]$$
$$= \frac{b^2 + b - a^2 + a}{2(b - a + 1)} = \frac{(b - a + 1)(a + b)}{2 (b - a + 1)} = \frac{a + b}{2}$$

$$E(X^2) = \frac{1}{b - a + 1} \bigg[ \sum_{x = 1}^b x^2 - \sum_{x = 1}^{a - 1} x^2 \bigg] = \frac{1}{b - a + 1} \bigg[ \frac{b(b + 1)(2b + 1)}{6} - \frac{(a-  1)a (2a - 1)}{6} \bigg]$$
$$= \frac{b(b + 1)(2b + 1) - (a - 1)(a)(2a - 1)}{6(b - a + 1)}$$

$$Var(X) = \frac{b(b + 1)(2b + 1) - (a - 1)(a)(2a - 1)}{6(b - a + 1)} - \bigg( \frac{a + b}{2} \bigg)^2$$
$$= \frac{b(b + 1)(2b + 1) - (a - 1)(a)(2a - 1) - 3(b - a + 1)(a + b)^2}{6(b - a + 1)}$$

## 3.2

A manufacturer receives a lot of 100 parts from a vendor. The lot will be unacceptable if more than five parts are defective. The manufacturer will randomly select $K$ parts from the lot for inspection and the lot will be accepted if no defective parts are found.

### (a)

How large does $K$ have to be to ensure the probability that the manufacturer accepts an unacceptable lot is less than 0.1?

The smallest probability of discovering a defective part will be when there are exactly 6 defective parts (minimum) in the sample. 

$$P(X = 0 \lvert M = 100, N = 6, K) = \frac{{6 \choose 0} {94 \choose K}}{{100 \choose K}} = \frac{\frac{94!}{K! (94 - K)!}}{\frac{100!}{K!(100 - K)!}} = \prod_{i = 95}^{100} \frac{(i - K)}{i}$$

```{r}

K <- 25:35
p <- sapply(K, function(x) prod((95:100 - x) / 95:100))

round(rbind(K, p), 3)

```

The manufacturer needs to sample at least 32 parts to keep the probability of accepting an unacceptable lot under 0.1.

### (b)

Suppose the manufacturer decides to accept the lot if there is at most one defective in the sample. How large does $K$ have to be to ensure the probability of accepting an unacceptable lot is less than 0.1?

$$P(X = 0 \text{ or }  1 \lvert M = 100, N = 6, K) = \frac{{6 \choose 0} {94 \choose K}}{{100 \choose K}} + \frac{{6 \choose 1} {94 \choose K - 1}}{{100 \choose K}}$$

```{r}

K <- 50:60

round(rbind(K, dhyper(0, 6, 94, K) + dhyper(1, 6, 94, K)), 3)

```

The manufacturer needs to sample at least 51 parts.

### 3.3

Suppose a pedestrian can cross the street only if there is no car passing during the next 3 seconds. Assume that there is a probability $p$ of a car passing during each second (which we treat as indivisible units here). Find the probability that the pedestrian has to wait exactly 4 seconds before starting to cross.

DO SOME OTHER TIME


