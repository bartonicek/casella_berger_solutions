---
title: "Soultions to Exercises from Casella & Berger"
author: "Adam Bartonicek"
date: 'Last updated: `r Sys.Date()`'
output:
  html_document: default
  pdf_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chaper 2: Transformations and expectations

## 2.1 

In each of the following, find the pdf of $Y$ and show that it integrates to 1:

### (a)

$$Y = X^3 \qquad f_X(x) = 42x^5 (1 - x) \qquad 0 < x < 1$$
$$y = y(x) = x^3$$
$$\implies x = x(y) = y^{\frac{1}{3}}$$
$$\Bigg\lvert \frac{\partial x}{\partial y} \Bigg\lvert = \Bigg\lvert \frac{1}{3} y^{-\frac{2}{3}} \Bigg\lvert$$

$$f_Y(y) = 42y^{\frac{5}{3}} (1 - y^{\frac{1}{3}}) \cdot \frac{1}{3}y^{-\frac{2}{3}} = 14y(1 - y^{\frac{1}{3}}) \qquad \text{for } 0 < y < 1$$

$$\int_0^1 14 y (1 - y^{\frac{1}{3}} ) dy = 14 \int_0^1 y - y^{\frac{4}{3}} dy = 14 \bigg[ \frac{y^2}{2} - \frac{3y^{\frac{7}{3}}}{7} \bigg]_0^1 = 14 \bigg( \frac{1}{2} - \frac{3}{7} \bigg) = \frac{14}{14} = 1$$

### (b)

$$Y = 4X + 3 \qquad f_X(x) = 7e^{-7x} \qquad 0 < x < \infty$$
$$y = y(x) = 4x + 3$$
$$\implies x = x(y) = \frac{y - 3}{4}$$
$$\Bigg\lvert \frac{\partial x}{\partial y} \Bigg\lvert = \Bigg\lvert \frac{1}{4} \Bigg\lvert$$

$$f_Y(y) = \frac{7}{4}e^{-\frac{7(y - 3)}{4}} \qquad \text{for } 3 < y < \infty$$

$$\int_3^\infty \frac{7}{4} e^{-\frac{7(y - 3)}{4}} dy = \frac{7}{4} \int_3^\infty e^{-\frac{7(y - 3)}{4}} dy = \frac{7}{4} \bigg[ - \frac{4e^{\frac{7(y-3)}{4}}}{7} \bigg]_3^\infty = \frac{7}{4} \bigg( 0 - \bigg( - \frac{4}{7} \bigg) \bigg) = 1$$

### (c)

$$Y = X^2 \qquad f_X(x) = 30x^2(1 - x^2) \qquad 0 < x < 1$$
$$y = y(x) = x^2$$
$$\implies x = x(y) = \sqrt{y} \qquad \text{(since } x \text{ is positive only)}$$
$$\Bigg\lvert \frac{\partial x}{\partial y} \Bigg\lvert = \Bigg\lvert \frac{1}{2 \sqrt{y}} \Bigg\lvert$$

$$f_Y(y) = 30y(1 - \sqrt{y})^2 \cdot \frac{1}{2 \sqrt{y}} = 15\sqrt{y}(1 - \sqrt{y})^2 \qquad \text{for } 0 < y < 1$$

$$\int_0^1 15\sqrt{y}(1 - \sqrt{y})^2 dy = 15 \int_0^1 \sqrt{y} - 2y + y^{\frac{3}{2}} dy = 15 \bigg[ \frac{2y^{\frac{3}{2}}}{3} - y^2 + \frac{2y^{\frac{5}{2}}}{5} \bigg]_0^1 = 15 \bigg( \frac{2}{3} - 1 + \frac{2}{5} \bigg) = 10 - 15 + 6 = 1$$

## 2.2

In each of the following, find the pdf of $Y$:

### (a)

$$Y = X^2 \qquad \text{ and } \qquad f_X(x) = 1, \qquad 0 < x < 1$$
$$y = y(x) = x^2$$
$$x = x(y) = \sqrt{y} \qquad \text{(since } x > 0)$$
$$\Bigg\lvert \frac{\partial x}{\partial y} \Bigg\lvert = \Bigg\lvert \frac{1}{2 \sqrt{y}} \Bigg\lvert$$

$$f_Y(y) = \frac{1}{2 \sqrt{y}}, \qquad 0 < y < 1$$

### (b)

$$Y = -\log(X) \qquad f_X(x) = \frac{(n + m + 1)!}{n!m!} x^n (1 - x)^m, \qquad 0 < x < 1$$
$$y = y(x) - log(x)$$
$$x = x(y) = e^{-y}$$
$$\Bigg\lvert \frac{\partial x}{\partial y} \Bigg\lvert = \Bigg\lvert -e^{-y} \Bigg\lvert$$
$$f_Y(y) = \frac{(n + m + 1)!}{n!m!} e^{-ny} (1 - e^{-y})^m e^{-y} = \frac{(n + m + 1)!}{n!m!} e^{-n(n + 1)y}(1 - e^{-y})^m \qquad 0 < y < \infty$$

### (c)

$$Y = e^X \qquad f_X(x) = \frac{1}{\sigma^2} xe^{-(x/\sigma)^2 / 2}$$
$$y = y(x) = e^x$$
$$x = x(y) = \log(y)$$
$$\Bigg\lvert \frac{\partial x}{\partial y} \Bigg\lvert = \frac{1}{y}$$

$$f_Y(y) = \frac{1}{\sigma^2} \log(y)e^{-(\log(y)/\sigma)^2 / 2} \cdot \frac{1}{y} = \frac{1}{\sigma^2} \log(y) e^{-(\log(y) / \sigma^2) / 2} \qquad 0 < y < \infty$$

## 2.3

Suppose $X$ has a geometric pmg $f_X(x) = \frac{1}{3} \bigg( \frac{2}{3} \bigg)^x, \qquad x = 0, 1, 2, \ldots$. Determine the distribution of $Y = X / (X + 1)$

$$y = y(x) = x/(x + 1)$$
$$x = x(y) = y/(1 - y)$$
Since $Y$ is a discrete random variable (same as $X$), we do not need to calculate a Jacobian of the transformation (each transformed outcome has the same probability as its corresponding untransformed outcome). 

$$f_Y(y) = \frac{1}{3} \bigg( \frac{2}{3} \bigg)^{\frac{y}{1- y}}$$

## 2.4

Let $\lambda$ be a fixed constant and define $f_X(x) = \frac{1}{2} \lambda e^{-\lambda x}$ if $x \geq 0$ and $f_X(x) = \frac{1}{2} \lambda e^{\lambdax}$ if $x < 0$.

### (a)

Verify that $f_X(x)$ is a pdf:

In both cases, the function $f_X(x)$ returns a non-negative number (density). Now:

$$\int f_X(x) = \frac{\lambda}{2} \int_0^\infty e^{-\lambda x} dx + \frac{\lambda}{2} \int_{-\infty}^0 e^{\lambda x} dx$$
$$= \frac{\lambda}{2} \bigg[- \frac{e^{-\lambda x}}{\lambda} \bigg]_0^\infty + \frac{\lambda}{2} \bigg[ \frac{e^{\lambda x}}{\lambda} \bigg]_{-\infty}^0 = \frac{1}{2} + \frac{1}{2} = 1$$

Hence $f_X(x)$ is a valid pdf.

### (b)

If $X$ is a random variable given by $f_X(x)$, find $P(X < t)$ for all $t$. Evaluate all intergrals.

$$P(X < t) = \begin{cases} \frac{\lambda}{2} \int_{-\infty}^t e^{-\lambda x} dx & \text{if } t < 0 \\ \frac{1}{2} + \frac{\lambda}{2} \int_{0}^t e^{\lambda x} dx & \text{if } t \geq 0  \end{cases}$$
$$ \frac{\lambda}{2} \int_{\infty}^t e^{\lambda x} = \frac{\lambda}{2} \bigg[\frac{e^{\lambda x}}{\lambda} \bigg]_{-\infty}^t = \frac{\lambda}{2} \bigg(\frac{e^{-\lambda t)} - 0}{\lambda} \bigg) = \frac{e^{\lambda t}}{2}$$
$$ \frac{\lambda}{2} \int_{0}^t e^{-\lambda x} = \frac{\lambda}{2} \bigg[ -\frac{e^{-\lambda x}}{\lambda} \bigg]_{0}^t = \frac{\lambda}{2} \bigg(\frac{1 -e^{-\lambda t)}}{\lambda} \bigg) = \frac{1 - e^{-\lambda t}}{2}$$

$$P(X < t) = \begin{cases} \frac{e^{\lambda t}}{2} & \text{if } t < 0 \\ \frac{1}{2} + \frac{1 - e^{-\lambda t}}{2} & \text{if } t \geq 0  \end{cases}$$

## 2.5

Let $Y = \sin^2(x)$, and $f_X(x) = \frac{1}{2 \pi}$ for $0 < x < 2 \pi$. Find the pdf of $Y$.

```{r}

x <- seq(0, 2 * pi, 0.01)
y <- sin(x) * sin(x)

plot(x, y, type = 'l', col = 'steelblue', axes = FALSE)
axis(1, tick = FALSE, at = pi * c(0, 1/2, 1, 3/2, 2), 
     labels = round(pi * c(0, 1/2, 1, 3/2, 2), 2))
box(bty = 'L', col = 'grey60')

```

The function is monotone in regions: $(0, \pi/2), (\pi/2, \pi), (\pi, 3/2 \pi), (3/2 \pi, 2 \pi)$.

The inverses will be: $g_1(y) = \sin^{-1}( \sqrt{y})$, $g_2(y) = \pi - \sin^{-1}( \sqrt{y})$, $g_3(y) = \pi + \sin^{-1}( \sqrt{y})$, $g_4(y) = 2 \pi - \sin^{-1}( \sqrt{y})$. Their Jacobians will be $\frac{1}{2\sqrt{y}} \frac{1}{\sqrt{1 - y}}$ (the only thing that differs in the derivative inverses is the sign, so the Jacobians are the same), so the pdf ends up being:

$$f_Y(y) = 4 \cdot \frac{1}{2 \pi} \frac{1}{2\sqrt{y}} \frac{1}{\sqrt{1 - y}} = \frac{1}{\pi \sqrt{y} \sqrt{1 - y}} \qquad 0 \leq y \leq 1$$


## 2.6

In each of the following find the pdf of $Y$ and shw that it integrates to 1.

### (a)

$$f_X(x) = \frac{1}{2}e^{- \lvert x \lvert}, \qquad -\infty < x < \infty; \qquad Y = \lvert X \lvert^3$$
$$y = y(x) = \lvert x \lvert ^{1/3}$$
$$x = x(y) = \pm y^{1/3}$$
$$\Bigg\lvert \frac{\partial x}{\partial y} \Bigg\lvert = \Bigg\lvert \pm \frac{1}{3}y^{-2/3} \Bigg\lvert = \frac{1}{3}y^{-2/3}$$
$$f_Y(y) = \frac{1}{2}e^{-\lvert y^{1/3} \lvert} \cdot \frac{1}{3}y^{-2/3} + \frac{1}{2}e^{-\lvert y^{1/3} \lvert} \cdot \frac{1}{3}y^{-2/3}$$
$$= \frac{1}{3}y^{-2/3}e^{-y^{1/3}} \qquad 0 < y < \infty$$

$$\int_0^\infty \frac{1}{3}y^{-2/3} e^{-y^{1/3}} dy$$

Let $z = y^{\frac{1}{3}}$, $y = z^3$, $dy = 3 z^2 dz$:

$$\frac{1}{3} \int_0^\infty z^{-2} e^{-z} \cdot 3 z^2 dz = \frac{1}{3} \int_0^\infty 3 e^{-z} dz = \frac{1}{3} \cdot 3 \cdot \Gamma(1) = 1$$

### (b)

$$f_X(x) = \frac{3}{8}(x + 1)^2, \qquad -1 < x < 1; \qquad Y = 1 - X^2$$
$$y = y(x) = 1 - x^2$$
$$x = x(y) = \pm \sqrt{1 - y}$$
$$\Bigg \lvert \frac{\partial x}{\partial y} \Bigg \lvert = \Bigg \lvert -\frac{1}{2 \sqrt{1 - y}} \Bigg \lvert = \frac{1}{2 \sqrt{1 - y}}$$
$$f_Y(y) = \frac{3}{8}[-\sqrt{1 - y} + 1]^2 \cdot \frac{1}{2 \sqrt{1 - y}} + \frac{3}{8}[\sqrt{1 - y} + 1] \cdot \frac{1}{2 \sqrt{1 - y}} $$
$$= \frac{3}{8} \frac{1 - y - 2\sqrt{1 - y} + 1}{2\sqrt{1 - y}} + \frac{3}{8} \frac{1 - y + 2\sqrt{1 - y} + 1}{2\sqrt{1 - y}} = \frac{3}{8} \frac{2 - y}{\sqrt{1 - y}}$$
$$= \frac{3}{8} \frac{1}{\sqrt{1 - y}} + \frac{3}{8} \sqrt{1 - y}, \qquad 0 < y < 1$$

$$\int_0^1 \frac{3}{8}  \frac{1}{\sqrt{1 - y}} + \frac{3}{8} \sqrt{1 - y} \; dy = \frac{3}{8} \bigg[ \int_0^1 \frac{1}{\sqrt{1 - y}} dy + \int_0^1 \sqrt{1 - y} \; dy  \bigg]$$
$$= \frac{3}{8} \Bigg[ -2 \bigg[ \sqrt{1-y} \bigg]_0^1 - \frac{2}{3} \bigg[ (1 - y)^{3/2} \bigg]_0^1 \Bigg]$$
$$= \frac{3}{8} \Bigg[ -2 (0 - 1) - \frac{2}{3}(0 - 1) \Bigg] = \frac{3}{8} \cdot \frac{8}{3} = 1$$

### (c)

$$f_X(x) = \frac{3}{8})(x + 1)^2, \qquad -1 < x < 1; \qquad Y = 1 - X^2 \; \text{ if } X \leq 0, \qquad Y = 1 - X \; \text{ if } X > 0$$
$$y = y(x) = \begin{cases} 1 - x^2 & \text{if } x \leq 0 \\ 1 - x & \text{if } x > 0  \end{cases} $$
$$x = x(y) = \begin{cases} -\sqrt{1 - y} \\ 1 - y  \end{cases}$$
$$\Bigg \lvert \frac{\partial x}{\partial y} \Bigg \lvert = \begin{cases} \frac{1}{2 \sqrt{1 - y}} \\ 1 \end{cases}$$
$$f_Y(y) = \frac{3}{8}(-\sqrt{1 - y} + 1)^2 \cdot \frac{1}{2 \sqrt{1 - y}} + \frac{3}{8}(1 - y + 1) \cdot 1$$
$$= \frac{3}{16} \frac{2 - y - 2 \sqrt{1 - y}}{\sqrt{1 - y}} + \frac{3}{8} (2 - y)^2 \qquad 0 < y < 1$$

## 2.7

Let $X$ have pdf $f_X(x) = \frac{2}{9}(x + 1), \qquad -1 \leq x \leq 2$

### (a)

Find the pdf of $Y = X^2$ (theorem 2.1.8 is not directly applicable):

$$P(Y \leq y) = P(X^2 \leq y) = \begin{cases} P(-\sqrt{y} \leq X \sqrt{y}) & \text{if } \lvert x \lvert \leq 1 \\  P(1 \leq X \leq \sqrt{y}) & \text{if } x \geq 1 \end{cases}$$
$$= \begin{cases} \int_{-\sqrt{y}}^{\sqrt{y}} f_X(x)dx & \text{if } \lvert x \lvert \leq 1 \\ \int_1^{\sqrt{y}} f_X(x) dx & \text{if } x \geq 1  \end{cases}$$
$$\int f_X(x) dx = \frac{2}{9} \int (x + 1) dx = \frac{2}{9} \bigg[ \frac{x^2}{2} + x \bigg]$$

$$\implies F_Y(y) = \begin{cases} \frac{2}{9} \bigg[ \frac{x^2}{2} + x \bigg]_{-\sqrt{y}}^{\sqrt{y}} & \text{if } y \leq 1 \\ \frac{2}{9} \bigg[ \frac{x^2}{2} + x \bigg]_{1}^{\sqrt{y}} & \text{if } y \geq 1  \ \end{cases}$$
$$= \begin{cases} \frac{4}{9} \sqrt{y} & \text{if } y \leq 1 \\ \frac{2}{9} \frac{y-3}{2} + \frac{2}{9} \sqrt{y} \qquad \text{if } y \geq 1  \end{cases}$$

Differentiate:

$$f_X(x) = \begin{cases} \frac{2}{9} \frac{1}{\sqrt{y}} & \text{if } y \leq 1 \\ \frac{1}{9} + \frac{1}{9} \frac{1}{\sqrt{y}} & \text{if } y \geq 1  \end{cases}$$

### (b)

If sets $B_1, B_2, \ldots B_K$ are partitions of the range of $Y$, we can write:

$$f_Y(y) = \sum_k f_Y(y) I(y \in B_k) $$
and then we can apply theorem 2.1.8 to each of the pieces $B_k$ and add them up. 

## 2.8

In each of the following show that the given function is a cdf and find $F^{-1}_X(y)$:

### (a)

$$F_X(x) = \begin{cases} 0 & \text{if } x < 0 \\ 1 - e^{-x} & \text{if } x \geq 0 \end{cases}$$

$$\lim_{x \to 0} 1-e^{-x} = 0 \qquad \lim_{x \to \infty} 1 - e^{-x} = 1$$

$F_X(x)$ is increasing in $x$, continuous.

$$F_X^{-1}(y) = -\log(1 - y)$$

### (b)

$$F_X(x) = \begin{cases} e^{x}/2 & \text{if } x < 0 \\ 1/2 & \text{if } 0 \leq x \leq 1 \\ 1 - e^{(1 - x)}/2 & \text{if } 1 \leq x \end{cases}$$

$$\lim_{x \to -\infty} e^x / 2 = 0 \qquad \lim_{x \to \infty} 1 - e^{1 - x}/2 = 1$$

$e^x/2$ is increasing in $x$, $\frac{1}{2}$ is non-decreasing, $1 - e^{1 - x} / 2$ is increasing in $x$.

$$F_X^{-1}(y) = \begin{cases} \log(2y) & \text{if } y \leq 1/2 \\ 1 - \log(2(1 - y)) & \text{if } y > 1/2 \end{cases}$$

### (c)

$$F_X(x) = \begin{cases} e^x / 4 & \text{if } x < 0 \\ 1 - (e^{-x} / 4) & \text{if } x \geq 0 \end{cases}$$

$$\lim_{x \to -\infty} e^x/4 = 0 \qquad \lim_{x \to \infty} 1 - (e^{-x} / 4) = 1$$

$F_X(x)$ is increasing in $x$.

$$F_X^{-1}(y) = \begin{cases} \log(4y) & \text{if } y \leq 1/4 \\ -\log(4(1 - y)) & \text{if } y > 1/4 \end{cases}$$

## 2.9

$X$ has a pdf:

$$f_X(x) = \begin{cases} \frac{x-1}{2} & \text{if } 1 < x < 3 \\ 0 & \text{otherwise} \end{cases}$$

Find a monotone function $u(x)$ such that $Y = u(x)$ has a $\text{Uniform}(0, 1)$ distribution.

If $u(x) = F_x(x)$ then $F_X(x) \sim \text{Uniform}(0, 1)$. Thus:

$$\implies y = y(x) = \int_1^x \frac{z - 1}{2} dz = \frac{1}{2} \bigg[ \frac{z^2}{2} - z \bigg]_1^x = \frac{1}{2} \bigg( \frac{x^2}{2} - x - \frac{1}{2} + 1 \bigg)$$
$$= \frac{1}{2} \bigg( \frac{x^2 - 2x + 1}{2} \bigg) = \frac{(x - 1)^2}{4}$$

Now:

$$y = y(x) = \frac{(x-1)^2}{4}$$
$$x = x(y) = \sqrt{4y} + 1$$
$$\Bigg \lvert \frac{\partial x}{\partial y} \Bigg \lvert = \frac{2}{\sqrt{4y}}$$
$$f_Y(y) = \frac{\sqrt{4y} + 1 - 1}{2} \cdot \frac{2}{\sqrt{4y}} = \frac{\sqrt{4y}}{\sqrt{4y}} = 1$$

## 2.10

Let $X$ be a discrete random variable with cdf $F_X(x)$ and define a random variable $Y$ as $Y = F_X(x)$

### (a)

Prove that $Y$ is stochastically greater than $\text{Uniform}(0, 1)$; that is, if $U \sim \text{Uniform}(0, 1)$, then:

$$P(Y > y) \geq P(U \geq y) = 1 - y \qquad \text{for all y}, 0 < y < 1$$
$$P(Y > y) > P(U > y) = 1 - y \qquad \text{for some y}, 0 < y < 1$$
DO SOME OTHER TIME

## 2.11

Let $X$ have the standard normal pdf, $f_X(x) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}}$

### (a)

Find $E(X^2)$ directly, and then by using the pdf of $Y = X^2$ and calculating $E(Y)$.

$$E(X^2) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty x^2  e^{-\frac{x^2}{2}} dx$$

Let $u = x, u' = 1, v = -\frac{e^{-\frac{x^2}{2}}}{2}, v' = xe^{-\frac{x^2}{2}}$

$$= \frac{1}{\sqrt{2 \pi}} \Bigg[ \bigg[ -xe^{-\frac{x^2}{2}} \bigg]_{-\infty}^{\infty} + \int_{-\infty}^{\infty} e^{-\frac{x^2}{2}} dz \Bigg]$$
$$= \frac{1}{\sqrt{2 \pi}} \bigg( 0 + \sqrt{2 \pi } \bigg)$$
$$= \frac{\sqrt{2 \pi}}{\sqrt{2 \pi}} = 1$$

We know that if $X \sim \text{N}(0, 1)$ and $Y = X^2$, then $Y \sim \chi^2_1$, $f_Y(y) = \frac{1}{\sqrt{2 \pi y}} e^{-\frac{y}{2}}$ for $0 < y < \infty$.

Then:

$$E(Y) = \int_0^\infty y \frac{1}{\sqrt{2 \pi y} } e^{-\frac{y}{2}} = \frac{1}{\sqrt{2 \pi}} \int_{0}^\infty y^{\frac{1}{2}}e^{-\frac{y}{2}} dy$$

Let $z = \frac{y}{2}$, $y = 2z$, $dy = 2 dz$:

$$= \frac{1}{\sqrt{2 \pi}} \int_{0}^\infty (2z)^{\frac{1}{2}}e^{-z} 2dz = \frac{2}{\sqrt{\pi}} \int_0^\infty z^{\frac{1}{2}}e^{-z} dz = \frac{2}{\sqrt{\pi}} \Gamma \bigg( \frac{1}{2} \bigg) = 1$$

(we could also note that the expectation of $\chi^2_k$ is $k$)

### (b)

Find the pdf of $Y = \lvert X \lvert$, and its mean and variance.

$$F_y(y) = P(Y \leq y) = P( \lvert X \lvert \leq y) = P(-y \leq X \leq y)$$
$$P(X \leq y) - P(X \leq - y) = F_X(y) - F_X(-y)$$
$$\implies F_Y(y) = \frac{\partial}{\partial y} F_X(y) - F_X(-y) = f_X(y) + f_X(-y) = \frac{1}{\sqrt{2 \pi}}e^{-\frac{y^2}{2}} + \frac{1}{\sqrt{2 \pi}}e^{-\frac{y^2}{2}} = \sqrt{\frac{2}{\pi}} e^{-\frac{y^2}{2}}$$

(there's a typo in the texbook solutions: $\sqrt{\frac{2}{\pi}} e^{-\frac{y}{2}}$ instead of $\sqrt{\frac{2}{\pi}} e^{-\frac{y^2}{2}}$)

$$E(Y) = \sqrt{\frac{2}{\pi}} \int_0^{\infty} ye^{-\frac{y^2}{2}} dy$$

Let $z = \frac{y^2}{2}$, $y = \sqrt{2z}$, $dy = \frac{1}{\sqrt{2z}}$:

$$= \sqrt{\frac{2}{\pi}} \int_0^{\infty} \sqrt{2z} e^{-z} \cdot \frac{1}{\sqrt{2z}} dz = \sqrt{\frac{2}{\pi}} \int_0^\infty e^{-z} dz = \sqrt{\frac{2}{\pi}}$$

$$E(Y^2) = \sqrt{\frac{2}{\pi}} \int_0^{\infty} y^2e^{-\frac{y^2}{2}} dy = \sqrt{\frac{2}{\pi}} \int_0^\infty 2ze^{-z} \cdot \frac{1}{\sqrt{2 z}} dz$$

$$\frac{2}{\sqrt{\pi}} \int_0^{\infty} z^{\frac{1}{2}} e^{-z} dz =  \frac{2}{\sqrt{\pi}} \Gamma \bigg( \frac{3}{2} \bigg) = \frac{2}{\sqrt{\pi}} \cdot \frac{\sqrt{\pi}}{2} = 1$$

$$\implies Var(Y) = 1 - \frac{2}{\pi}$$

## 2.12

A random right triangle can be constructed by drawing an angle $X \sim \text{Uniform}(0, \pi/2)$, and then drawing a line of a distance $d$ such that $X$ is the angle between it and the x-axis. Let $Y$ be the height of the random triangle. Find the distribution and the mean of $Y$.

$$\tan x = \frac{y}{d} \qquad \tan^{-1} \bigg( \frac{y}{d} \bigg) = x \qquad  \frac{\partial}{\partial y} \tan^{-1} \bigg( \frac{y}{d} \bigg) = \frac{1}{1 + (y / d)^2} \cdot \frac{1}{d}$$

Hence:

$$f_Y(y) = f_X(g^{-1}(y)) \Bigg \lvert \frac{\partial x}{\partial y} \Bigg \lvert = \frac{2}{\pi} \cdot \frac{1}{1 + (y / d)^2} \cdot \frac{1}{d} \qquad 0 < y < \infty$$

I.e. a half-Cauchy distribution, with an infinite mean.

## 2.13

Consider a sequence of coin flips, each of which has a probability $p$ of landing heads. Define $X$ as the length of the run of either heads or tails, starting with first trial. Find the distribution of $X$ and $E(X)$

$$f_X(x) = p^x(1-p) + (1-p)^x p$$
$$E(X) = \sum_{x = 1}^\infty x \bigg[ p^x(1-p) + (1-p)^x p \bigg]$$
$$= p(1-p) \bigg[ \sum_{x=1}^\infty xp^{x - 1} + \sum_{x = 1}^\infty x(1-p)^{x - 1} \bigg]$$
$$= p(1-p) \bigg[ \frac{\partial}{\partial p} \sum_{x=1}^\infty p^{x} + \frac{\partial}{\partial p} \sum_{x = 1}^\infty -(1-p)^{x} \bigg]$$
$$= p(1 - p) \bigg[ \frac{\partial}{\partial p} \bigg( \frac{1}{1 - p} - 1 \bigg) + \frac{\partial}{\partial p} \bigg( -\frac{1}{p} - 1 \bigg) \bigg]$$
$$= p(1 - p) \bigg[ \frac{\partial}{\partial p} \bigg( \frac{p}{1 - p} \bigg) + \frac{\partial}{\partial p} \bigg( -\frac{1 - p}{p} \bigg) \bigg]$$
$$= p(1 - p) \bigg[ \frac{(1 - p) + p}{(1 - p)^2} +  -\frac{-  p - (1 - p)}{p^2} \bigg]$$
$$= \frac{p}{1 - p} + \frac{1 - p}{p}$$

## 2.14

Let $X$ be a continuous, non-negative random variable ($f_X(x) = 0 \text{ for } x < 0)$. Show that:

$$\int_0^\infty [1 - F_X(x)] dx = \int_0^\infty P(X > x) dx$$
$$= \int_0^\infty \int_x^\infty f_X(y) dy dx$$
$$= \int_0^\infty \int_0^y dx f_X(y) dy \qquad \text{(changing the order of integration)}$$
$$= \int_0^\infty y f_X(y) dy = E(X)$$

This is possible by switching the order of integration and changing the limits: the area described by $x \leq y < \infty$ and $0 \leq x < \infty$ (the first set of limits), is the same as $0 \leq x \leq y$ and $0 \leq y \leq \infty$.

```{r, echo = FALSE}

plot(0:10, 0:10, type = 'n', xlab = 'x', ylab = 'y', axes = FALSE, asp = 1)
polygon(c(0, 10, 0), c(0, 10, 10), col = 'antiquewhite', border = NA)
text(3, 7, labels = expression({x <= y} < infinity), col = 'steelblue')
text(3, 6, labels = expression({0 <= x} < y), col = 'firebrick')
text(5, 0.5, labels = expression({0 <= x} < infinity), col = 'steelblue')
text(-2, 5, labels = expression({0 <= y} < infinity), col = 'firebrick')
axis(1, at = c(0, 10), labels = c(0, expression(infinity)), tick = FALSE)
axis(2, at = 10, labels = expression(infinity), las = 1, tick = FALSE)
box(bty = 'l', col = 'grey80')

```

### (b)

Let $X$ be a discrete random variable whose range is the non-negative integers. Show that:

$$E(X) = \sum_{x = 0}^\infty (1 - F_X(x))$$

$$\sum_{x = 0}^\infty (1 - F_X(x)) = \sum_{x = 0}^\infty P(X > x)$$
$$= \sum_{x=0}^\infty \sum_{k = x + 1}^\infty f_X(k)$$ 
$$= \sum_{k = \infty}^0 \sum_{x = 0}^k f_X(x) \qquad \text{(each } k \text{ gets counted } x = k \text{ times, starting from the end)}$$

$$\sum_{k=0}^\infty k f_X(k) = E(X)$$

## 2.15

Let $X$ and $Y$ be any two random variables and define:

$$X \wedge Y = \min(X, Y) \qquad X \vee Y = \max(X, Y)$$
Show that: 

$$E(X \wedge Y) = E(X) + E(Y) - E(X \vee Y)$$
Starting from the assumption that sum of $X$ and $Y$ is equal to the sum of their respective minimum and maximum:

$$E(X + Y) = E(X \wedge Y) + E(X \vee Y)$$
$$\implies E(X) + E(Y) = E(X \wedge Y) + E(X \vee Y)$$
$$\implies E(X \vee Y) = E(X) + E(Y) - E(X \wedge Y)$$
## 2.16

Use the result of Exercise 2.14 to find the mean duration of certain telephone calls, where we assume that the duration, $T$, of a particular call can be described probabilistically by $P(T > t) = ae^{-\lambda t} + (1 - a)e^{-\mu t}$, where $0 < a < 1$, $\lambda > 0$, $\mu > 0$.

$$E(T) = \int_0^\infty [1 - F_T(t)] dt = \int_0^\infty ae^{-\lambda t} + (1 - a)e^{-\mu t} dt$$
$$= a\int_0^\infty e^{-\lambda t} dt + (1- a) \int_0^\infty e^{-\mu t} dt$$
$$= a \bigg[ - \frac{e^{-\lambda t}}{\lambda} \bigg]_0^\infty + (1 - a) \bigg[ -\frac{e^{-\mu t}}{\mu} \bigg]_0^\infty$$
$$= \frac{a}{\lambda} + \frac{1 - a}{\mu}$$

## 2.17

A median of a distribution is a value $m$ such that $P(X \leq m) \geq \frac{1}{2}$ and $P(X \geq m) \leq \frac{1}{2}$. Find the median of the following distributions:

### (a)

$$f_X(x) = 3x^2, \qquad 0 < x < 1$$
$$F_X(x) = \int_0^x 3y^2 dy = 3 \bigg[ \frac{y^3}{3} \bigg]_0^x = x^3 \qquad 0 < x < 1$$

$$F_X(m) = \frac{1}{2}$$
$$\implies m^3 = \frac{1}{2}$$
$$\implies m = \sqrt[3]{\frac{1}{2}} \approx 0.79$$

### (b)

$$f_X(x) = \frac{1}{\pi (1 + x^2)}, \qquad -\infty < x < \infty$$

The pdf is symmetric around 0, so the median will be 0, as long as the function is a valid pdf. Check if integrates to 1:

$$\frac{1}{\pi} \int_{-\infty}^\infty \frac{1}{1 + x^2} dx = \frac{1}{\pi} \bigg[ \tan^{-1}(x) \bigg]_{-\infty}^\infty = \frac{1}{\pi} \bigg( \frac{\pi}{2} - \bigg(- \frac{\pi}{2} \bigg) \bigg) = 1$$

## 2.18

Show that if $X$ is a continuous R.V., then $\text{min}_a E \lvert X - a \lvert = E \lvert X - m \lvert$

where $m$ is the median of $X$.

$$E(\lvert X - a \lvert ) = \int_{-\infty}^a -(x - a) f_X(x) dx + \int_{a}^\infty (x - a) f_X(x) dx$$
$$\frac{\partial}{\partial a} \int_{-\infty}^a -(x - a) f_X(x) dx + \int_{a}^\infty (x - a) f_X(x) dx = \int_{-\infty }^a f_X(x) dx - \int_{a}^\infty f_X(x) dx$$
$$\implies \text{Minimized if } \int_{-\infty }^a f_X(x) dx = \int_{a}^\infty f_X(x) dx$$
$$\implies a \text{ is the median}$$

## 2.19

Prove that $\frac{\partial}{\partial a} E(X - a)^2 = 0 \iff E(X) = a$

$$E(X - a)^2 = \int_{-\infty}^\infty (x - a)^2 f_X(x) dx$$

$$\frac{\partial}{\partial a} \int_{-\infty}^\infty (x - a)^2 f_X(x) dx = \int_{-\infty}^\infty (x - a)^2 f_X(x) dx$$
$$= -\int_{-\infty}^\infty 2x f_X(x)dx + \int_{-\infty}^\infty 2a f_X(x) dx$$
$$= -2 E (X) + 2a \int_{-\infty}^\infty f_X(x) dx = -2 E (X) + 2a$$
$$\implies \text{Will be zero iff } a = E(X)$$

## 2.20

A couple decides to continue having children until a daughter is born. What are the expected number of children of this couple?

$$E(X) = 1 + \sum_{k = 0}^\infty k(1 - p) p^k$$
$$= 1 + (1 - p)  \sum_{k = 0}^\infty k p^k$$
$$= 1 + (1 - p)p  \sum_{k = 0}^\infty k p^{k-1}$$
$$= 1 + (1 - p)p \frac{\partial}{\partial p}  \sum_{k = 0}^\infty p^{k}$$
$$= 1 + (1 - p)p  \frac{\partial}{\partial p} \frac{1}{1 - p}$$
$$= 1 + (1 - p) p \cdot \frac{1}{(1 - p)^2} = 1 + \frac{p}{1 - p} = \frac{1 - p + p}{1 - p} = \frac{1}{1 - p}$$

If we assume the probability of daughter born $= \frac{1}{2}$, then $E(X) = 2$

## 2.21

Prove the "two-way" rule for expectations: $E(g(X)) = E(Y)$ where $Y = g(X)$. Assume $g(x)$ is a monotone function.

$$E(g(X)) = \int_{-\infty}^\infty g(x) f_X(x) dx$$
$$E(Y) = \int_{-\infty}^\infty y f_Y(y) dy = \int_{-\infty}^\infty g(x) f_X(g^{-1}(y)) \frac{dx}{dy} dy = \int_{-\infty}^\infty g(x) f_X(x) dx$$

## 2.22

Let $X$ have pdf:

$$f_X(x) = \frac{4}{\beta^3 \sqrt{\pi}} x^2 e^{-x^2 / \beta^2} \qquad 0 < x < \infty, \qquad \beta > 0$$

### (a) 

Verify that $f_X(x)$ is a pdf:

$$\int_0^\infty \frac{4}{\beta^3 \sqrt{\pi}} x^2 e^{-x^2 / \beta^2} dx = \frac{4}{\beta^3 \sqrt{\pi}} \int_0^\infty x^2 e^{-x^2 / \beta^2} dx$$

Let $z = \frac{x^2}{\beta^2}$:

$$x = \sqrt{\beta^2 z} \qquad \text{(since } x \text{ is positive)}$$
$$dx = \frac{\beta}{2 \sqrt{z}} dz$$

$$\implies \frac{4}{\beta^3 \sqrt{\pi}} \int_0^\infty \beta^2 z e^{-z} \frac{\beta}{2 \sqrt{z}} dz = \frac{2}{\sqrt{\pi}} \int_0^\infty z^{\frac{1}{2}}e^{-z}$$
$$= \frac{2}{\sqrt{\pi}} \cdot \Gamma \bigg(\frac{3}{2} \bigg) = \frac{2}{\sqrt{\pi}} \cdot \frac{\sqrt{\pi}}{2} = 1$$

### (b)

Find the mean and variance of $X$:

$$E(X) = \int_0^\infty x f_X(x) dx = \frac{4}{\beta^3 \sqrt{\pi}} \int_0^\infty x^3 e^{-x^2 / \beta^2} dx$$

Let $z = \frac{x^2}{\beta^2}$:

$$x = \sqrt{\beta^2 z} \qquad \text{(since } x \text{ is positive)}$$
$$x^3 = \beta^3 z^{\frac{3}{2}}$$
$$dx = \frac{\beta}{2 \sqrt{z}} dz$$

$$\implies E(X) = \frac{4}{\beta^3 \sqrt{\pi}} \int_0^\infty  \beta^3 z^{\frac{3}{2}} e^{-z} \frac{\beta}{2 \sqrt{z}} dx$$
$$= \frac{2 \beta}{\sqrt{\pi}} \cdot \Gamma(2) = \frac{2 \beta}{\sqrt{\pi}}$$

$$\implies E(X^2) = \frac{2 \beta^2}{\sqrt{\pi}} \cdot \Gamma \bigg( \frac{5}{2} \bigg) = \frac{2 \beta^2}{\sqrt{\pi}} \cdot \frac{3 \sqrt{\pi}}{4} = \frac{3 \beta^2}{2}$$

$$Var(X) = E(X^2) - E(X)^2 = \frac{3 \beta^2}{2} - \bigg( \frac{2 \beta}{\sqrt{\pi}} \bigg)^2 = \frac{3 \beta^2}{2} - \frac{4 \beta^2}{\pi} = \frac{\beta^2(3 \pi - 8)}{2 \pi} $$

## 2.23 

Let $X$ have pdf: 

$$f_X(x) = \frac{1}{2} (1 + x) \qquad -1 < x < 1$$

### (a)

Find the pdf of $Y = X^2$:

$$f_Y(y) = f_X(g^{-1}(y)) \Bigg\lvert \frac{dx}{dy} \Bigg\lvert$$

$$y = g(x) = x^2$$
$$x = g^{-1}(y) = \pm \sqrt{y}$$
$$dx = \pm \frac{1}{2 \sqrt{y}} dy$$
$$\Bigg\lvert \frac{dx}{dy} \Bigg\lvert = \frac{1}{2 \sqrt{y}}$$

$$\implies f_Y(y) = \frac{1}{2}(1 - \sqrt{y}) \cdot \frac{1}{2 \sqrt{y}} + \frac{1}{2}(1 + \sqrt{y}) \cdot \frac{1}{2 \sqrt{}y} = \frac{1 - \sqrt{y} + 1 + \sqrt{y}}{4 \sqrt{y}}$$
$$= \frac{1}{2 \sqrt{y}} \qquad \text{for } 0 < y < 1$$


### (b)

Find the mean and variance of $y$:

$$E(Y) = \int_{0}^1 y\frac{1}{2 \sqrt{y}} dy = \frac{1}{2} \int_{0}^1 \sqrt{y} dy = \frac{1}{2} \bigg[ \frac{2y^{\frac{3}{2}}}{3} \bigg]_0^1 = \frac{1}{3}$$
$$E(Y^2) = \frac{1}{2} \int_0^1 y^{\frac{3}{2}} dy = \frac{1}{2} \bigg[ \frac{2y^{\frac{5}{2}}}{5} \bigg] = \frac{1}{5}$$
$$Var(Y) = E(Y^2) - E(Y)^2 = \frac{1}{5} - \bigg( \frac{1}{3} \bigg)^2 = \frac{1}{5} - \frac{1}{9} = \frac{9 - 5}{45} = \frac{4}{45}$$

## 2.24

Compute $E(X)$ and $Var(X)$ for each of the following pdfs:

### (a)

$$f_X(x) = ax^{a - 1} \qquad 0 < x < 1, \qquad a > 0$$

$$E(X) = \int_0^1 xax^{a - 1} dx = a \int_0^1 x^a dx = a \bigg[ \frac{x^{a + 1}}{a + 1} \bigg]_0^1 = \frac{a}{a + 1}$$
$$E(X^2) = \ldots = \frac{a}{a + 2}$$
$$Var(X) = E(X^2) - E(X)^2 = \bigg( \frac{a}{a + 2} \bigg) - \bigg( \frac{a}{a + 1} \bigg)^2$$

### (b)

$$f_X(x) = \frac{1}{n}, \qquad x = 1, 2, \ldots, \qquad n > 0$$

$$E(X) = \sum_{x = 1}^n \frac{x}{n} = \frac{1}{n} \bigg( \frac{n(n + 1)}{2} \bigg) = \frac{n + 1}{2}$$
$$E(X^2) = \sum_{x = 1}^n \frac{x^2}{n} = \frac{1}{n} \bigg( \frac{n(n + 1)(2n + 1)}{6} \bigg) = \frac{(n + 1)(2n + 1)}{6}$$
$$Var(X) = \frac{(n + 1)(2n + 1)}{6} - \bigg( \frac{n + 1}{2} \bigg)^2 = \frac{(n + 1)(2n + 1)}{6} - \frac{(n + 1)^2}{4}$$
$$= \frac{2(n + 1)(2n + 1) - 3(n + 1)^2}{12}$$
$$= \frac{(n + 1)[4n + 2 - 3n - 3]}{12}$$
$$= \frac{n^2 - 1}{12}$$

(there's a typo in the texbook solutions: $\frac{n^2 + 1}{12}$ instead of $\frac{n^2 - 1}{12}$)

### (c)

$$f_X(x) = \frac{3}{2}(x - 1)^2, \qquad 0 < x < 2$$
$$E(X) = \frac{3}{2} \int_0^2 x(x - 1)^2 dx = \int_0^2 x^3 - 2x^2 = x dx = \frac{3}{2} \bigg[ \frac{x^4}{4} - \frac{2x^3}{3} + \frac{x^2}{2} \bigg]_0^2$$
$$= \frac{3}{2} \bigg( \frac{16}{4} - \frac{16}{3} + \frac{4}{2} \bigg)$$
$$= \frac{3}{2} \bigg( \frac{48 - 64 + 24}{12} \bigg) = \frac{3}{2} \cdot \frac{8}{12} = 1$$

$$E(X^2) = \int_0^2 x^4 - 2x^3 + x^2 dx = \frac{3}{2} \bigg[ \frac{x^5}{5} - \frac{2x^4}{4} + \frac{x^3}{3} \bigg]_0^2$$
$$= \frac{3}{2} \bigg( \frac{32}{5} + \frac{32}{4} + \frac{8}{3} \bigg)$$
$$= \frac{3}{2} \bigg( -\frac{32}{20} + \frac{8}{3} \bigg)$$
$$= \frac{3}{2} \bigg(\frac{8}{3} - \frac{8}{5} \bigg)$$
$$= \frac{3}{2} \bigg( \frac{40 - 24}{15} \bigg) = \frac{3}{2} \cdot \frac{16}{15} = \frac{8}{5}$$
$$Var(X) = \frac{8}{5} - 1^2 = \frac{3}{5}$$

## 2.25

Suppose the pdf $f_X(x)$ of a random variable $X$ is an even function ($f_X(x) = f_X(-x)$). Show that:

### (a)

$X$ and $-X$ are identically distributed:

Let $Y = -X$, $g^{-1}(y) = -y$. Then:

$$f_Y(y) = f_X(g^{-1}(y)) \Bigg\lvert \frac{\partial x}{\partial y} \Bigg\lvert = f_X(-x) \cdot 1 = f_X(x)$$
### (b)

$M_X(t)$ is symmetric around zero.

$$M_X(0 + \epsilon) = \int_{-\infty}^\infty e^{(0 + \epsilon)x} f_X(x) dx$$
$$= \int_{-\infty}^0 e^{\epsilon x} f_X(x) dx + \int_0^\infty e^{\epsilon x} f_X(x) dx$$
$$= \int_0^\infty e^{-\epsilon x} f_X(-x) dx  + \int_{-\infty}^0 e^{-\epsilon x} f_X(-x) dx $$
$$= \int_{-\infty}^\infty e^{(0 - \epsilon) x} f_X(-x) dx$$
$$= M_X(0 - \epsilon)$$

## 2.26

Let $f_X(x)$ be a pdf and let $a$ be a number such that, for all $\epsilon > 0$, $f(a + \epsilon) = f(a - \epsilon)$. Such pdf is said to be symmetric around point $a$.

### (a)

Give three examples of such pdfs:

$\text{Normal}(0, 1)$ is symmetric around $a = 0$, $\text{Normal}(\mu, \sigma^2)$ is symmetric around $a = \mu$, $\text{Uniform}(a, b)$ is symmetric around $a = (a + b)/2$.

### (b)

Show that if $X$ is symmetric around $a$, then $a$ is the median of $X$.

$$\int_a^\infty f_X(x) dx$$
Let $\epsilon = x - a, \qquad 0 < \epsilon < \infty$. Then:

$$\int_a^\infty f_X(x) dx = \int_0^\infty f_X(a + \epsilon) d \epsilon$$
$$= \int_0^\infty f_X(a - \epsilon) d \epsilon$$
$$= \int_{-\infty}^a f_X(x) dx$$

Now:

$$\int_{-\infty}^a f_X(x) dx + \int_a^\infty f_X(x) dx = 1$$
and:

$$\int_{-\infty}^a f_X(x) dx = \int_a^\infty f_X(x) dx$$

$$\implies \int_{-\infty}^a f_X(x) dx = \int_a^\infty f_X(x) dx = \frac{1}{2}$$

### (c)

Show that if $f_X(x)$ is symmetric and $E(X)$ exists, then $E(X) = a$

$$E(X - a) = \int_{-\infty}^\infty (x - a) f_X(x) dx = \int_{-\infty}^a (x - a) f_X(x) dx + \int_a^{\infty} (x - a) f_X(x)dx$$
$$= \int_0^\infty - \epsilon f_X(a - \epsilon) d \epsilon + \int_0^\infty \epsilon f_X(a + \epsilon) d \epsilon$$
$$= \int_0^\infty - \epsilon f_X(a - \epsilon) d \epsilon + \int_0^\infty \epsilon f_X(a - \epsilon) d \epsilon \qquad (f_X(a - \epsilon) \text{ and } f_X(a + \epsilon) \text{ are the same})$$
$$= 0$$

$$\implies E(X - a) = E(X) - a = 0 \implies E(X) = a$$

### (d)

Show that $f_X(x) = e^{-x}, \qquad x > 0$ is not a symmetric pdf:

$$f_X(a - \epsilon) = e^{-(a - \epsilon)} \neq e^{-(a + \epsilon)} = f_X(a + \epsilon) \qquad \text{for all } a \text{ and } \epsilon > 0$$

### (e)

Show that for the pdf in part (d), the median is less than the mean:

$$F_X(x) = 1 - e^{-x}$$
$$m = F_X^{-1}(1/2)$$
$$\implies 1 - e^{-m} = \frac{1}{2}$$
$$\implies e^{-m} = \frac{1}{2}$$
$$\implies m = - \log \bigg( \frac{1}{2} \bigg) = \log(2) \approx 0.69$$

$$E(X) = \int_0^\infty xe^{-x} dx = \bigg[ -xe^{-x} \bigg]_0^\infty - \int_0^\infty -e^{-x}  = 0 + \bigg[ -e^{-x} \bigg]_{0}^\infty = (-0 - (-1)) = 1$$

## 2.27

Let $f(x)$ be a pdf and let $a$ be a number such that if $a \geq x \geq y$ then $f(a) \geq f(x) \geq f(y)$ and if $a \leq x \leq y$ then $f(a) \geq f(x) \geq f(y)$. Such pdf is called unimodal with mode equal to $a$.

### (a)

Give example of a unimodal pdf for which the mode is unique:

Standard normal, normal, Cauchy, Gamma, etc...

### (b)

Give example of a unimodal pdf for which the mode is not unique:

Uniform

### (c)

Show that if $f_X(x)$ is both symmetric and unimodal, the point of symmetry is the mode.

Let $a = m + \epsilon$ be some point at distance $\epsilon > 0$ from the mode $m$.

$$f(m) > f(m + \epsilon) \geq f(m + 2 \epsilon)$$
$$\implies f(a - \epsilon) > f(a) \geq f(a + \epsilon)$$
$$\implies f(a - \epsilon) > f(a + \epsilon)$$
$$\implies \text{pdf is NOT symmetric}$$
Thus, $a$ has to be the mode. 


### (d)

Consider the pdf $f_X(x) = e^{-x}, \qquad x > 0$. Show that this pdf is unimodal. What is the mode?

$f(0) > f(x) > f(y)$ for all $0 < x < y$. Thus, $f_X(x)$ is unimodal and 0 is the mode.

## 2.28

Let $\mu_n$ denote the $n$th central moment of a random variable $X$. Two quantities of interest, in addition to mean and variance, are:

$$\alpha_3 = \frac{\mu_3}{(\mu_2)^{3/2}} \qquad \text{and} \qquad \alpha_4 = \frac{\mu_4}{\mu_2^2}$$

$\alpha_3$ is called skewness and $\alpha_4$ is called kurtosis. 

### (a)

Show that if a pdf is symmetric about a point $a$, then skewness $\alpha_3 = 0$.

$$\int_{-\infty}^\infty (x - a)^3 f_X(x) dx = \int_{-\infty}^a (x - a)^3 f_X(x) dx + \int_a^{\infty} (x - a)^3 f_X(x) dx$$
$$= \int_{-\infty}^0 \epsilon^3 f_X(\epsilon + a) d \epsilon + \int_0^\infty \epsilon^3 f_X(\epsilon + a) d \epsilon$$

$$= \int_0^\infty -\epsilon^3 f_X(-\epsilon + a) d \epsilon + \int_0^\infty \epsilon^3 f_X(\epsilon + a) d \epsilon$$
$$= \int_0^\infty -\epsilon^3 f_X(\epsilon + a) d \epsilon + \int_0^\infty \epsilon^3 f_X(\epsilon + a) d \epsilon \qquad \text{(since } f_X(a + \epsilon) = f_X(a - \epsilon))$$

$$= 0$$

### (b)

Calculate $\alpha_3$ for $f_X(x) = e^{-x}, \qquad x \geq 0$, a pdf that is skewed to the right.

$$\mu_1 = E(X) = \int_0^\infty xe^{-x} dx = \bigg[ -xe^{-x} \bigg]_0^\infty - \int_0^\infty -e^{-x}  = 0 + \bigg[ -e^{-x} \bigg]_{0}^\infty = (-0 - (-1)) = 1$$
$$\mu_2 = \int_0^\infty (x - 1)^2 e^{-x} dx = \int_0^\infty x^2 e^{-x} dx - 2 \int_0^\infty xe^{-x} + \int_0^\infty e^{-x} dx$$
$$= \bigg[ -x^2e^{-x} \bigg]_{0}^\infty -2 \int_{0}^\infty -xe^{-x} dx - 2 + 1$$
$$= 0 + 2 - 2 + 1 = 1$$

$$\mu_3 = \int_0^\infty (x - 1)^3 e^{-x} dx = \int_0^\infty x^3 e^{-x} dx - 3\int_0^\infty x^2 e^{-x} dx + 3 \int_0^1 xe^{-x} dx - \int_0^\infty e^{-x} dx$$
$$\Gamma(4) - 3 \Gamma(3) + 3 \Gamma(2) - \Gamma(1)$$
$$= 6 - 3 \cdot 2 + 3 - 1 = 2$$

$$\alpha_3 = \frac{\mu_3}{(\mu_2)^{3/2}} = \frac{2}{1} = 2$$

### (c)

Calculate $\alpha_4$ for each of the following pdfs and comment on the peakedness of each:

#### i.

$$f_X(x) = \frac{1}{\sqrt{2 \pi}} e^{-x^2 / 2} \qquad - \infty < x < \infty$$

Since the mean $\mu$ is zero, and the second moment $\sigma^2$ is 1, we just need the fourth moment, $E(X^4)$. Let $W = X^2 \implies E(X^4) = E(W^2)$. Then:

$$E(W^2) = Var(W) + E(W)^2$$

$W$ is a Chi-squared distribution with one degree of freedom, so its expectation is 1 and its variance is 2. Thus:

$$\alpha_4 = mu_4 = E(X^4) = E(W^2) = 2 + 1^3 = 3$$

#### ii.

$$f_X(x) = \frac{1}{2}, \qquad -1 < x < 1$$
$$\mu = E(X) = 0$$
$$\mu_2 = \frac{1}{2} \int_{-1}^1 x^2 = \frac{1}{2} \bigg[ \frac{x^3}{3} \bigg]_{-1}^1 = \frac{1}{2} \bigg( \frac{1}{3} - \bigg( -\frac{1}{3} \bigg) \bigg) = \frac{1}{3}$$
$$\mu_4 = \frac{1}{2} \int_{-1}^1 x^4 dx = \frac{1}{2} \bigg[ \frac{x^5}{5} \bigg] = \frac{1}{5}$$
$$\alpha_4 = \frac{\mu_4}{\mu_2^2} = \frac{\frac{1}{5}}{ \big( \frac{1}{3} \big)^2} = \frac{9}{5}$$

#### iii.

$$f_X(x) = \frac{1}{2}e^{- \lvert x \lvert}, \qquad -\infty < x < \infty$$
$$\mu = E(X) = 0$$
$$\mu_2 = \frac{1}{2} \int_{-\infty}^\infty x^2e^{- \lvert x \lvert} dx  = \frac{1}{2} \int_{-\infty}^0 x^2e^x dx + \frac{1}{2} \int_0^\infty x^2e^{-x} dx$$
$$= \int_0^\infty x^2 e^{-x} dx \qquad \text{(by symmetry)} $$
$$= \Gamma(3) = 2$$
$$\mu_4 = \frac{1}{2} \int_{-\infty}^\infty x^4 e^{- \lvert x \lvert} dx  = \Gamma(5) = 24$$
$$\alpha_4 = \frac{\mu_4}{\mu_2^2} = \frac{24}{2^2} = 6$$

## 2.29

To calculate moments of discrete distributions, it is often easier to work with the factorial moments.

### (a)

Calculate the factorial moment $E[X(X-1)]$ for the Binomial and Poisson distribution.

For Binomial:

$$E[X(X - 1)] = \sum_{x=0}^n x(x-1) {n \choose x} p^x (1 - p)^{n - x}$$
$$= \sum_{x = 0}^n x(x - 1) \frac{n!}{x! (n - x)!} p^x (1 - p)^{n - x}$$
$$= n(n - 1)p^2 \sum_{x =0}^n \frac{(n-2)!}{(x - 2)! (n - x)!} p^{x - 2} (1 - p)^{n - x}$$
$$= n(n-1)p^2$$

For Poisson:

$$E[X(X - 1)] = \sum_{x = 0}^\infty x(x-1) \frac{\lambda^x e^{-\lambda}}{x!}$$
$$= \lambda^2 \sum_{x = 0}^\infty \frac{\lambda^{x - 2} e^{\lambda}}{(x - 2)!} = \lambda^2$$

### (b)

Use the results from (a) to calculate variances.

For Binomial:

$$Var(X) = E(X^2) - E(X)^2 = E[X(X - 1]) - E(X)^2 + E(X)$$
$$= n(n-1)p^2 - n^2 p^2 + np$$
$$= n[(n - 1)p^2 - np^2 + p]$$
$$= n[p - p^2] = np(1 - p)$$
For Poisson:

$$Var(X) = \lambda^2 - \lambda^2 + \lambda = \lambda$$

### (c)

A particularly nasty discrete distribution is Beta-Binomial, with pmf:

$$P(Y = y) = a(y + a) \frac{{n \choose y} {a + b - 1 \choose a}}{{n + a + b - 1 \choose y + a}}$$
Use factorial moments to calculate the variance of Beta-Binomial:

$$E[Y(Y-1)] = \sum_{y = 0}^n y(y - 1) a (y + a) \frac{{n \choose y} {a + b - 1 \choose a}}{{n + a + b - 1 \choose y + a}}$$
$$= \frac{a^2 n(n-1)}{(y + a)(y - 1 + a)} \sum_{y = 0}^n a(y + a - 2) \frac{{n-2 \choose y-2} {a + b - 1 \choose a}}{{n + a + b - 1 \choose y - 2 + a}} $$

DO SOME OTHER TIME

## 2.30

Find the moment generating functions for the following pdfs:

### (a)

$$f_X(x) = \frac{1}{c}, \qquad 0 < x < c$$

$$M_X(t) = \frac{1}{c} \int_{0}^c e^{tx} dx = \frac{1}{c} \bigg[ \frac{e^{tx}}{t} \bigg]_0^c = \frac{1}{c} \cdot \frac{e^{tc} - 1}{t} = \frac{e^{tc} - 1}{ct}$$
### (b)

$$f_X(x) = \frac{2x}{c^2}, \qquad 0 < x < c$$

$$M_X(t) = \frac{2}{c^2} \int_{0}^c xe^{tx} dx = \frac{2}{c^2} \Bigg[ \bigg[ xe^{tx} \bigg]_0^c - \int_0^c e^{tx} dx \Bigg]$$
$$= \frac{2}{c^2} \Bigg[ ce^{tc} - \bigg[ e^{tx} \bigg]_0^c \Bigg]$$
$$= \frac{2}{c^2} \bigg( ce^{tc} - e^{tc} + 1 \bigg) = \frac{2(ce^{tc} - e^{tc} + 1)}{c^2}$$

### (c)

$$f_X(x) = \frac{1}{2 \beta} e^{- \lvert x - \alpha \lvert / \beta}, \qquad -\infty < x < \infty, \; -\infty < \alpha < \infty, \; \beta > 0$$
$$M_X(t) = \frac{1}{2 \beta} \int_{-\infty}^\alpha e^{tx} e^{(x - \alpha) / \beta} dx + \frac{1}{2 \beta} \int_\alpha^\infty e^{tx} e^{(\alpha - x) / \beta} dx$$
$$= \frac{e^{-\alpha/\beta}}{2 \beta} \int_{-\infty}^\alpha e^{ x / \beta + xt} dx + \frac{e^{-\alpha/\beta}}{2 \beta} \int_{\alpha}^\infty e^{-x / \beta + xt} dx$$
$$= \frac{e^{-\alpha/\beta}}{2 \beta} \bigg[ \frac{e^{x(1 / \beta + t)}}{1/\beta + t} \bigg]_{-\infty}^\alpha + \frac{e^{-\alpha/\beta}}{2 \beta}  \bigg[ \frac{e^{x(-1 / \beta + t)}}{-1/\beta + t} \bigg]_{-\infty}^\alpha$$
$$= \frac{e^{-\alpha/\beta}}{2 \beta} \cdot \frac{e^{\alpha (1 / \beta + t)}}{1 / \beta + t} - \frac{e^{-\alpha \beta}}{2 \beta} \cdot \frac{e^{\alpha(-1 / \beta + t)}}{1 / \beta - t}$$

### (d)

$$f_X(x) = {r + x - 1 \choose x} p^r ( 1 - p)^x, \qquad x = 0, 1, \ldots, \; 0 < p < 1, \; r > 0$$
$$M_X(t) = \sum_{x = 0}^\infty e^{tx} {r + x - 1 \choose x} p^r ( 1 - p)^x $$
$$= p^r \sum_{x = 0}^\infty {r + x - 1 \choose x} [(1 - p)e^t]^x$$
$$= p^r \cdot \frac{1}{\big(1 - (1 - p)e^t \big)^r} \qquad \text{(normalizing constant for the original pmf)}$$
$$\bigg( \frac{p}{1 - (1 -p)e^t} \bigg)^r \qquad t < - \log(1 - p)$$

## 2.31

Does a distribution exist for which $M_X(t) = t / (1 - t), \; \lvert t \lvert < 1$. If yes, find it. If no, prove it.

$$M_X(0) = E(e^{0}) = 1$$
However, $\frac{0}{1 - 0} = 0$, therefore it cannot be an mgf. 

## 2.32

Let $M_X(t)$ be the moment generating function of $X$ and definte $S(t) = \log(M_X(t))$. Show that:

$$\frac{\partial}{\partial t} S(t) \Bigg\lvert_{t = 0} = E(X) \qquad \text{and} \qquad \frac{\partial^2}{\partial t^2} S(t) \Bigg\lvert_{t = 0} = Var(X)$$

$$\frac{\partial}{\partial t} S(t) = \frac{\partial}{\partial t} \log(M_X(t)) = \frac{\frac{\partial}{\partial t} M_X(t)}{M_X(t)} \Bigg\lvert_{t = 0} = \frac{E(X)}{1} = E(X)$$
$$\frac{\partial^2}{\partial t^2} S(t) = \frac{\partial^2}{\partial t^2} \log(M_X(t)) = \frac{\partial}{\partial t} \frac{\frac{\partial}{\partial t} M_X(t)}{M_X(t)} = \frac{\frac{\partial^2}{\partial t^2} M_X(t) \cdot M_X(t) - \big[ \frac{\partial}{\partial t} M_X(t) \big]^2}{\big[ M_X(t) \big]^2} \Bigg\lvert_{t=0} = \frac{E(X^2) - E(X)^2}{1^2} = Var(X)$$

## 2.33

In each of the following cases verify the expression given for the moment generating function, and in each case use mgf to calculate $E(X)$ and $Var(X)$:

### (a)

$$f_X(x) = \frac{e^{-\lambda} \lambda^x}{x!}, \qquad M_X(t) = e^{\lambda(e^t - 1)}, \qquad x = 0, 1, \ldots \; \lambda >0$$
$$M_X(t) = \sum_{x = 0}^\infty e^{tx} \frac{e^{-\lambda} \lambda^x}{x!} = e^{-\lambda} \sum_{x = 0}^\infty \frac{(\lambda e^t)^x}{x!} = e^{-\lambda} e^{\lambda e^t} = e^{\lambda(e^t - 1)}$$
$$E(X) = \frac{\partial}{\partial t} M_X(t) \Bigg \lvert_{t = 0} = e^{\lambda(e^t - 1)}  \lambda e^t \Bigg \lvert_{t = 0} = \lambda$$
$$E(X) = \frac{\partial^2}{\partial t^2} M_X(t) \Bigg \lvert_{t = 0} = \frac{\partial}{\partial t} \lambda e^{\lambda(e^t - 1) + t}   \Bigg \lvert_{t = 0} = \lambda e^{\lambda(e^t - 1) + t} \lambda e^t +  \lambda e^{\lambda(e^t - 1) + t}   \Bigg \lvert_{t = 0} = \lambda^2 + \lambda$$
$$Var(X) = \lambda^2 + \lambda - \lambda^2 = \lambda$$

### (b)

$$f_X(x) = p(1-p)^x, \qquad M_X(t) = \frac{p}{1 - (1 - p)e^t}, \qquad x = 0, 1, \ldots, \; 0 < p < 1$$
$$M_X(t) = \sum_{x = 0}^\infty e^{tx} p(1 - p)^x = p \sum_{x = 0}^\infty [(1 - p)e^t]^x = \frac{p}{1 - (1 - p)e^t} \qquad t < -\log(1 - p)$$
$$E(X) = \frac{\partial}{\partial t} \frac{p}{1 - (1 - p)e^t} \Bigg \lvert_{t = 0} = p \frac{(1 - p)e^t}{[1 - (1 - p)e^t]^2} \Bigg \lvert_{t = 0} = \frac{p(1- p)}{p^2} = \frac{1 - p}{p}$$
$$E(X^2) = \frac{\partial}{\partial t} p \frac{(1 - p)e^t}{[1 - (1 - p)e^t]^2} \Bigg \lvert_{t = 0} = p(1 - p) \frac{e^t[1 - (1 - p)e^t]^2 + 2[1 - (1 - p)e^t](1 - p)e^t}{[1 - (1 - p)e^t]^4} \Bigg \lvert_{t = 0}$$
$$= p(1 - p) \frac{p^2 + 2p(1 - p)}{p^4} = \frac{(1 - p)p + 2(1 - p)^2}{p^2}$$
$$Var(X) = \frac{(1 - p)p + 2(1 - p)^2}{p^2} - \bigg( \frac{1 - p}{p} \bigg)^2 =  \frac{(1 - p)p + 2(1 - p)^2 - (1 -p^2)}{p^2}$$
$$= \frac{(1 - p)p + (1 - p)^2}{p^2} = \frac{p - p^2 + 1 -2p + p^2}{p^2} = \frac{1 - p}{p^2}$$

### (c)

$$f_X(x) = \frac{e^{-(x - \mu)^2 / (2 \sigma^2)}}{\sqrt{2 \pi \sigma}}, \qquad M_X(t) = e^{\mu t + \sigma^2 t^2 / 2} \qquad -\infty < \mu < \infty; \sigma > 0$$

$$M_X(t) = \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^\infty e^{tx} e^{-(x - \mu)^2 / (2 \sigma^2)} dx = \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^\infty  e^{ -(x^2 - 2 \mu x - 2 \sigma^2 t + \mu^2) / (2 \sigma^2) } dx$$

Complete the square in the exponent:

$$x^2 - 2 \mu x - 2 \sigma^2 t + \mu^2 = x^2 - 2x(\mu + \sigma^2 t) + \mu^2$$
$$= [x - (\mu  + \sigma^2 t)]^2 - (\mu + \sigma^2 t)^2 + \mu^2$$
$$= [x - (\mu  + \sigma^2 t)]^2 - [2 \mu \sigma^2 t + (\sigma^2 t)^2]$$
Going back to the density:

$$\frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^\infty  e^{ -(x^2 - 2 \mu x - 2 \sigma^2 t + \mu^2) / (2 \sigma^2)} dx = \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^\infty  e^{ -([x - (\mu + \sigma^2 t)]^2 + [2 \mu \sigma^2 t + (\sigma^2 t)^2]) / (2 \sigma^2)} dx$$
$$= \frac{e^{[2 \mu \sigma^2 t + (\sigma^2 t)^2] / (2 \sigma^2)}}{\sqrt{2 \pi \sigma}} \int_{-\infty}^\infty e^{-[x - (\mu + \sigma^2 t)]^2 / (2 \sigma^2)} dx$$
$$= e^{\mu t + (\sigma^2 t^2) / 2}$$
$$E(X) = \frac{\partial}{\partial t} M_X(t) \Bigg \lvert_{t = 0} = \frac{\partial}{\partial t} e^{\mu t + (\sigma^2 t^2) / 2} \Bigg \lvert_{t = 0} =  (\mu + \sigma^2 t) e^{\mu t + (\sigma^2 t^2) / 2} \Bigg \lvert_{t = 0} = \mu$$
$$E(X^2) = \frac{\partial}{\partial t} \mu + \sigma^2 t(e^{\mu t + (\sigma^2 t^2) / 2}) \Bigg \lvert_{t = 0} = (\mu + \sigma^2 t)^2 e^{\mu t + (\sigma^2 t^2) / 2} + \sigma^2 e^{\mu t + (\sigma^2 t^2) / 2} \Bigg \lvert_{t = 0} = \mu^2 + \sigma^2$$
$$Var(X) = \sigma^2$$

## 2.34

Let $X$ have the standard normal distribution:

$$f_X(x) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}}, \qquad -\infty < x < \infty$$

Define a discrete random variable $Y$ scuh that:

$$f_Y(y) = \begin{cases} \frac{1}{6} & \text{if } y = \sqrt{3} \\ \frac{2}{3} & \text{if } y = 0 \\ \frac{1}{6} & \text{if } y = -\sqrt{3} \end{cases}$$
Show that:

$$E(X^r) = E(Y^r) \qquad \text{for } r = 1, 2, 3, 4, 5$$
$$E(X) = 0 \qquad E(Y) = \frac{1}{6} \cdot \sqrt{3} + \frac{2}{3} \cdot 0 - \frac{1}{6} \cdot \sqrt{3} = 0$$
$$E(X^2) = \mu^2 + \sigma^2 = 1 \qquad E(Y) = \frac{1}{6} \cdot \sqrt{3}^2 + \frac{1}{6} \cdot (-\sqrt{3})^2 = 1$$
$$E(X^3) = \frac{\partial^3}{\partial t^3} e^{t^2 / 2} \Bigg \lvert_{t = 0} = \frac{\partial^2}{\partial t^2} te^{t^2 / 2} \Bigg \lvert_{t = 0} = \frac{\partial}{\partial t} e^{t^2 / 2} + t^2e^{t^2 / 2} \Bigg \lvert_{t = 0} = te^{t^2 / 2} + 2te^{t^2 / 2} + t^3e^{t^2 / 2} \Bigg \lvert_{t = 0} = 0$$
$$E(Y^3) = \frac{1}{6} \cdot 3^{3/2} - \frac{1}{6} \cdot 3^{3/2} = 0$$

$$E(X^4) = \frac{\partial}{\partial t} te^{t^2 / 2} + 2te^{t^2 / 2} + t^3 e^{t^2 / 2} \Bigg \lvert_{t = 0} = e^{t^2 / 2} + t^2 e^{t^2 / 2} + 2 e^{t^2 / 2} + 2t^2 e^{t^2 / 2} + 3t^2 e^{t^2 / 2} + t^4 e^{t^2 / 2} \Bigg \lvert_{t = 0} = 3$$
$$E(Y^4) = \frac{1}{6} \cdot 3^2 + \frac{1}{6} \cdot 3^2 = \frac{18}{6} = 3$$

$$E(X^5) = 0 \qquad \text{(there's no terms with } t^1 \text{ in the previous equation)}$$
$$E(Y^5) = \frac{1}{6} \cdot 3^{\frac{5}{2}} - \frac{1}{6} \cdot 3^{\frac{5}{2}} = 0$$
## 2.35

Let $X$ have the following distribution:

$$f_X(x) = \frac{1}{\sqrt{2 \pi} x} e^{-[\log(x)]^2 / 2}, \qquad 0 \leq x < \infty$$

### (a)

Show that:

$$E(X^r) = e^{r^2 / 2}$$

$$E(X^r) = \frac{1}{\sqrt{2 \pi}} \int_0^\infty x^{r - 1} e^{-[\log(x)]^2 / 2} dx$$
$$= \frac{1}{\sqrt{2 \pi}} \int_{0}^\infty (e^z)^{r - 1} e^{-z^2 / 2} e^z dz \qquad \text{(change of variable } z = \log(x))$$
$$= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty e^{-(z^2 - 2zr) / 2} dz$$
$$= \frac{1}{\sqrt{2 \pi }} \int_{-\infty}^\infty e^{- [(z - r)^2 - r^2] / 2} \qquad \text{(complete the square: } z^2 - 2zr = (z - r)^2 - r^2)$$
$$= \frac{1}{\sqrt{2 \pi}} e^{r^2 / 2} \int_{-\infty}^{\infty} e^{-(z - r)^2 / 2}$$
$$= e^{r^2 / 2}$$
### (b)

Now show that:

$$\int_0^\infty x^r \sin[2 \pi \cdot \log(x)] f_X(x)  dx = 0$$

$$\int_0^\infty x^r \sin[2 \pi \cdot \log(x)] f_X(x)  dx = \frac{1}{\sqrt{2 \pi}} \int_0^\infty x^{r - 1} \sin[2 \pi \cdot \log(x)]  e^{-[\log(x)]^2 / 2} dx$$
$$= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty e^{zr} \sin[2 \pi \cdot \log (e^z)] e^{-z^2 / 2} dz \qquad \text{(change of variable } z = \log(x))$$
$$= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty \sin[2 \pi \cdot z] e^{-(z^2 - 2zr) / 2}$$
$$= \frac{1}{\sqrt{2 \pi }} \int_{-\infty}^\infty \sin[2 \pi z] e^{-[(z - r)^2 - r^2] / 2}$$

The integrand is an odd function: $$\sin[2 \pi z] e^{-[(z - r)^2 - r^2] / 2} = -\sin[2 \pi z] e^{-[(z - r)^2 - r^2] / 2}$$. As such, the integral will be 0. 

```{r}

r <- 3
x <- seq(-4 * pi, 4 * pi, 0.01)
f <- function(x) sin(2 * pi * x) * exp(-((x - r)^2 - r^2)/ 2)

plot(x, f(x), type = 'l', axes = FALSE, 
     ylab = expression(f[X](x)), col = 'steelblue')
axis(1, tick = FALSE)
axis(2, tick = FALSE, las = 1)
box(bty = 'L')

integrate(f, -4 * pi, 4 * pi)

```

## 2.36

The lognormal distribution has an interesting property. If we have the pdf: 

$$f_X(x) = \frac{1}{\sqrt{2 \pi} x} e^{-[\log(x)]^2 / 2}, \qquad 0 \leq x < \infty$$

Then, as the previous exercise (Exercise 2.35) shows, all moments of the function exists and are finite. However, the distribution does not have a moment generating function. Prove this.

$$M_X(t) = \frac{1}{\sqrt{2 \pi}} \int_0^\infty e^{tx} e^{-[\log(x)]^2 / 2} dx$$
$$= \frac{1}{\sqrt{2 \pi}} \int_0^\infty e^{-\log(x)^2 /  2 + tx} dx$$

Now:

$$\lim_{x \to \infty} e^{tx - \log(x)^2/2 } = \lim_{x \to \infty} \frac{e^{tx}}{e^{\log(x)^2 / 2}} = \lim_{x \to \infty} \frac{te^{tx}}{e^{\log(x)^2 / 2} \frac{\log(x)}{x}} = \lim_{x \to \infty} \frac{xte^{tx}}{e^{\log(x)^2 / 2} \log(x)} = \infty$$

As such, the integrand will tend to infinity and the integral won't converge, hence mgf does not exist. 

## 2.37

Let:

$$f_X(x) = \frac{1}{\sqrt{2 \pi}} e^{-x^2 / 2} \qquad f_Y(y) = \frac{1}{\sqrt{2 \pi}} e^{-x^2 / 2} \bigg[ 1 + \frac{1}{2} \sin(2 \pi x) \bigg]$$

be pdfs with the following cumulant generating functions:

$$K_X(t) = t^2 / 2 \qquad \text{and} K_Y(t) = K_X(t) + \log \bigg[ 1 + \frac{1}{2}e^{-2 \pi^2 } \sin(2 \pi t) \bigg]$$

### (a)

Plot $f_X(x)$ and $f_Y(y)$ to illustrate their difference:

```{r}

x <- seq(-5, 5, 0.01)
d1 <- dnorm(x)
d2 <- dnorm(x) * (1 + 1/2 * sin(2 * pi * x))

plot(x, d2, type = 'l', col = 'firebrick', ylab = 'Density', axes = FALSE)
lines(x, d1, col = 'steelblue')
axis(1, tick = FALSE)
box(bty = 'L', col = 'grey80')

```

### (b)

Plot the cumulant generating functions $K_X$ and $K_Y$ to illustrate their similarity:

```{r}

t <- seq(-5, 5, 0.01)

k1 <- t^2 / 2
k2 <- t^2 / 2 + log(1 + 1/2 * exp(-2 * pi^2) * sin(2 * pi * t))

plot(x, k1, type = 'l', col = 'firebrick', ylab = '', axes = FALSE)
lines(x, k2, col = 'steelblue', lty = 'dashed')
axis(1, tick = FALSE)
box(bty = 'L', col = 'grey80')

```

### (c)

Calculate moment generating functions of $f_X(x)$ and $f_Y(Y)$. Are they similar or different?

DO SOME OTHER TIME

## 2.38

Let $X$ have a negative binomial distribution with pmf:

$$f_X(x) = {r + x - 1 \choose x} p^r ( 1 - p)^x, \qquad x = 0, 1, \ldots, \; r = 0, 1, \ldots, \; 0 < p < 1$$

### (a)

Calculate the mgf of $X$:

$$M_X(t) = p^r \sum_{x = 0}^\infty e^{tx} {r + x - 1 \choose x}  (1 - p)^x$$
$$= p^r \sum_{x = 0}^\infty {r + x - 1 \choose x} [(1 - p)e^t]^x$$
$$= \frac{p^r}{[1 - (1 - p)e^t]^r} = \bigg( \frac{p}{1 - (1 - p)e^t} \bigg)^r$$

### (b)

Define a new random variable $Y = 2 pX$. Show that as $p \to 0$, the mgf of $Y$ converges to that of a Chi-squared random variable with $2r$ degrees if freedom by showin that:

$$\lim_{p \to 0} M_Y(t) = \bigg( \frac{1}{1 - 2t} \bigg)^r, \qquad \lvert t \lvert < \frac{1}{2}$$

$$M_Y(t) = M_X(2pt) = \bigg( \frac{p}{1 - (1 - p)e^{2 pt}} \bigg)^r$$

$$\lim_{p \to 0 } \frac{p}{1 - (1 - p)e^{2 pt}} = \lim_{p \to 0} \frac{1}{1 + pe^{2pt} - 2t(1 - p) e^{2pt}} = \frac{1}{1 - 2t}$$

$$\implies \lim_{p \to 0} M_Y(t) = \bigg( \frac{1}{1 - 2t} \bigg)^r$$

## 2.39

In each of the following cases calculate the indicated derivatives, justifying all operations:

### (a)

$$\frac{\partial}{\partial x} \int_0^x e^{-\lambda t} dt = e^{-\lambda x}$$

Verify:

$$\frac{\partial}{\partial x} \bigg[ \int_0^x e^{-\lambda t} dt \bigg] = \frac{\partial}{\partial x} \bigg[ -\frac{e^{-\lambda t}}{\lambda} \bigg]_0^x = \frac{\partial}{\partial x} \bigg( \frac{1}{\lambda} - \frac{e^{-\lambda x}}{\lambda} \bigg) = e^{-\lambda x}$$

### (b)

$$\frac{\partial}{\partial \lambda} \int_0^\infty e^{-\lambda t} dt =  \int_0^\infty \frac{\partial}{\partial \lambda} e^{-\lambda t} dt = \int_0^\infty -te^{-\lambda t} dt = - \frac{\Gamma(2)}{\lambda^2} = -\frac{1}{\lambda^2}$$

Verify:

$$\frac{\partial}{\partial \lambda} \bigg[ \int_0^\infty e^{-\lambda t} dt \bigg] =  \frac{\partial}{\partial \lambda} \bigg( \frac{1}{\lambda} \bigg) = -\frac{1}{\lambda^2}$$

### (c)

$$\frac{\partial}{\partial t} \int_0^1 \frac{1}{x^2} dx = -\frac{1}{t^2}$$

Verify:

$$\frac{\partial}{\partial t} \bigg[ \int_0^1 \frac{1}{x^2} dx \bigg] = \frac{\partial}{\partial t} \bigg[ -\frac{1}{x} \bigg]_t^1$$
$$= \frac{\partial}{\partial t} \bigg( \frac{1}{t} - 1  \bigg) = -\frac{1}{t^2}$$

### (d)

$$\frac{\partial}{\partial t} \int_1^\infty \frac{1}{(x - t)^2} dx = \int_1^\infty \frac{\partial}{\partial t} \frac{1}{(x - t)^2} dx = \int_1^\infty -2(x - t)^{-3} dx = \bigg[(x - t)^{-2} \bigg]_{1}^\infty = \frac{1}{(1 - t)^2}$$
Verify:

$$\frac{\partial}{\partial t} \bigg[ \int_1^\infty \frac{1}{(x - t)^2} dx \bigg] = \frac{\partial}{\partial t} \bigg[ - \frac{1}{x - t} \bigg]_1^\infty = \frac{\partial}{\partial t} \frac{1}{1 - t} = \frac{1}{(1 - t)^2}$$

## 2.40

Prove:

$$\sum_{k=0}^x {n \choose k} p^k (1 - p)^{n - k} = (n - x) {n \choose x} \int_{0}^{1 - p} t^{n - x - 1}(1 - t)^x dx$$

(hint: integrate by parts or differentiate both sides wre to $p$)

DO SOME OTHER TIME

# Chapter 3: Common families of distributions

## 3.1

Find expressions for $E(X)$ and $Var(X)$ if $X$ is a random variable with the general discrete $\text{Uniform}(a, b)$ distribution that puts equal probability on values $a, a + 1, \ldots, +b$.

$$E(X) = \sum_{x=a}^b \frac{x}{b - a + 1} = \frac{1}{b - a + 1} \bigg[ \sum_{x = 1}^b x - \sum_{x = 1}^{a - 1} x \bigg] = \frac{1}{b - a + 1} \cdot \bigg[ \frac{b(b + 1)}{2} - \frac{(a - 1)a}{2} \bigg]$$
$$= \frac{b^2 + b - a^2 + a}{2(b - a + 1)} = \frac{(b - a + 1)(a + b)}{2 (b - a + 1)} = \frac{a + b}{2}$$

$$E(X^2) = \frac{1}{b - a + 1} \bigg[ \sum_{x = 1}^b x^2 - \sum_{x = 1}^{a - 1} x^2 \bigg] = \frac{1}{b - a + 1} \bigg[ \frac{b(b + 1)(2b + 1)}{6} - \frac{(a-  1)a (2a - 1)}{6} \bigg]$$
$$= \frac{b(b + 1)(2b + 1) - (a - 1)(a)(2a - 1)}{6(b - a + 1)}$$

$$Var(X) = \frac{b(b + 1)(2b + 1) - (a - 1)(a)(2a - 1)}{6(b - a + 1)} - \bigg( \frac{a + b}{2} \bigg)^2$$
$$= \frac{b(b + 1)(2b + 1) - (a - 1)(a)(2a - 1) - 3(b - a + 1)(a + b)^2}{6(b - a + 1)}$$

## 3.2

A manufacturer receives a lot of 100 parts from a vendor. The lot will be unacceptable if more than five parts are defective. The manufacturer will randomly select $K$ parts from the lot for inspection and the lot will be accepted if no defective parts are found.

### (a)

How large does $K$ have to be to ensure the probability that the manufacturer accepts an unacceptable lot is less than 0.1?

The smallest probability of discovering a defective part will be when there are exactly 6 defective parts (minimum) in the sample. 

$$P(X = 0 \lvert M = 100, N = 6, K) = \frac{{6 \choose 0} {94 \choose K}}{{100 \choose K}} = \frac{\frac{94!}{K! (94 - K)!}}{\frac{100!}{K!(100 - K)!}} = \prod_{i = 95}^{100} \frac{(i - K)}{i}$$

```{r}

K <- 25:35
p <- sapply(K, function(x) prod((95:100 - x) / 95:100))

round(rbind(K, p), 3)

```

The manufacturer needs to sample at least 32 parts to keep the probability of accepting an unacceptable lot under 0.1.

### (b)

Suppose the manufacturer decides to accept the lot if there is at most one defective in the sample. How large does $K$ have to be to ensure the probability of accepting an unacceptable lot is less than 0.1?

$$P(X = 0 \text{ or }  1 \lvert M = 100, N = 6, K) = \frac{{6 \choose 0} {94 \choose K}}{{100 \choose K}} + \frac{{6 \choose 1} {94 \choose K - 1}}{{100 \choose K}}$$

```{r}

K <- 50:60

round(rbind(K, dhyper(0, 6, 94, K) + dhyper(1, 6, 94, K)), 3)

```

The manufacturer needs to sample at least 51 parts.

### 3.3

Suppose a pedestrian can cross the street only if there is no car passing during the next 3 seconds. Assume that there is a probability $p$ of a car passing during each second (which we treat as indivisible units here). Find the probability that the pedestrian has to wait exactly 4 seconds before starting to cross.

DO SOME OTHER TIME

### 3.4 

A man with $n$ keys wants to open his door and tries the keys at random. Exactly one key will open the door. Find the mean number of trials if:

### (a) unsucessfull keys are not eliminated from further selections

Let $p = \frac{1}{n}$:

$$E(X) = \sum_{x=1}^\infty xp(1 - p)^{x - 1}$$
$$= p \sum_{x = 1}^\infty x(1 -p)^{x - 1}$$
$$= p \cdot - \frac{\partial}{\partial p} \sum_{x = 1}^\infty (1 -p)^x$$
$$p \cdot - \frac{\partial}{\partial p} \bigg[ \frac{1}{1 - (1 - p)} - 1 \bigg]$$
$$= p \cdot - \frac{\partial}{\partial p} \frac{1 - p}{p} = p \cdot \frac{-p - (1 - p)}{p^2} = \frac{1}{p} = n$$

(i.e. expectation of a Geometric variable)

### (b) unsuccesful keys are eliminated

$$P(X = x) = P(X = x \lvert X > x - 1)$$ 
$$= P(X = x \lvert X > x - 1) \cdot P(X > x - 1 \lvert X > x - 2) \cdot \ldots P(X > 2 \lvert X > 1) \cdot P(X > 1)$$
$$= \frac{1}{n - (x - 1)} \cdot \bigg(1 - \frac{1}{n - (x - 2)} \bigg) \cdot \bigg( 1 - \frac{1}{n - (x - 3)} \bigg) \cdot \ldots \cdot \bigg( 1 - \frac{1}{n - 1} \bigg) \cdot \bigg(1 - \frac{1}{n} \bigg)$$
$$= \frac{1}{n - (x - 1)} \cdot \bigg(\frac{n - (x - 1)}{n - (x - 2)} \bigg) \cdot \bigg(\frac{n - (x - 2)}{n - (x - 3)} \bigg) \cdot \ldots \cdot \frac{n-2}{n-1} \cdot \frac{n - 1}{n}$$
$$= \frac{1}{n}$$

Therefore:

$$E(X) = \sum_{x = 1}^\infty x \cdot \frac{1}{n} = \frac{1}{n} \sum_{x = 1} x = \frac{1}{n} \cdot \frac{n(n + 1)}{2} = \frac{n + 1}{2}$$

### 3.5 

A standard drug is known to be effective in 80% of the cases in which it is used. A new drug is tested on 100 patients and found to be effective in 85 cases. Is the new drug superior? 

(hint: evaluate probability of observing 85 or more successes assuming that the new drug and old drug are equally effective)

$$P(X = 85 \lvert p = 0.8) = \sum_{x = 85}^{100} {100 \choose x} 0.8^{x} (1 - 0.8)^{100 - x} = 0.1285$$
Would not reject the null under the conventional $\alpha = 0.05$.

## 3.6

We test a commercial insecticide that is advertised to be 99% effective. Let's say we observe 2,000 insects infest a rose-garden, and let $X$ be the number of surviving insects.

### (a)

What probability distribution might provide a reasonable model for this experiment?

Binomial.

### (b)

Write down, but do not evaluate, an expression for the probability that fewer than 100 insects survive, using model in part (a).

$$P(X < 100 \lvert p = 0.99) = \sum_{x = 0}^{100} {2000 \choose x} 0.01^x \cdot 0.99^{2000 - x}$$
### (c)

Evaluate an approximation to the probability in (b).

We can use the normal distribution to approximate the binomial, since although $p$ is close to 1, $N$ is large. The distribution will have the following parameters:

$$\mu = 0.01 \cdot 2000 = 20 \qquad \sigma^2 = 2000 \cdot 0.01 \cdot 0.99 = 19.8$$

Thus the probability will be:

```{r}

pnorm(100, 20, sqrt(19.8), log = TRUE)
pbinom(100, 2000, 0.01, log = TRUE)

```

Both models give a very high probability that fewer than 100 insects survive (close to 1).

## 3.7

Let the number of chocolate chips in a certain type of cookie have a Poisson distribution. We want the probability that a randomly chosen cookie has at least two chocolate chips to be greater than 0.99. Find the smallest value of the mean of the distribution that ensures this probability

$$P(X \geq 2) = P(X > 1) = 1 - P(X = 0) - P(X = 1) = 1 - e^{-\lambda} - \lambda e^{-\lambda}$$
$$1 - e^{-\lambda}(1  + \lambda) = 0.99$$
$$\implies -e^{-\lambda}(1 + \lambda) = -0.01$$
$$\implies e^{-\lambda}(1 + \lambda) = 0.01$$

Solve this via Newton-Raphson

```{r}

flam <- function(lam) exp(-lam) * (1 + lam) - 0.01
dlam <- function(lam) - exp(- lam) * (1 + lam) + exp(- lam)

xold <- Inf
xnew <- 5
epsilon <- 1e-05

while(abs(xnew - xold) > epsilon) {
  
  xold <- xnew
  xnew <- xnew - flam(xnew) / dlam(xnew)
  
}

xnew

```

## 3.8

Two movie theaters compete for the business of 1,000 customers. Assume that each customer chooses between the movie theaters independently and with "indifference". Let $N$ denote the number of seats in each theater.

### (a)

Using binomial model, find and expression for $N$ that will guarantee that the probability of turning a customer away (because of a full house) is less than 1%.

$$P(X > N) = \sum_{x = N + 1}^{1000} {1000 \choose x} 0.5^x (1 - 0.5)^{1000 - x} = \sum_{x = N + 1}^{1000} {1000 \choose x} 0.5^{1000}$$
$$\implies 0.5^{1000} \sum_{x = N + 1}^{1000} {1000 \choose x} < 0.01$$
### (b)

Use normal approximation to get numberical estimate for $N$:

$$\mu = 0.5 \cdot 1,000 = 500 \qquad \sigma^2 = 1,000 \cdot 0.5^2 = 250$$
$$P(X > N) = P\bigg( \frac{X - 500}{\sqrt{250}} > \frac{N - 500}{\sqrt{250}} \bigg) < 0.01$$
$$P \bigg( Z > \frac{N - 500}{\sqrt{250}} \bigg) < 0.01$$

```{r}

qnorm(0.99)

```

$$\frac{N - 500}{\sqrt{250}} = 2.326 \implies N = 500 + \sqrt{250} \cdot 2.326 \implies N = 537$$

## 3.9

An elementary school in New York state reported that its incoming kindergarten class contained five sets of twints. This was quoted to be a "statistical impossibility". Was it?

### (a)

The probability of a twin birth is approximately 1/90, and we can assume that elementary school will have approximately 60 children entering kindergarten. Explain how our "statistically impossible" event can be thought of as the probability of 5 or more successes from a $\text{Binomial(60, 1/90)}$.

We can think of the children entering as independent Bernoulli trials, each with a probability 1/90 of being a twin birth.

```{r}

(p <- 1 - pbinom(4, 60, 1/90))

p * 1e6
1e6 / (p * 1e6)

```

This is fairly rare, but definitely not "one-in-a-million" rare. 

### (b)

Consider this could happen in any school in any county & would've been reported exactly the same. Is this still rare, or something that could be expected to occur?

It can't really be considered unusual within that context, and further even less so if we consider all the different states & times this could've happened.

## 3.10

A Florida police department seized 496 suspected packets of cocaine, of which four were randomly selected and found to actually be cocaine. The police then chose two more packets at random and, posing as drug dealers, sold the packets to the defendent. These last two packets were lsot before they could be tested.

### (a)

If the original 496 packets were composed of $N$ packets of cocaine and $M = 496 - N$ non-cocaine, show that the probability of selecting 4 cocaine packets and then 2 noncocaine packets is:

$$\frac{{N \choose 4} {M \choose 2}}{{N + M \choose N} {N + M - 4 \choose 2}}$$
We can think of the event as two independent hypergeometric trials: we first draw 4 packets (all cocaine) out of $N + M$ total packets, and then we draw 2 packets (both non-cocaine) out of $N + M - 4$ packets:

$$P(X = 4 \text{ and } Y = 2 \lvert N, M) = \frac{{N \choose 4} {N + M - N \choose 4 - 4}}{{N + M \choose 4}} \cdot  \frac{{M \choose 2} {N + M -4 - M \choose 2 - 2}}{{N + M - 4 \choose 2}} = \frac{{N \choose 4} {M \choose 2}}{{N + M \choose 4} {N + M - 4 \choose 2}}$$
### (b)

Maximizing the probability in part (a) maximizes the defendants innocence probability. Show that this probability is 0.022 attained at $M = 165$, $N = 331$.

Since the expression involves a lot of nasty factorials I ended up using grid search:

```{r}

lprob <- function(x) {
  N <- x[1]; M <- x[2]
  lchoose(N, 4) + lchoose(M, 2) - lchoose(N + M, 4) - lchoose(N + M - 4, 2)
} 

N <- 4:494
M <- 496 - N

theta <- cbind(N, M)
prob <- apply(theta, 1, lprob)

M[which.max(prob)]
N[which.max(prob)]

```

## 3.14

The hypergeometric distribution can be approximated by either the Binomial or the Poisson distribution. Let $X$ have the hypergeometric distribution:

$$P(X = x \lvert N, M, K) = \frac{{M \choose x} {N - M \choose K - x}}{{N \choose K}}$$
### (a)

Show that as $N \to \infty, \; M \to \infty, \; M / N \to p$,

$$P(X = x \lvert N, M, K) \to {K \choose x} p^x (1 - p)^{K - x}$$

(hint: Stirling's formula may be helpful)

$$\lim_{N \to \infty, \; M \to \infty, \; M / N \to p} \frac{{M \choose x} {N - M \choose K - x}}{{N \choose K}} = \frac{K!}{x!(K - X)!} \lim_{N \to \infty, \; M \to \infty, \; M / N \to p} \frac{M! (N - M)! (N - K)! }{(M - x)! (N - M - K + x)! N!}$$

Stirling's formula is:

$$n! \approx \sqrt{2 \pi} n^{n + 1/2}e^{-n}$$
We can replace each of the factorial terms with its Stirling approximation, the $\sqrt{2 \pi}$ terms cancel out, and, since $N \to \infty$ and $M \to \infty$, the $e^{-n}$ terms cancel out as well. So we're left with:

$${K \choose x} \lim_{N \to \infty, \; M \to \infty, \; M / N \to p} \frac{M^{M + 1/2} (N - M)^{N - M + 1/2} (N - K)^{N - K + 1/2} }{(M - x)^{M - x + 1/2} (N - M - K + x)^{N - M - K + x + 1/2} N^{N + 1/2}}$$

We can now break down the limit into simpler terms & evaluate each separately:

$$\lim_{M \to \infty} \bigg( \frac{M}{M - x} \bigg)^M = \lim_{M \to \infty} \frac{1}{\bigg( \frac{M- x}{M} \bigg)^M} = \lim_{M \to \infty} \frac{1}{\bigg(1 - \frac{x}{M} \bigg)^M} = \frac{1}{e^{-x}} = e^{x}$$
Similarly:

$$\lim_{N \to \infty, M \to \infty} \bigg( \frac{N - M}{N - M - K + x} \bigg)^{N - M} = e^{K - x}$$

And:

$$\lim_{N \to \infty} \bigg( \frac{N - K}{N}^N \bigg) = e^{-K}$$

The product of these three limits is 1: $e^{x} \cdot e^{K - x} \cdot e^{-K } = 1$

Further terms:

$$\lim_{M \to \infty} \bigg( \frac{M}{M - x} \bigg)^{1/2} = 1$$
$$\lim_{N \to \infty, \; M \to \infty} \bigg( \frac{N - M}{N - M - K + x} \bigg)^{1/2} = 1$$
$$\lim_{N \to \infty} \bigg( \frac{N - K}{N} \bigg)^{1/2} = 1$$

The last term:

$$\lim_{N \to \infty, M \to \infty, N / M \to p} \frac{(M - x)^x (N - M + K - x)^{K - x}}{(N - K)^K} $$
$$= \lim_{N \to \infty, M \to \infty, N / M \to p} \bigg( \frac{M - x}{N - K}^x  \bigg)\bigg( \frac{N - M + K - x}{N - K} \bigg)^{K - x}$$
$$= p^x (1 - p)^{K - x}$$

Thus, finally:

$$P(X = x \lvert N, M, K) \to {K \choose x} p^x ( 1 - p)^{K - x}, \qquad x = 0, 1, \ldots, K$$

### (b)

Use the fact that the Binomial can be approximated by Poisson to show that if $N \to \infty$, $M \to \infty$, $K \to \infty$, $M / N \to 0$, and $KM / N \to \lambda$, then:

$$P(X = x \lvert N, M, K) \to \frac{e^{-\lambda} \lambda^x}{x!} , \qquad x = 0, 1, \ldots$$
DO SOME OTHER TIME

### (c)

DO SOME OTHER TIME

## 3.12

Suppose $X$ has a $\text{Binomial}(n, p)$ distribution and let $Y$ have a $\text{Negative Binomial}(r, p)$ distribution. Show that $F_X(r - 1) = 1 - Y_X(n - r)$:

$$F_X(r - 1) = P(X \leq r - 1) = P(r\text{th success occurs after } n + 1 \text{ trials or later})$$
$$= P(\text{at least } n + 1 - r \text{ failures occure before the } r \text{th success})$$
$$P(Y \geq n + 1 - r)$$
$$= 1 - P(Y \leq n - r) = 1 - F_Y(n - r)$$

## 3.13

A 0-truncated random variable $X_T$ has pmf:

$$P(X_T = x) = \frac{P(X = x)}{P(X > 0)}, \qquad x = 1, 2, \ldots$$

Find the pmf, mean, and variance of the following 0-truncated random variables:

### (a)

$X \sim \text{Poisson}(\lambda)$

$$P(X > 0) = 1 - P(X = 0) = 1 - e^{-\lambda}$$
$$f_{X_T}(x) = \frac{\lambda^x e^{-\lambda}}{x! (1 - e^{-\lambda})}$$
$$E(X_T) = \sum_{x = 1}^\infty x \frac{P(X = x)}{P(X > 0)} = \frac{1}{P(X > 0)}E(X) = \frac{\lambda}{1 - e^{-\lambda}}$$
$$E(X_T^2) = \sum_{x = 1}^\infty x \frac{P(X = x)}{P(X > 0)} = \frac{1}{P(X > 0)}E(X^2) = \frac{\lambda^2 + \lambda}{1 - e^{-\lambda}}$$
$$Var(X) = \frac{\lambda^2 + \lambda}{1 - e^{- \lambda}} - \bigg( \frac{\lambda}{1 - e^{-\lambda}} \bigg)^2$$

### (b)

$$X \sim \text{Negative Binomial}(r, p)$$

$$P(X > 0) = 1 - P(X = 0) = 1 - {0 + r - 1 \choose 0} (1 - p)^0 p^r = 1 - p^r$$
$$E(X_T) = \frac{r(1- p)}{p (1 - p^r)}$$
$$E(X_T^2) = \frac{r(1 - p) + r^2(1 - p)^2}{p^2(1 - p^r)}$$
$$Var(X_T) = \frac{r(1 - p) + r^2(1 - p)^2}{p^2(1 - p^r)} - \bigg( \frac{r(1 - p)}{p(1 - p^r)} \bigg)^2$$

## 3.14

Starting from the 0-truncated Negative Binomial, if we let $r \to 0$, we get an interesting distribution, the logarithmic series distribution. A random variable $X$ has a logarithmic series distribution with parameter $p$ if:

$$P(X = x) = \frac{-(1 - p)^x}{x \cdot \log(p)}, \qquad x = 1, 2, \ldots, \qquad 0 < p < 1$$

### (a) Verify that this defines a legitimate probability function

$$\sum_{x = 1}^\infty \frac{-(1 - p)^x}{x \cdot \log(p)} = \frac{1}{\log(p)} \sum_{x = 1}^\infty \frac{-(1 - p)^x}{x} = \frac{1}{\log(p)} \cdot \log(p) = 1 \qquad \text{(Taylor series of } \log(p) )$$

### (b) Find the mean and the variance of $X$.

$$E(X) = \sum_{x = 0}^\infty x \cdot \frac{-(1 - p)^x}{x \cdot \log(p)} = -\frac{1}{\log(p)} \sum_{x = 0}^\infty (1 - p)^x - 1 = -\frac{1}{\log(p)} \cdot \frac{1 - p}{p} = - \frac{1 - p}{p \cdot \log(p)}$$
$$E(X^2) = \sum_{x = 0}^\infty x^2 \cdot \frac{-(1 - p)^x}{x \cdot \log(p)} = -\frac{1}{\log(p)} \sum_{x = 0}^\infty x \cdot (1 - p)^x - 1 = -\frac{1 - p}{\log(p)} \frac{\partial}{\partial p} \sum_{x = 0}^\infty (1 - p)^{x} - 1 =  -\frac{1 - p}{\log(p)} \frac{\partial}{\partial p} \frac{1 - p}{p}$$
$$= -\frac{1 - p}{\log(p)} \cdot \frac{-p - (1 - p)}{p^2} = -\frac{1 - p}{p^2 \cdot \log(p)}$$
$$Var(X) = -\frac{1 - p}{p^2 \cdot \log(p)} - \bigg( -\frac{1 - p}{p \cdot \log(p)} \bigg)^2  = -\frac{\log(p)(1 - p) + (1 - p)^2}{p^2 \cdot \log(p)^2}$$

## 3.15

The $\text{Poisson}(\lambda)$ distribution is the limit of the $\text{Negative Bnomial}(r, p)$ distribution as $r \to \infty$, $p \to 1$, and $r(1 - p) \to \lambda$. Show that under these conditions, the mgf of the Negative Binomial converges to that of the Poisson.

$$M_X(t) = \bigg( \frac{p}{1 - (1 - p)e^t} \bigg)^r$$

Since $r \to \infty$, we want the term in the brackets to have a form of $\bigg(1  + \frac{A}{cr} \bigg)^r$, so that we can apply the limit: $\lim_{n \to \infty} \bigg( 1 + \frac{1}{n} \bigg)^n = e$.

We want $(1 + \frac{A}{1 - (1 - p)e^t}) = \frac{p}{1 - (1 - p)e^t}$. If we focus just on the numerator, we have:

$$A + [1 - (1 - p)e^t] = p$$
$$\implies A = p - 1 + e^t - pe^t$$
$$\implies A = (1 - p)(e^t - 1)$$

So:

$$M_X(t) = \bigg( \frac{p}{1 - (1 - p)e^t} \bigg)^r = \bigg(1 + \frac{(1 - p)(e^t - 1)}{1 - (1 - p)e^t} \bigg)^r = \bigg(1 + \frac{r(1 - p)(e^t - 1)}{r[1 - (1 - p)e^t]} \bigg)^r$$

Now, the term:

$$\lim_{r \to \infty, \; p \to 1, \; r(1 -p) \to \lambda} \frac{r(1 - p)(e^t - 1)}{1 - (1 - p)e^t} = \frac{\lambda(e^t - 1)}{1} = \lambda(e^t - 1)$$

And so:

$$\lim_{r \to \infty, \; p \to 1, \; r(1 -p) \to \lambda} M_X(t) = e^{\lambda (e^t - 1)}$$
Which is the Poisson MGF.

## 3.16

Verify these two identities regarding the gamma function:

### (a)

$$\Gamma(\alpha + 1) = \alpha \Gamma(\alpha)$$

$$\Gamma(\alpha + 1) = \int_0^\infty x^{\alpha + 1 - 1} e^{-x} dx = \int_0^\infty x^\alpha e^{-x}dx$$

Do partial fractions: $u = x^\alpha, \; u' = \alpha x^{\alpha - 1}, \; v = -e^{-x}, \; v' = e^{-x}$:

$$= \bigg[-x^\alpha e^{-x} \bigg]_0^\infty - \alpha \int_0^\infty -x^{\alpha - 1} e^{-x}dx = 0 + \alpha \int_0^\infty x^{\alpha - 1} e^{-x} dx = \alpha \Gamma(\alpha)$$

### (b)

$$\Gamma \bigg(\frac{1}{2} \bigg) = \sqrt{\pi}$$
$$\Gamma \bigg(\frac{1}{2} \bigg) = \int_0^\infty x^{\frac{1}{2} - 1} e^{-x} dx$$
$$= \int_0^\infty \frac{\sqrt{2}}{z} e^{-z^2 / 2} z dz \qquad \text{(change of variable} \; z = \sqrt{2x})$$
$$= \sqrt{2} \int_0^\infty e^{-z^2 / 2} dz = \sqrt{2} \cdot \frac{\sqrt{\pi}}{\sqrt{2}} = \sqrt{\pi}$$

## 3.17

Establish the following formulae: if $X \sim \text{Gamma}(\alpha, \beta)$, then for any positive constant $\nu$:

$$E(X^{\nu}) = \frac{\beta^{\nu} \Gamma(\nu + \alpha)}{\Gamma(\alpha)}$$

$$E(X^{\nu}) = \int_0^\infty x^{\nu} \frac{1}{\Gamma(\alpha) \beta^\alpha} x^{\alpha - 1} e^{-x/\beta} dx$$
$$= \frac{1}{\Gamma(\alpha) \beta^\alpha} \int_0^\infty x^{\alpha + \nu - 1} e^{-x / \beta} dx$$
$$= \frac{\Gamma(\alpha + \nu) \beta^{\alpha + \nu}}{\Gamma(\alpha) \beta^\alpha} = \frac{\Gamma(\alpha + \nu) \beta^\nu}{\Gamma(\alpha)}$$

## 3.18 

Let $Y$ be a Negative Binomial random variable. Show that as $p \to 0$, the MGF of the random variable $pY$ converges to that of a Gamma distribution with parameters $r$ and 1.

$$M_{pY}(t) = \bigg( \frac{p}{1 - (1 - p)e^{pt}} \bigg)^r$$

$$\lim_{p \to 0}  \frac{p}{1 - (1 - p)e^{pt}} = \lim_{p \to 0} \frac{1}{e^{pt} - t(1 - p)e^{pt}} = \frac{1}{1 - t}$$

Thus:

$$M_{pY}(t) = (1 - t)^{-r}$$

Which is a MGF of $\text{Gamma}(r, 1)$

## 3.19

Show that:

$$\int_x^\infty \frac{1}{\Gamma(\alpha)} z^{\alpha - 1} e^{-z} dz = \sum_{y = 0}^{\alpha - 1} \frac{x^y e^{-x}}{y!}, \qquad \alpha = 1, 2, 3, \ldots$$

(hint: use integration by parts)

$$\int_x^\infty \frac{1}{\Gamma(\alpha)} z^{\alpha - 1} e^{-z} dz = \frac{1}{(\alpha - 1)!} \bigg[ -z^{\alpha - 1}e^{-z} \bigg]_x^\infty - \frac{1}{(\alpha - 2)!} \int_x^\infty -z^{\alpha - 2} e^{-z} dz = \sum_{y = 0}^{\alpha - 1} \frac{x^y e^{-x}}{y!}$$

Thus, if $X \sim \Gamma(\alpha, 1)$ and $Y \sim \text{Poisson}(x)$, then $P(X \geq x) = P(Y \leq \alpha - 1)$, i.e. the probability that we get $X$ or more failures is the same as the probability that observe less than $\alpha - 1$ occurences.

## 3.20 

Let the random variable $X$ have the pdf:

$$f_X(x) = \frac{2}{\sqrt{2 \pi}} e^{-x^2 / 2}, \qquad 0 < x < \infty$$

### (a)

Find the mean and variance of $X$ (the distribution is called folded normal).

$$E(X) = \frac{2}{\sqrt{2 \pi}} \int_0^\infty xe^{-x^2 / 2} dx = \frac{2}{\sqrt{2 \pi}} \int_0^\infty  \sqrt{2z} e^{-z} \frac{1}{\sqrt{2z}} dz = \frac{2}{\sqrt{2\pi}} \cdot \int_0^\infty e^{-z} dz = \frac{2}{\sqrt{2\pi}}$$
$$E(X^2) = \frac{2}{\sqrt{2 \pi}} \int_0^\infty x^2e^{-x^2 / 2} dx = \frac{2}{\sqrt{2 \pi}} \int_0^\infty  2z e^{-z} \frac{1}{\sqrt{2z}} dz = \frac{\sqrt{2}}{\sqrt{\pi}} \cdot \int_0^\infty z^{\frac{1}{2}}e^{-z} dz = \frac{\sqrt{2}}{\sqrt{\pi}} \cdot \Gamma \bigg(\frac{1}{2} \bigg) = \sqrt{2}$$

### (b)

If $X$ has the folded normal distribution, find the transformation $g(X) = Y$ and values of $\alpha$ and $\beta$ so that $Y \sim \text{Gamma}(\alpha, \beta)$.

DO SOME OTHER TIME.

## 3.21

Write the integral that would define the mgf of the pdf:

$$f_X(x) = \frac{1}{\pi} \frac{1}{1 + x^2}$$
$$M_Y(t) = \int_{0}^\infty e^{tx} f_X(x) dx = \int_0^\infty \frac{e^{tx}}{1 + x^2} dx > \int_0^\infty \frac{x}{1 + x^2} dx = \frac{1}{2} \bigg[ \log(1 + x^2) \bigg]_0^\infty = \infty$$

Thus the moment generating function does not exist. This makes sense, since the mean and variance don't exist for Student's t-distribution with $\nu = 1$.

# 3.22

For each of the following distributions, verify the formulas for $E(X)$ and $Var(X)$ given in the text.

### (a)

Verify $Var(X)$ if $X$ has a Poisson distribution. (hint: compute $E[X(X - 1)] = E(X^2) - E(X)$)

$$E(X) = \lambda$$
$$E[X(X - 1)] = \sum_{x=0}^\infty x(x - 1) \frac{\lambda^x e^{-\lambda}}{x!} = \lambda^2 \sum_{x=2}^\infty  \frac{\lambda^{x - 2} e^{-\lambda}}{(x - 2)!} = \lambda^2$$
$$Var(X)= E(X^2) - E(X)^2 = \lambda^2 + \lambda - (\lambda)^2 = \lambda$$

### (b)

Verify $Var(X)$ if $X$ has a $\text{Negative Binomial}(r, p)$ distribution.

DO SOME OTHER TIME

### (c)

Verify $Var(X)$ if $X$ has a $\text{Gamma}(\alpha, \beta)$ distribution.

$$E(X) = \int_0^\infty x \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha - 1} e^{-\beta x} dx$$
$$= \frac{\beta^\alpha}{\Gamma(\alpha)} \int_0^\infty x^{\alpha + 1 - 1} e^{-\beta x} dx = \frac{\beta^\alpha}{\Gamma(\alpha)} \cdot \frac{\Gamma(\alpha + 1)}{\beta^{\alpha + 1}} = \frac{\alpha}{\beta}$$


$$E(X^2) = \int_0^\infty x^2 \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha - 1} e^{-\beta x} dx$$
$$= \frac{\beta^\alpha}{\Gamma(\alpha)} \int_0^\infty x^{\alpha + 2 - 1} e^{-\beta x} dx = \frac{\beta^\alpha}{\Gamma(\alpha)} \cdot \frac{\Gamma(\alpha + 2)}{\beta^{\alpha + 2}} = \frac{(\alpha + 1)\alpha}{\beta^2}$$
$$Var(X) = \frac{(\alpha + 1)\alpha}{\beta^2} - \frac{\alpha^2}{\beta^2} = \frac{\alpha}{\beta^2}$$

### (d)

Verify $E(X)$ and $Var(X)$ if $X$ has a $\text{Beta}(\alpha, \beta)$ distribution.

$$E(X) = \int_0^1 x \cdot \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha - 1} (1 - x)^{\beta - 1} dx$$
$$= \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \int_0^1 x^{\alpha + 1 - 1} (1 - x)^{\beta - 1} dx = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \cdot \frac{\Gamma(\alpha + 1) \Gamma(\beta)}{\Gamma(\alpha + \beta + 1)} = \frac{\alpha}{\alpha + \beta}$$

$$E(X^2) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \int_0^1 x^{\alpha + 2 - 1} (1 - x)^{\beta - 1} dx = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \cdot \frac{\Gamma(\alpha + 2)\Gamma(\beta)}{\Gamma(\alpha + \beta + 2)} = \frac{(\alpha + 1)\alpha}{(\alpha + \beta + 1)(\alpha + \beta)}$$

$$Var(X) = \frac{(\alpha + 1)\alpha}{(\alpha + \beta + 1)(\alpha + \beta)} - \frac{\alpha^2}{(\alpha + \beta)^2}$$
$$= \frac{(\alpha + \beta)(\alpha^2 + \alpha) - (\alpha + \beta + 1)\alpha^2}{(\alpha + \beta + 1)(\alpha + \beta)^2}$$
$$= \frac{\alpha^3 + \alpha^2 + \alpha^2 \beta + \alpha \beta - \alpha^2 -\alpha^2 \beta - \alpha^2}{(\alpha + \beta + 1)(\alpha + \beta)^2}$$
$$= \frac{\alpha \beta}{(\alpha + \beta + 1)(\alpha + \beta)^2}$$

### (e)

Verify $E(X)$ and $Var(X)$ if $X$ has a $\text{Double Exponential}(\mu, \sigma)$ distribution.

$$E(X) = \int_{-\infty}^\infty x \cdot \frac{1}{2\sigma} e^{-\frac{\lvert x - \mu \lvert}{\sigma}} dx$$
$$= \frac{1}{2 \sigma} \int_{-\infty}^\infty ue^{-\lvert u \lvert} dx + \mu \qquad \text{(set } u = x- \mu)$$
$$=\frac{1}{2\sigma} \int_{-\infty}^0 ue^{\frac{ u  }{\sigma}} dx + \frac{1}{2\sigma} \int_{0}^\infty ue^{-\frac{ u}{\sigma}} dx + \mu$$
$$= -\frac{1}{2\sigma} \int_{0}^\infty ue^{-\frac{u}{\sigma}} du + \frac{1}{2\sigma} \int_{0}^\infty ue^{-\frac{u}{\sigma}} du + \mu \qquad \text{(in the first term, set } u = -u)$$
$$= \mu \qquad$$

$$E(X^2) = \int_{-\infty}^\infty x^2 \cdot \frac{1}{2\sigma} e^{-\frac{\lvert x - \mu \lvert}{\sigma}} dx$$
$$= \frac{1}{2\sigma} \int_{-\infty}^\infty u^2 e^{-\frac{\lvert u \lvert }{\sigma}} du + \frac{2\mu}{2\sigma} \int_{-\infty}^\infty ue^{-\frac{\lvert u \lvert}{\sigma}} du + \mu^2 $$
$$= \frac{1}{2\sigma} \int_{-\infty}^0 u^2 e^{\frac{u}{\sigma}} + \frac{1}{2\sigma} \int_0^\infty u^2 e^{-\frac{u}{\sigma}} + \frac{2\mu}{2\sigma} \cdot 0 + \mu^2$$
$$= \frac{1}{2\sigma} \int_{0}^\infty (-u)^2 e^{-\frac{u}{\sigma}} du + \frac{1}{2\sigma} \int_0^\infty u^2 e^{-\frac{u}{\sigma}}du + \mu^2$$
$$= \frac{1}{\sigma} \int_0^\infty u^2 e^{-\frac{u}{\sigma}} du + \mu^2$$
$$\frac{1}{\sigma} \frac{\Gamma(3) }{\bigg( \frac{1}{\sigma} \bigg)^3} + \mu^2 = 2\sigma^2 + \mu^2 \qquad \text{(recognize the integral above as kernel of Gamma(} 3, \sigma^{-1}))$$

## 3.23

The Pareto distribution with parameters $\alpha$ and $\beta$ has pdf:

$$f_X(x) = \frac{\beta \alpha^\beta}{x^{\beta + 1}}, \qquad \alpha < x < \infty, \; \alpha > 0, \; \beta > 0$$
### (a)

Verify that $f_X(x)$ is a pdf:

$$\beta \alpha^\beta \int_\alpha^\infty \frac{1}{x^{\beta + 1}} dx = \beta \alpha^\beta \bigg[-\frac{1}{\beta x^\beta} \bigg]_\alpha^\infty = \beta \alpha^\beta \cdot 0 + \beta \alpha^\beta \cdot \frac{1}{\beta \alpha^\beta} = 1$$

### (b)

Derive the mean and variance of this distribution

$$E(X) = \beta \alpha^\beta \int_\alpha^\infty x \cdot \frac{1}{x^{\beta + 1}} dx = \beta \alpha^\beta \int_\alpha^\infty  \frac{1}{x^{\beta}} dx = \beta \alpha^\beta \bigg[-\frac{1}{(\beta - 1) x^{\beta - 1}} \bigg]_\alpha^\infty = \frac{\alpha \beta}{\beta - 1}$$

$$E(X^2) = \beta \alpha^\beta \bigg[- \frac{1}{(\beta - 2)x^{\beta - 2}} \bigg]_\alpha^\infty = \frac{\alpha^2 \beta}{\beta - 2}$$
$$Var(X) = \frac{\alpha^2 \beta}{\beta - 2} - \frac{\alpha^2 \beta^2}{(\beta - 1)^2}$$

### (c)

Prove that variance does not exist if $\beta \leq 2$

If $\beta \leq 2$, then the integrand $-\frac{1}{(\beta - 2) x^{\beta - 2}}$ will be infinite and so will be integral (of the second moment). 

## 3.24

Many "named" distributions are special cases of the more common distributions already discussed. For each of the following named distributions derive the form of the pdf, verify that it is a pdf, and calculate mean and variance

### (a)

If $X \sim \text{Exponential}(\beta)$, then $Y = X^{\frac{1}{\gamma}}$ has a $\text{Weibull}(\gamma, \beta)$ distribution where $\gamma > 0$ is a constant.

$$y = y(x) = x^{\frac{1}{\gamma}}$$
$$x = x(y) = y^\gamma$$
$$\Bigg\lvert \frac{\partial x}{\partial y} \Bigg\lvert = \gamma y^{\gamma - 1}$$

$$f_Y(y) = \frac{1}{\beta} e^{-y^\gamma / \beta} \gamma y^{\gamma - 1} = \frac{\gamma}{\beta}y^{\gamma - 1}e^{-y^\gamma / \beta}$$

$$E(Y^n) = \frac{\gamma}{\beta} \int_0^\infty y^{n + \gamma - 1} e^{-y^\gamma / \beta} dy$$
$$= \frac{\gamma}{\beta} \int_0^\infty (\beta u)^{(n + \gamma - 1) / \gamma} e^{-u} \frac{1}{\gamma}(\beta u)^{1 / \gamma - 1} \beta du$$
$$= \int_0^\infty (\beta u)^{(n + \gamma) / \gamma - 1} e^{-u} du = \beta^{n / \gamma} \Gamma \bigg(\frac{n}{\gamma} + 1 \bigg)$$

So:

$$E(Y) = \beta^{1 / \gamma} \Gamma\bigg(\frac{1}{\gamma} + 1 \bigg)$$
$$E(Y^2) = \beta^{2 / \gamma} \Gamma\bigg(\frac{2}{\gamma} + 1 \bigg)$$
$$Var(Y) = \beta^{2 / \gamma} \Gamma\bigg(\frac{2}{\gamma} + 1 \bigg) - \beta^{2 / \gamma} \Gamma\bigg(\frac{1}{\gamma} + 1 \bigg)^2 = \beta^{2 / \gamma} \bigg[\Gamma\bigg(\frac{2}{\gamma} + 1 \bigg) - \Gamma\bigg(\frac{1}{\gamma} + 1 \bigg)^2 \bigg]$$

### (b)

If $X \sim \text{Exponential}(\beta)$, then $Y = (2X / \beta)^{1/2}$ has the Rayleigh distribution.

$$y = y(x) = (2X / \beta)^{1/2}$$
$$x = x(y) = \beta y^2 / 2$$
$$ \Bigg\lvert \frac{\partial x}{\partial y} \Bigg \lvert = \beta y$$

$$f_Y(y) = \frac{1}{\beta} e^{-(\beta y^2) / 2 \beta} \beta y = ye^{-y^2 / 2}, \qquad y > 0$$

$$\int_0^\infty ye^{-y^2 / 2} dy = \int_0^\infty \sqrt{2u} e^{-u} \frac{1}{\sqrt{2u}} du =  \int_0^\infty e^{-u} du = 1$$

$$E(X) = \int_0^\infty y^2e^{-y^2 / 2} dy = \int_0^\infty 2u e^{-u} \frac{1}{\sqrt{2u}} du = \sqrt{2} \int_0^\infty u^{1/2} e^{-u} du = \sqrt{2} \Gamma\bigg(\frac{3}{2} \bigg) = \frac{\sqrt{2 \pi} }{2}$$
$$E(X^2) = \int_0^\infty y^3 e^{-y^2 / 2} = \int_0^\infty (2u)^{3/2} e^{-u} (2u)^{-1/2} du = 2 \int_0^\infty u e^{-u} du = 2 \Gamma(2) = 2$$
$$Var(X) = 2 - \frac{2\pi}{4} = \frac{(4 - \pi)}{2}$$

### (c)

If $X \sim Gamma(\alpha, \beta)$, then $Y = 1 / X$ has the $\text{Inverse-Gamma}(\alpha, \beta)$ distribution. 

$$y = y(x) = 1 / x$$
$$x = x(y) = 1 / y$$
$$\Bigg \lvert \frac{\partial x}{\partial y} \Bigg \lvert = \frac{1}{y^2}$$

$$f_Y(y) = \frac{\beta^\alpha}{\Gamma(\alpha)} y^{-\alpha + 1} e^{\frac{\beta}{y}} \cdot y^{-2} = \frac{\beta^\alpha}{\Gamma(\alpha)} y^{-\alpha - 1} e^{\frac{\beta}{y}}, \qquad 0 < y < \infty, \; \alpha > 0, \; \beta > 0$$
(shape and rate)

Or:

$$f_Y(y) = \frac{1}{\beta^\alpha \Gamma(\alpha)} y^{-\alpha - 1} e^{\frac{1}{\beta y}} \qquad 0 < y < \infty, \; \alpha > 0, \; \beta > 0$$
(shape and scale)

$$\frac{\beta^\alpha}{\Gamma(\alpha)} \int_0^\infty y^{-\alpha - 1} e^{-\beta / y} dy = \frac{\beta^\alpha}{\Gamma(\alpha)} \int_0^\infty (\beta / u)^{-\alpha - 1} e^{-u} \beta u^{-2} du = \frac{\beta^\alpha}{\Gamma(\alpha)} \cdot \beta^{-\alpha} \int_0^\infty u^{\alpha - 1} e^{-u} du = 1$$

$$E(Y) = \frac{\beta^\alpha}{\Gamma(\alpha)} \int_0^\infty y^{-\alpha} e^{-\beta / y} dy = \frac{\beta^\alpha}{\Gamma(\alpha)} \cdot \beta^{-\alpha + 1} \int_0^\infty  u^{\alpha} e^{-u} du = \frac{\beta}{\alpha - 1}$$
$$E(Y^2) = \frac{\beta^\alpha}{\Gamma(\alpha)} \int_0^\infty y^{-\alpha + 1} e^{-\beta / y} dy = \frac{\beta^\alpha}{\Gamma(\alpha)} \cdot \beta^{-\alpha + 2} \int_0^\infty  u^{\alpha + 1} e^{-u} du = \frac{\beta^2}{(\alpha - 1)(\alpha - 2)}$$
$$Var(Y) = \frac{\beta^2}{(\alpha - 1)(\alpha - 2)} - \frac{\beta^2}{(\alpha - 1)^2} = \frac{(\alpha - 1)\beta^2 - (\alpha - 2)\beta^2}{(\alpha-1)^2(\alpha - 2)} = \frac{\beta^2}{(\alpha - 1)^2(\alpha - 2)}$$
